\chapter{Lessons learned and discussion}
\label{chap5}

\section{Design for a practical compiler implementing the proposed type system}
\label{new_approach}

We do not recommend following the prototype implementation shown in the \xxx{ref to chap 3}. Instead, the definition of this implementation, with little modification, can serve as a theoretic model for describing various invariants on an inference algorithm that implements the type system \xxx{ref to chap 2; add the things about 5 name spaces and that 4 of them can be quantified in schemes (implication constraints)}.

First, we will go through the various aspects of the prototype implementation and the type system and we will specify how the practical compiler should differ in that aspect, and then, finally, we will describe its algorithm.

\subsection{Inferring programs by components}

For the nature of the proposed type system (it turns into a minimization problem for each subprogram), the type inference greatly benefits from being performed in topological order on strongly connected components of the program defined by explicit references (function calls, types in declarations of variables, etc.) and implicit references (eg., a class instance implicitly references the corresponding class definition, each struct implicitly references the implicitly defined field accessor instances, which then implicitly reference their method definitions).

\begin{remark}
    If the user explicitly specifies the type scheme for an object, we ignore any references to its definition and we store this scheme as if it came from an component defining it, the component containing such an object then just type-checks the provided definition, reporting any errors. We do the same for any blackbox bindings specified as imports.
\end{remark}

A similar component analysis to this is already showcased by the prototype inference layer. The approach we propose for future work constitutes moving this component analysis even before the type inference preprocessor itself.

And then we propose performing the both phases on much smaller subprograms (the singular strongly connected components) with some blackbox schemes for any outside references. \xxx{we then can make an option to the compiler which uses this phase for refactoring}

\subsection{Effects of component-oriented approach to inference}

Inferring single components in their topological order has an effect of making the ``instantiation constraint'' \xxx{introduced here and here} completely obsolete as the type inference preprocessor already knows what types are to be instantiated, as they are part of one of the already solved components, and what types are to be subjected to the replacement of the instantiation constraints with equality constraints \xxx{already described in chap 3}.

An another significant improvement is a great decrease in the complexity (both developmental and computational) of closing the currently solved component, since the prototype implementation has to analyze the space of type variables for any type variables relevant to the current component \xxx{ref chap 3}.

In this proposed topological approach, after a component is closed, we can forget all the variables and constraints after we make sure that there is no information lost by removing them from the current state. This is done by generating the schemes  corresponding to the definitions in the currently closed component (these definitions are then equivalent to arbitrary blackboxes annotated with quantified and constrained type signatures), storing them in the state of the inferencer.

\subsection{Improved representation for subtype dimensions better corresponding to Deferred Inference}

We assume the type system for Deferred Inference with added typing variables, kinding variables and consting variables as described \xxx{in the second chapter}.

Note that the variables from each of the three dimensions in this proposed algorithm occupy a separate space (this has an effect of simplifying the statements about unifications \xxx{reference the four different unification cases}; each of these spaces contains a supply of variables, a set of constructors (for typings, they are the same as for type constructors; for data kinds and constnesses, we have a set of nullary constructors representing their respective constants), a supply of skolem constants - for subtype dimensions, the skolems are just regular variables with a special mark specifying that it represents a skolem constant with a certain context level, this mark is preserved upon unification and if two of these marks meet, only the lesser one is preserved, and a special constructor which represents the $\sup$ operation and takes a set as its argument).

\begin{remark}[Separate spaces for each dimension]
    It is important to note that this separation does not affect how the algorithm operates, it just makes it easier to specify what is to be unified.
\end{remark}

Also, each class constraint (and other constraints depending on whether we want to support existential types, for example) is annotated with a witness, which also occupies a separate space; The proposed proof-checking is the same as in Deferred Inference, but performed strictly on the typing dimension.

\subsubsection{Extended constraint language, }

We propose extending the constraint language with the following constraints (for each subtype dimension $s$).

\begin{enumerate}
    \item $t \to_s s_t$: type variable $t$ is mapped to the subtype variable $s_t$

    \item $s_1 \leq_s s_2$: subtype variable $s_1$ is lesser (less general: with less possible register types, or more constant) than $s_2$. (this one does not apply to typings)

    \item $s_1 \leq_s s_2$: subtype variable $s_1$ is lesser (less general: with less possible register types, or more constant) than $s_2$. (for typings, we define only $t_1 \sim_T t_2$).
\end{enumerate}

\begin{remark}
    It should be clear this models the various mapping functions and sets with very complicated variables \xxx{described in where}. For example, the $t \to_K s_K$ models the $k$ type property. Similarly for other properties. Then, the $s_1 \leq_s s_2$ form, for data kinds, models the $\mcal{K}$ graph, and similarly for $\mcal{C}$.

    Type explanation $p$ is completely unnecessary and the result renaming $u$ as well.
\end{remark}

\subsubsection{Elaborating AST with (qualified) types and schemes directly}

We recommend elaborating the AST nodes with types, schemes and qualified types (schemes for polytyped top level definitions; qualified types, if the corresponding node requires some witness; qualified types are instantiated schemes with constraints not removed from the type signature) and not just type variables, the semantics of which have to be retrieved from the inferencer state.

This approach then reduces the number of generated constraints as we do not need to generate a constraint for every single subterm (A trivial example: if we generate a type \li{bits32} for a \li{TBits 32} node, we can already generate a type \li{ptr (bits32)} for the parent node \li{TPtr (TBits 32)}).

This allows the preprocessor to directly solve some types without the need for more complicated type inference (this comes to design decisions and there are more possible ways to implement this - \xxx{comparing them is a possible concern for a future work}).

Another benefit is that passing the inferencer state to monomorphization and other components of the compiler is then made unnecessary as all type information is directly accessible from the elaborated AST alone.

\begin{remark}
    In the prototype implementation, the only reason for passing the inferencer state to subsequent compiler components is the incompleteness of type information without its interpretation of the type variables. \xxx{ref how the type variables are interpreted}
\end{remark}

\subsection{Revised inference algorithm}

We propose the following type inference pipeline:

(note that all mentions of rewrites apply to all the constraints left to be solved as well as the annotations in AST (limited to the component); for the AST, it might be beneficial towards the performance of the algorithm to store the effects of the five orthogonal sequences of rewrites in five mappings and then to apply these mappings as a final step of solving the type inference of the given component)

(also note that whenever the algorithm describes generating a new constraint $c$ derived from some other constraint $c'$, then $c$ is added to context scope, in which $c'$ is)

\begin{enumerate}
    \item We determine the strongly connected components of the program and their topology order \xxx{as discussed blahblahblah}

    \item Then for each component:

    \begin{enumerate}
        \item We generate the constraints that capture the semantics of the current component, while elaborating each node of AST with its corresponding type.

        For all references within this component, we generate equality constraints (instead of instantiation constraints used in the prototype implementation); then we do the same for references to any monotyped objects from the previously solved components, otherwise, we directly instantiate the schemes of the polytyped objects with fresh type variables (note that, for any qualified object (for example, methods), we elaborate the references to them with these qualifications).

        \item For any constraint containing at least one type $t$, and for each subtype dimension $s$, we generate a constraint $t \to_s s_1$, where $s_1$ is a fresh subtype variable.

        This ensures (together with the next step) that the algorithm models the $t, k, c$ mappings.

        \item We do the same for any skolem constant $t$, with subtype skolem constants instead of subtype variables and we mark it with the context level of $t$. \label{gen_subskolem}

        Note that, because of the occurrence of so many constraints of this form, it might be beneficial to store them ordered by $t$ so that we can quickly identify any pairs (and generally groups) $t \to_s s_1$ and $t \to_s s_2$ and unify them. \label{mapping_once}

        \xxx{Since we do not require any specific ordering, it can be ordered by a hashing function; but there are orderings that are almost invariant under rewrites that might be good as well (orderings determined by the outside constructor first)}

        \item Then, for each dimension $s$, we rewrite any constraints of the form $t_1 \leq_s t_2$ for any two types $t_1$ and $t_2$ into constraints $t_1 \to_s s_1, t_2 \to_s s_2, s_1 \leq_s s_2$, where $s_1, s_2$ are fresh subtype variables in the space of $s$. Similarly for any constraints of the form $t_1 \sim_s t_2$.

        This models the various steps regarding subtypes in the prototype algorithm.

        \item Then, we perform rewrites of the constraints according to type equality constraints, typing equality constraints, and attempting to lookup proofs for any class constraints (by instantiating the respective matching proof schemes). Note that if the classes have any functional dependency sets defined, we perform the replacement  \xxx{defined where?}. But since we require explicit witnesses, we should state the new constraints carry the same witness as the original constraint for the trivial rule and fresh witnesses for other rules (in both cases of the replacement; in proofs, the witnesses are constant).

        If we find a matching proof, we unify the constraint's witnesses with the proof's witness. It should be noted that the proof witnesses are constants as they serve to identify the respective instances.

        This does not differ from the prototype algorithm.

        \item Then, we determine the skolem constants of the functions (or other definitions in the component; WLOG, we will assume them to be functions) by retrieving the free variables from each function's type (stored in the AST), and then quantifying them. Note that the skolem constants for each function might be explicitly requested by the user and thus the constraint are already quantified, then we just check whether such functions have any free variables, and if so, we report errors (or we store the errors to be reported later, quantifying the free variables so the inferring continues as if the user patched this problem by including them in the skolems for each  respective function) \label{func_skolem}

        This step is, in the prototype algorithm, done by just collecting all free variables parented by the function.

        \item If we generated new skolem constants, we perform the step \ref{gen_subskolem} on these skolem constants.

        % \xxx{lemma: if a type variable is bound in an unsolved constraint, it is a skolem or it has lower context level than the current scope}.

        \item (Just a comment, the constraints regarding subkinding and subconsting are postponed to this phase as they do not affect the class resolution nor the types and typings.)

        \item For each subtype dimension $s$, we collect the constraints of the form $t \to_s s$, then, we group them by $t$ and if there is any group of more than one element, we unify them. This then means that each type is mapped to exactly one subtype variable in the given subtype dimension. We can skip this step if we keep the invariant that each type is mapped to exactly one subtype, see the step \ref{mapping_once}.

        \item Then, for each subtype dimension $s$, we determine the strongly connected components of the graphs defined by the subtype variables' topology and we unify the variables present in any such strongly connected component, given the size of the corresponding component is greater than one.

        \item For each subtype dimension $s$, We then identify the fixed lower bounds (as described by \xxx{chap 2}) of each subtype variable $s_1$ that is not marked.

        We then remove all constraints regarding $s_1$. And we unify it with the $\sup$ constructor applied to the set of its fixed lower bounds.

        \xxx{lemma: Then, all subtypes appearing in the constraints of the current component are either: subtype constants, or they are marked to represent skolem type constants, or they are defined via the $\sup$ constructor by other, marked, variables or constants}

        \item We add all the skolem subtype constants coming from the skolems of each context scope of the implication constraints to the quantification of the given context scope.

        Note that this can be done very efficiently as the subtype skolems are marked with the context levels of their respective original type skolems.

        \xxx{lemma: the constraints representing the component now contain no free variables}

        \item If the types of the functions are specified by the unsolved implication constraints, we inject these constraints to the stored type schemes, adding the quantifications and the assumptions of the constraint.
    \end{enumerate}

    \item Then we look at the functions specified as \li{foreign "C"}. And report failure if any such functions are not monotyped (and therefore monomorphic). Otherwise we add them to a list \li{C_symbols} of functions , names of which we do not want to mangle.

    \item The monomorphization then starts by puting the \li{foreign "C"} to the resulting program and then for each reference to a monomorphic object, we simply add it to the monomorphization queue with no substitution. If the referenced object is not monomorphic, we instantiate its scheme, unifying it with the type of the reference resulting in the unification $u$ and then we add it to the monomorphization queue with the unification $u$ and any witnesses the referenced object requests (we either pass our own witnesses or we pass the constant witnesses included with reference). \label{mono_step}

    \item Then, we take the first item from the monomorphization queue and, if it represents a monomorphic object, we add it to the resulting program, following the same process as in the step \ref{mono_step}. If the object is polymorphic, we first check whether it is an instance and if so, it requested a witness, which we then use to efficiently lookup the corresponding instance.

    We then apply the provided substitution to all the nodes in the object. And then we follow the same process as in the step \ref{mono_step}.

    Note that the constant witnesses can be just references to the corresponding instances.

    \item We then mangle all the names that do not appear in \li{C_symbols}.
\end{enumerate}

\subsection{Benefits of the revised algorithm over the prototype implementation}

This different approach differs in only the part of the pipeline which is directly linked to the type inference. It does not require any significant change to the tokenizing, parsing, flattening nor blockifying phase. Only some, not very significant changes, to the translation (code generation) phase reflecting the change from storing the type information in the state of the inferencer to storing the type information directly in the annotations corresponding to each node.

The benefit of this approach is that it, apart from being a much easier implementation and have more space for performance optimization, allows for type-guided constant propagation and all violations to constexpr-related and linkexpr-related constraints are reported as type errors. \xxx{It also allows for writing algorithm akin to \li{template<int N> int f(int &args[N]) \{ ... \}}}.

The other, more significant benefit, is that, instead of defaulting to some default value whenever a data kind is omitted in the \cmm{} code, it is inferred.

We believe that the significance of the primitives used in this design surpass the significance of the proposed implementation. \xxx{maybe move to the very front}

\section{Subtype-dependent class constraint resolution}

In our design, in the prototype implementation just as well as in the proposed design for the practical implementation, we disconnected the concept of subtyping from the mechanism of class constraint resolution.

This is due to the impossibility of solving such systems efficiently \xxx{ref intersection types}.

\section{\xxx{Things discovered along the way}}

As this work is our first compiler with no significant dependencies on an established framework (with the exception of the code generation), our approach was at times, quite experimental. This lead to many design-related findings, some of which might be beneficial to any future work in similar areas.

\begin{itemize}
    \item One such finding is the design pattern we used for defining the AST nodes in Haskell. We will explain it on an example:

    \begin{lstlisting}[language=Haskell]
data TopLevel a
    = TopSection StrLit [Annot Section a]
    | TopDecl (Annot Decl a)
    | TopProcedure (Annot Procedure a)
    | TopClass (Annot Class a)
    | TopInstance (Annot Instance a)
    | TopStruct (Annot Struct a)
    deriving (Show, Functor, Foldable, Traversable, Data)
    \end{lstlisting}

    This pattern is characterized by wrapping each nested node \li{n a} in an \li{Annot n a} structure. We then declare each node to derive \linebreak\li{Functor, Foldable, Traversable, Data}.

    This pattern allows us to represent the AST in a type-safe manner with arbitrary annotations. They can be directly referred to when performing some transformation or data mining process on the given node without \xxx{requiring} any external state.

    The \li{Functor} class then gives us for free very easy way to strip all the annotations: simply by \li{void node}. Or, more generally, to transform each annotation with a pure function \li{f}: using similarly simple \li{f <\$> node}.

    The \li{Foldable} class then allows us the abstraction to perform various folds on the node's annotations (perhaps, to collect some values from them), while \li{Traversable} allows us to perform monadic transformations on them (stateful transformations).

    The \li{Data} class then allows us to write very succinct implementations for transformations of the AST which access only a handful of nodes of the whole structure. The best example of using these would be the implementation for the mangle function, which mangles the names in the AST.

    \begin{ex}[mangle function]
        The following function traverses the AST and replaces the nodes of the type \li{AST.LValue}. This showcases the possible benefits of deriving the \li{Data} class.

        \begin{lstlisting}[language=Haskell]
mangle n@(_ `Annot` (_ :: a)) = go n
  where
    go :: Data d => d -> Inferencer d
    go = gmapM go `extM` lValueCase
    lValueCase (AST.LVName name `Annot` (a :: a)) = do
      name' <- case getTypeHole a of
        SimpleTypeHole {} -> return name
        LVInstTypeHole {} -> AST.Name . addName name
          . fromMaybe "!error" <$>
            mangledFromHoled True a
        _ -> do
          registerASTError a
            . IllegalTypeInformation $ getTypeHole a
          return name
      return $ AST.LVName name' `Annot` a
    lValueCase lvRef = gmapM go lvRef
        \end{lstlisting}

        The important part of the example, we want to show, is the function \linebreak \li{go = gmapM go `extM` lValueCase}. This function, which can be extended with more \li{`extM` xCase} cases performs the monadic (= stateful) transformation \li{lValueCase} on the given input, or, if the type of the argument does not match the type expected by the case, it traverses all immediate subterms, again, attempting to perform one of the monadic transformations, and then recursively on their immediate subterms, etc., until each branch of the expression has match or ends in a leaf.

        Notice that if the case matches the expected type, but not the expected value, we explicitly (using \li{gmapM go}) propagate the transformation to child nodes of the matched case.
    \end{ex}

    The \li{go = gmapM go `extM` lValueCase} pattern does not look that impressive at first, but it is a generalized visitor pattern which can quite significantly shorten the development time and the code complexity. For stateless transformations, use \li{gmapT} and \li{extT}.

    \item The second finding is the importance of separating the whole compilation into layers similarly to the UNIX philosophy. Each of such layers should do one job and do it well. This approach greatly improves the debugging and the analysis of the program.

    As an anti-example, take the monomorphization layer. It has two (albeit very related) jobs: it stores the types inferred by the inference layer into the annotations in AST and it also monomorphizes the code - those are two distinct responsibilities. Separating them into two layers would have an effect of it being possible to use the above pattern to simplify one of such layers into mere hundreds of lines of code.

    Good examples of using this principle are the modules \li{CMM.Mangle} (source of the above example \xxx{ref}) and \li{CMM.FillElabs}.

    \item The third finding is the benefit of defining the layers with no assumptions on the actual structure of the node annotations but requiring a type class which defines the access to the required part of the annotation (most preferably, by a lens). We adhered to this principle quite closely and this allowed us to quite easily introduce new annotations to the AST nodes and to quickly change the the layer pipeline.

    \item The fourth finding is the benefit of adding unnecessary debug information to the data passed around \xxx{the comment constraints in preprocessing; that is not recommended by any source}. And it is especially if this generation of debug information can be turned off by a build option. This principle extends to explicit invariant checks.

    \xxx{more findings}
\end{itemize}
