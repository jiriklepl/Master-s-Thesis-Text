\chapter{Lessons learned and discussion}
\label{chap5}

In this section, we discuss and evaluate various design decisions made during the implementation of the prototype compiler. The prototype offered a good insight into the intricacies of extending type inference by parametric polymorphism with typeclasses and subtyping, and some of the design decisions that were initially considered good have turned to be suboptimal. We summarize such findings, and describe several alternatives that may alleviate the issues we encountered. Finally, we highlight how the implementation in a functional language simplified various complicated compiler constructions.

\section{Compiler architecture drawbacks and possible improvements}

\subsection{Complexity of the inferencer state}
\label{sec:state}

The biggest issue with the prototype compiler is that it transforms all input constraints into a centralized inner state with numerous invariants that have to be explicitly ensured by various ``repair steps''. The reason behind this design decision was that:

\begin{enumerate}
    \item The original vision of subtyping considered propagation of subtype inequalities with type algebra. This was later shown to be incompatible with the desired semantics. Two pointer types constructed from the same target type would share their subtype inequalities \xxx{cref}.

    \item The original intent included an attempt to make typeclass resolution dependent on subtypes. We didn't manage to find any practical model without ambiguity or undecidability.

    Since we define semantics of considered subtypes as a minimization problem, any typeclass allowing at least two minimal instances for some set of inequalities is ambiguous.
\end{enumerate}

Since both of these attempts showed no practical use and the subtype system matured into different way, there are no lasting reasons for this design.

\paragraph{Possible improvement} The inferencer state can be vastly simplified by introducing special type variables with a separate namespace for each (subtype) dimension $s$ and, for each, two extra constraints instead of $t \leq_s t'$:

\begin{itemize}
    \item $t \to_s v$, which binds a type to a subtype dimension (variable, constant, or $\sup_s$ applied to a set of such)
    \item $v \leq_s v'$, which represents inequality on subtype dimensions
\end{itemize}

If we extend this with the $\sup_s$ operator, we can model the same subtyping system with just constraints instead of the complicated structure of state variables. In turn, constraints in the form $t \to_s v$ substitute the functionality variable $\mcal D$, constraints of the form $v \leq_s v'$ replace the variables $\mcal C$, $\mcal K$, $b_C$ and $b_K$, and the variable $\mcal P$ is not required at all.

\subsection{Omission of context levels}
\label{sec:global}

The prototype compiler performs most constraint solving steps on the whole program globally (on all constraints) and not just on strongly connected subprogram components (top-level definitions with cyclic dependencies). This then called for reimplementation of even stronger construct than context levels, ``references to parent type variables'' (each uniquely representing a top-level definition). This is required in the prototype design for deciding what type variables (representing types) are present in each top-level definition.

When the type inference of a top-level definition is closed, the compiler looks up all type variables with the corresponding reference to parent type variable and constructs a scheme for the given definition, which is then added to the state. This is very inefficient and error-prone.

Another consequence of this is the ``instantiation constraint'' introduced by the prototype compiler. If the compiler solved each subprogram component separately, these constraints could be immediately replaced with appropriate instantiations or just unifications, depending on whether the `polytype' operand is present in the same component or in already solved component.

\paragraph{Possible improvement}
It would be much easier to implement the inference in such a way that each subprogram component is solved separately in their topological order.

In such case, the references to parent type variables are not required, because all considered type variables are relevant to the given component and we solve each subprogram component with different variables. Similarly, the instantiation constraint is completely unnecessary. During inference preprocessing for the given component, we already know what references are pointing to polytypes, and we can generate the appropriate constraints.

\subsection{Inferencer state required to interpret elaborations}
\label{sec:inferInter}

A problem connected to the one discussed in \cref{sec:state} is that the prototype compiler stores types exclusively in the inferencer state. This means that this state has to be passed to all subsequent phases. This is most significant during monomorphization, which has to change type elaborations of monomorphic copies of polymorphic top-level definitions. It has to repeatably perform lookups of type interpretations for each elaboration present in the given top-level definition.

\paragraph{Possible improvement}
Alternatively, we can elaborate each node of the inferred code with a fully substituted type (or qualified type for local reference to a polytype; type scheme for a polytype top-level definition).

That way, the qualified types would take a witness of the corresponding instance, and they would also contain the instantiated type of the instance, making the monomorphization (and partially also the subsequent compilation phases) much easier.

\subsection{Weakness in propagating subtype inequalities}
\label{sec:weakness}

The type system does not propagate subtype inequalities as discussed in \cref{sec:state}. This has an effect of ``forgetting'' appropriate subtypes for arguments, when a reference to a procedure is passed. We have explored the implications more closely in \cref{sec:tsLims}.

\paragraph{Possible improvement}
We could add an extra subtype dimension $a$ with only one constant $\bot_a$. For each assignment (for example, \li{x = y}, function argument passing, etc.), we still consider the left-hand operand to be more general in all dimensions than the right-hand operand, including the $a$ dimension.

With that, after we solve the typings in each \emph{program} (external binding with predefined subtype dimensions of arguments), for all pairs of types $t, t'$ such that $t \geq_a t'$, we add more constraints for the desired semantics in other subtype dimensions. For example, if $t \equiv (\alpha \to \beta)$ and $t' \equiv (\alpha' \to \beta')$, we might add the constraints $\alpha \sim \alpha'$ and $\beta' \sim \beta$ (if we require the arguments and the returns of subtype functions be the same).

\section{Useful functional constructs for compiler implementation}

As this work is our first compiler with no significant dependencies on an established framework (with the exception of the code generation), our approach was at times, quite experimental. This lead to many design-related findings, some of which might be beneficial to any future work in similar areas.

\begin{itemize}
    \item One such finding is the design pattern we used for defining the AST nodes in Haskell. We will explain it on an example:

    \begin{lstlisting}[language=Haskell]
data TopLevel a
    = TopSection StrLit [Annot Section a]
    | TopDecl (Annot Decl a)
    | TopProcedure (Annot Procedure a)
    | TopClass (Annot Class a)
    | TopInstance (Annot Instance a)
    | TopStruct (Annot Struct a)
    deriving (Show, Functor, Foldable, Traversable, Data)
    \end{lstlisting}

    This pattern is characterized by wrapping each nested node \li{n a} in an \li{Annot n a} structure. We then declare each node to derive \linebreak\li{Functor, Foldable, Traversable, Data}.

    This pattern allows us to represent the AST in a type-safe manner with arbitrary annotations. They can be directly referred to when performing some transformation or data mining process on the given node without \xxx{requiring} any external state.

    The \li{Functor} class then gives us for free very easy way to strip all the annotations: simply by \li{void node}. Or, more generally, to transform each annotation with a pure function \li{f}: using similarly simple \li{f <\$> node}.

    The \li{Foldable} class then allows us the abstraction to perform various folds on the node's annotations (perhaps, to collect some values from them), while \li{Traversable} allows us to perform monadic transformations on them (stateful transformations).

    The \li{Data} class then allows us to write very succinct implementations for transformations of the AST which access only a handful of nodes of the whole structure. The best example of using these would be the implementation for the mangle function, which mangles the names in the AST.

    \begin{ex}[mangle function]
        The following function traverses the AST and replaces the nodes of the type \li{AST.LValue}. This showcases the possible benefits of deriving the \li{Data} class.

        \begin{lstlisting}[language=Haskell]
mangle n@(_ `Annot` (_ :: a)) = go n
  where
    go :: Data d => d -> Inferencer d
    go = gmapM go `extM` lValueCase
    lValueCase (AST.LVName name `Annot` (a :: a)) = do
      name' <- case getTypeHole a of
        SimpleTypeHole {} -> return name
        LVInstTypeHole {} -> AST.Name . addName name
          . fromMaybe "!error" <$>
            mangledFromHoled True a
        _ -> do
          registerASTError a
            . IllegalTypeInformation $ getTypeHole a
          return name
      return $ AST.LVName name' `Annot` a
    lValueCase lvRef = gmapM go lvRef
        \end{lstlisting}

        The important part of the example, we want to show, is the function \linebreak \li{go = gmapM go `extM` lValueCase}. This function, which can be extended with more \li{`extM` xCase} cases performs the monadic (= stateful) transformation \li{lValueCase} on the given input, or, if the type of the argument does not match the type expected by the case, it traverses all immediate subterms, again, attempting to perform one of the monadic transformations, and then recursively on their immediate subterms, etc., until each branch of the expression has match or ends in a leaf.

        Notice that if the case matches the expected type, but not the expected value, we explicitly (using \li{gmapM go}) propagate the transformation to child nodes of the matched case.
    \end{ex}

    The \li{go = gmapM go `extM` lValueCase} pattern does not look that impressive at first, but it is a generalized visitor pattern which can quite significantly shorten the development time and the code complexity. For stateless transformations, use \li{gmapT} and \li{extT}.

    \item The second finding is the importance of separating the whole compilation into layers similarly to the UNIX philosophy. Each of such layers should do one job and do it well. This approach greatly improves the debugging and the analysis of the program.

    As an anti-example, take the monomorphization layer. It has two (albeit very related) jobs: it stores the types inferred by the inference layer into the annotations in AST and it also monomorphizes the code - those are two distinct responsibilities. Separating them into two layers would have an effect of it being possible to use the above pattern to simplify one of such layers into mere hundreds of lines of code.

    Good examples of using this principle are the modules \li{CMM.Mangle} (source of the above example \xxx{ref}) and \li{CMM.FillElabs}.

    \item The third finding is the benefit of defining the layers with no assumptions on the actual structure of the node annotations but requiring a typeclass which defines the access to the required part of the annotation (most preferably, by a lens). We adhered to this principle quite closely and this allowed us to quite easily introduce new annotations to the AST nodes and to quickly change the the layer pipeline.

    \item The fourth finding is the benefit of adding unnecessary debug information to the data passed around \xxx{the comment constraints in preprocessing; that is not recommended by any source}. And it is especially if this generation of debug information can be turned off by a build option. This principle extends to explicit invariant checks.

    \xxx{more findings}
\end{itemize}
