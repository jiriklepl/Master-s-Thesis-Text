\chapter{Design of the prototype compiler}

\label{chap3}

\begin{figure}
    \includegraphics[width=\linewidth]{img/out/arch.pdf}
\caption{Overview schema of the architecture of the prototype compiler.}
\label{fig:arch}
\end{figure}

\section{Overview}

The overall architecture of the prototype implementation is shown in \cref{fig:arch}. The data flow diagram shows the transformation of the input through the components (depicted by light blue rectangles) of the compiler, each somehow transforming the intermediate representation (AST; depicted by light green parallelograms) and/or generating some error information (depicted by, slightly darker, orange parallelograms).

In this chapter, we will describe the purpose and function of each of the components separately.

\section{Flattening and blockifying}

Flattening and blockifying phase convert a possibly nested \cmm{} source code into code with no nesting that is separated into simple blocks of continuous execution (corresponding to the LLVM basic blocks \xxx{todo ref}). These blocks form a semi-sequential representation of the code organized in a graph structure that reflects the program's control flow. Later, this allows for a straightforward compilation to the SSA form of LLVM.

\begin{remark}
    It is unusual to perform flattening and blockifying before type inference, but it is necessary as these two steps generate calls to the \li{drop} function for each resource-representing object (see \xxx{ref the RAII}) and these calls' types then have to be type-inferred in order to perform the desired resource management actions.
\end{remark}

\xxx{\url{https://llvm.org/doxygen/classllvm_1_1BasicBlock.html}}

LLVM basic block is defined as a sequence of consecutive (non-terminating, if not malformed) statements that starts with a label (or an entry point of a procedure) and ends with a `terminator' statement, such as a branch, a return, an explicitly marked unreachable statement, or an implicit fallthrough to another basic block. Notably, basic blocks can not contain labels other than the initial ones.

For \cmm{}, we work with even more restricted definition of a basic block that only allows branch statements at the end of the basic blocks and we transform the code to specifically this form.

\subsection{Flattening}

Flattening phase transforms all nested statements into trivial single-command statements that are simpler to convert into LLVM basic blocks.

An example of the flattening phase transformation is shown in \cref{lst:flat}.

\begin{listing}
\begin{center}
%\begin{minipage}[t]{0.5\linewidth}
\begin{minipage}{0.5\linewidth}
\begin{lstlisting}
// original code
while:
if w != 0 {
    r = w;
    w = u % w;
    u = r;
goto while;
}
...\end{lstlisting}
\end{minipage}%
\begin{minipage}{0.5\linewidth}
\begin{lstlisting}
// flattened code
while:
if w != 0 {
    goto F_then_1;
} else {
    goto F_fi_1;
}
F_then_1:
r = w;
w = u % w;
u = r;
goto while;
goto F_fi_1;
F_fi_1:
...\end{lstlisting}
\end{minipage}
\end{center}
\caption{Example source code before and after the flattening phase.}
\label{lst:flat}
\end{listing}

Flattening moves the nested bodies of each \li{if} statement to the same level as the \li{if} statement, replacing them with a single \li{goto} statement and adding other \li{goto}s where needed to preserve the semantics. Deeply nested code is flattened recursively. For consistency in subsequent processing, it also generates an explicit \li{goto} for each fallthrough.

The resulting code structure can be directly transcribed into LLVM, with \li{if} statements containing nested bodies with just a single branch expression directly relating to LLVM conditional branches. We perform the transformation similarly for \li{switch} statements. Although \cmm{} does not specify any other control flow statements with nested bodies, the flattening would be applicable even for \li{while} statements and other, more complex conditional execution.

After transforming all function bodies into flat forms, the flattening phase also reorders the bodies of functions so that all declarations are at the top. This is not necessary, but it improves the readability of the generated code for programmers, and also simplifies the implementation of several other parts of the compiler pipeline.

\subsubsection{Filling in calls to the \li{drop} function}

One more task for the flattening phase in the scope of this programming language is that it studies the presence of any resource-representing objects (stack allocated objects carrying \li{new} specification, see \xxx{ref ref ref}) and puts calls to the \li{drop} function accordingly to the function's exit points. This is not a usual goal for the flattening phase, but it fitted its algorithm well enough, so there was no necessity for an extra phase.

\subsection{Blockifying}

The blockifying phase follows the flattening phase and it annotates the statements of functions with annotations stating whether the node starts a basic block, is part of a certain basic block, or that it is not a part of any basic block (and for such, generates warnings). A statement is not a part of any basic block, for example, when it follows a \li{goto} statement and is not preceded by any label. For the identified basic blocks, there is generated an accompanying control-flow graph. And for each block and each variable, the blockifier remembers whether the variable is read before the first write to it and whether it is written to.

\begin{ex}
    An example of blockifying:
    \begin{lstlisting}
while: // starts 1
if x == 0 { goto _then; } // p. of 1; reads "x"; cf += (1, 2)
else      { goto _else; } // ... cf += (1, 3)
_then: // starts 2
x = 5; // part of 2; writes "x"
y = x; // part of 2; writes "y" (does not read "x" before w.)
goto _fi; // part of 2; cf += (2, 4)
_else: // starts 3
x = x + 5; // part of 3; writes "x", reads "x"
goto _fi; // part of 3; cf += (3, 4)
_fi: // starts 4
... // part of 4
    \end{lstlisting}

    The control flow graph contains: $\{(1,2), (1,3), (2,4), (3,4)\}$

\end{ex}


Blockifying is performed by reading the code sequentially and marking each label as a start of a new basic block and each \li{goto} and \li{return} as end of a basic block. The blockifier keeps track of the current basic block in its inner state. If the blockifier identifies a branch from one basic block to another, it adds an edge to the control-flow graph.

Blockifying also identifies illegal fallthroughs (fallthroughs to continuation statements and fallthroughs outside of the procedure) and generates errors for such.

\subsubsection{Live range analysis}

After the blockifier annotates all basic blocks in a procedure and builds the whole control-flow graph, it then measures live ranges of all variables and gives errors for uninitialized variables \xxx{link bednarek}. It also finds out unreachable labels and generates warnings for such.

\subsubsection{Blockifier state uses by other compiler components}

The state the blockifier keeps is usable for translating the code into an SSA form. After the blockifying is finished, for each basic block, it keeps the list of the basic blocks it is preceded by in the control flow of the corresponding procedure, and for each variable present relevant to the basic block, it contains whether the variable is required to be alive at the basic block's entry and whether it is written to (note that the variable can be requested alive even if it is never referred to in the scope of the basic block).

\begin{remark}[Blocks table]
    Note that each basic block is referred to using an unique integer identifier and therefore the blockifier's state also contains a \emph{blocks table}, which translates the identifier back into names. This will be important in \xxx{ref translation}.
\end{remark}

The SSA form then can be generated by a two-phase algorithm where the first phase prepares the relevant phi nodes with placeholders for variables being exported from the preceding basic blocks and then the second phase fills in these placeholders with actual names of the exported variables (they will likely not match the original names of the variables as we have to distinguish between different writes to them). \xxx{connect it to translation}

\section{Type System}

We derived the type system from the description in \xxx{ref prev section}. But there were many flaws in the original design. First, and the most significant flaw, was assuming that the types are equal if they are equal in every subtype dimension and their typing.

\begin{ex}[Inconsistency of assuming type equality equivalent to combination of typing equality and subtype equality in all dimensions]
    Let us assume that the types $t$ and $t'$ are equal in every subtype dimension and the typing dimension. That is, $t =_s t'$ for every subtype dimension $s$ and $t =_T t'$ \xxx{according to def. ref here}.

    Then if they are type applications $\alpha \beta$ and $\alpha' \beta'$, respectively, then from $t =_T t'$, it also follows that $\alpha =_T \alpha'$ and $\beta =_T \beta'$, but it does not follow that $\alpha =_s \alpha'$ and $\beta =_s \beta'$ for every subtype dimension. We will offer a counterexample.

    Let us say, $t$ and $t'$ are pointers to functions that take a 32-bit bitvector and return the same type, they are both constant expressions and the both pointers are stored on the same register (they have the same data kind). This means that $t$ and $t'$ follow the assumptions listed above. But one of them can be constrained to be a pointer to a function that takes an integer and returns an integer, while the other can be constrained to be a pointer to a function that takes a float number and returns a float number. If we assumed that $t = t'$, we would have to solve the constraint $\mtt{float} = \mtt{int}$, where both operands are constants.

    Note that there can be systems with different subtype dimensions, in which the two relations can indeed be defined as equivalent. An example of such a system would be a system with no subtype dimensions.
\end{ex}

This fatal flaw was removed from the system.

\section{Type Inference}

In the modified \cmm{} language, we do not support existentials (\xxx{ref reasoning}) and all polymorphic definitions have to be at top level (or they are easily representable as such - applies to definitions of methods and their instances). This allows the solver to solve more eagerly, since there are no context levels, deferring the constraints only on two cases (see the \cref{main_loop}).

\subsection{Type inference preliminaries}

The basic idea of the implementation of type inference used in our solution is that each type is represented by a type variable. And the semantic value of this variable is defined by the state of inferencer, structure that controls the inference algorithm.

In our solution, the type inferencer keeps track of the type definitions of all known types in the terms of a type constructor applied to variables representing each of its immediate subterms (so called ``primitive patterns''). And for each type variable it contains a handle which stores its subtype dimensions (fully substituted \xxx{?}, contrasting the primitive patterns). In addition to these two databases of ``alive variables'', it also keeps the mapping from each variable's original name (as it appears in annotated AST \xxx{preprocessing}) as the algorithm performs many unifications and substitutions which often causes the variables being renamed.

\subsubsection{Representaition of type-related values}

\begin{defn}[Type variables]
    We will call the space of type variables $\mathbb{V}$.

    Each type variable is represented by an unique integer identifier, its type kind and a reference to a parent type variable, if it has any.

    \emph{Type variable identifier} is an unique number generated by incrementing a specific counter in the preprocessor state \xxx{ref} and then later in the inferencer state.

    \emph{Reference to a parent type variable} is a reference to a variable, which uniquely represents the nested scope in which the variable was generated. The references to parent variables are a generalization of the idea presented by the deferred inference algorithm  \xxx{ref to deferred inference algorithm}. They are used mainly for debugging.
\end{defn}

\begin{defn}[Alive type variable]
    We will call the variables not being renamed to another variable by the algorithm ``alive variables''. Renaming is performed every time two distinct variables are to be unified.
\end{defn}

The representation of types follows the one introduced in the \cref{typing_def}, but extended by the necessary extra constructions.

\begin{defn}[Typings]
    \label{typing_gram}
    $\mathbb{T}$ is a space of typings defined inductively by \cref{lang:typing}.
\end{defn}

\begin{lang}
  \def\grammarP{0.6}
  \begin{grammar}
    \firstcaseP{\mathbb{T}}{\mathbb{V}}{Variable}
    \otherformP{\left[\mathbb{T}\right] \to \mathbb{T}}{Function (each function takes a certain number of argument and returns a type - usually a tuple)}
    \otherformP{\left[\mathbb{T}\right]}{Tuple (function return type)}
    \otherformP{\mathbb{T}\ \mathbb{T}}{Type application (for structs and for constraints)}
    \otherformP{\mtt{Addr}\ \mathbb{T}}{address type (representing a pointer) }
    \otherformP{\cdots}{Primitive types (such as \texttt{bits\_n})}
  \end{grammar}
  \caption{The language $\mathbb{T}$ of flat typings and the basic semantics of types.}
  \label{lang:typing}
\end{lang}

\begin{defn}[Primitive patterns]
    Primitive patterns $\mathbb{P}$ follow the same grammar as typings, see the \cref{typing_gram} but, without the very first grammar case and with type variables ($\mbb{V}$) appearing in the grammar cases in place of typings ($\mbb{T}$).

    Primitive patterns represent the definition of types broken up into single applications of type constructors.
\end{defn}

\begin{remark}[Bijection between type variables and primitive patterns]
    A bijective mapping from type variables to primitive patterns is used to:

    \begin{itemize}
        \item reconstruct a type represented by a variable (recursively)
        \item lookup the variables representing each subterm of a type
        \item lookup the variable which represents the given type
    \end{itemize}

    Keeping a consistent mapping of primitive patterns is quite costly, but it allows for many experiments in type inference and it can also help in debugging.
\end{remark}

\begin{defn}[Type reconstruction]
    Type reconstruction is evaluated by substituting primitive patterns for type variables iteratively until no type variable is possible to be substituted with a primitive pattern. The algorithm starts with a single type variable.
\end{defn}

\begin{lemma}[Primitive patterns]
    \label{typesObs}
    Every type $t$ can be equivalently represented by a type variable, by its primitive pattern (if it has any), and by its reconstructed type acquired by type reconstruction.
\end{lemma}

\begin{proof}
    For types, the theory assumes that  $\alpha \beta = \alpha' \beta' \Leftrightarrow \alpha = \alpha' \land \beta = \beta'$  (generalized to all constructors listed in \xxx{ref}, \xxx{ref}). And then $t = \alpha \beta \land t' = \alpha' \beta' \Leftrightarrow t = t'$. FThe above statement is a direct consequence of these assumptions; for reconstruction, shown by induction.
\end{proof}

\begin{defn}[Subtype dimensions]
    The algorithm recognizes two subtype dimensions, \emph{constness} dimension and \emph{data kind} dimension:

    \begin{itemize}
        \item  The constness dimension, represented by space of constness constants $\mathbb{C}$ represents the bounds of constraints on when the value of each variable has to be known, and, when it can be known. The constness constants are defined as follows:

\begin{center}\begin{grammar}
  \firstcase{\mathbb{C}}{\mathtt{Regular}}{regular runtime variables}
  \otherform{\mathtt{Constexpr}}{variables known at compilation}
  \otherform{\mathtt{Linkexpr}}{variables known at linkage}
\end{grammar}\end{center}

        We define the ordering on constnesses: $\bot = \mathtt{Constexpr} < \mathtt{Linkexpr} < \mathtt{Regular} = \top$. And the whole system of constnesses follows a linear ordering.

        \item The data kind dimension represents the bounds of constraints on on which registers (register types) the variables can be stored. For the types in returned tuples, for example, this can specify whether the variable is integral or a floating point number.

        $\mathbb{K}$ is the space of data kinds, each data kind is a subset of the power set of the set of registers on the given architecture. The kinds are ordered by the relation of being a subset.
    \end{itemize}
\end{defn}

\begin{defn}[Type properties]
    Type properties represent the various dimension of a type. The functions that project objects from $\mbb{D}$ to the respective properties are called $t$, $c$ and $k$.

    $\mathbb{D}$ is the space of type properties given as tuples: $(typing: \mathbb{T}, constness: \mathbb{V}, kind: \mathbb{V})$
\end{defn}

\subsubsection{Modifications to the constraints language of deferred inference}

\begin{defn}[Constraints]
    With $\mbb{W}$ standing for a flat constraint or an implication constraint \xxx{ref defer}, the space of flat constraints $\mathbb{F}$ is defined inductively by a grammar as \cref{lang:flat}.
\end{defn}

\begin{lang}
  \def\grammarP{0.6}
  \def\arraystretch{1.4}
  \begin{grammar}
    \firstcaseP{\mathbb{F}}{\mathbb{T} \sim \mathbb{T}}{Equality constraint (the types are identical)}
    \otherformP{\mathbb{T} \sim_T \mathbb{T}}{Typing equality (the types are identical in the typing dimension)}
    \otherformP{\mathbb{T} \leq_K \mathbb{T}}{SubKind (the right-hand operand has more general data kind than the left-hand operand)}
    \otherformP{\mathbb{T} \leq_C \mathbb{T}}{SubConst (the right-hand operand has more general constness than the left-hand operand)}
    \otherformP{\mathbb{K} \leq_K \mathbb{T} \leq_K \mathbb{K}}{Kind bounds (the type has a data kind from the given range)}
    \otherformP{\mathbb{C} \leq_C \mathbb{T} \leq_C \mathbb{C}}{Const bounds (the type has a constness from the given range)}
    \otherformP{C?\ \mathbb{T}}{Class constraint (the constraint in the type has to be instantiable)}
    \otherformP{C!\ \mathbb{T}}{Class fact (the given constraint is known to be instantiable)}
    \otherformP{\mathbb{T} \sqsupseteq_M \mathbb{T}}{Instantiation constraint (the left-hand operand is a monotype instance of the polytype right-hand operand)}
  \end{grammar}
  \caption{The language $\mathbb{F}$ of flat constraints and their basic semantics.}
  \label{lang:flat}
\end{lang}

\begin{remark}[Derived constraints]
    Sometimes we will use derived constraints like $\mathbb{T} \leq_K \mathbb{K}$, $\mathbb{T} =_K \mathbb{T}$, etc. These are either special cases of the aforementioned constraints, or compositions of multiple constraints. The most complicated derived constraint we will use is the ``SubType'' constraint, $\mathbb{T} \leq \mathbb{T}$, which is a combination of $\mathbb{T} \sim_T \mathbb{T}$, $\mathbb{T} \leq_K \mathbb{T}$ and $\mathbb{T} \leq_C \mathbb{T}$.
\end{remark}

\begin{remark}[Impossible constraints]
    It should be noted that, within the defined language, we cannot state the constraints $\mathbb{T} \not\sim_T \mathbb{T}$, $\mathbb{T} \not\leq_K \mathbb{T}$, $\mathbb{T} \not\leq_C \mathbb{T}$ and $\mathbb{T} \not\sqsupseteq_M \mathbb{T}$ and thus these (and similar ones) are not even derived constraints.
\end{remark}


\begin{defn}[Bounds]
    We call the right-hand operand of a bounds constraint the upper-bound and the left-hand operand the lower-bound.
\end{defn}

\begin{defn}[Trivial bounds]
    If a constraint $F$ states $k_1 \leq_K \tau \leq_K k_2$ and $k_2 \leq k_1$, or $c_1 \leq_C \tau \leq_C c_2$ and $c_2 \leq c_1$, we call such a bounds constraint trivial.

\end{defn}

\begin{lemma}[Trivial bounds]
    \label{trivBoundsObs}
    \begin{enumerate}
        \item If the bounds of a trivial bounds constraint are equal, the type constesses of the types constrained by such a constraint have to be the same and we can unify them.
        \item If, for a bounds constraint, the upperbound is lower than the lowerbound, the constraint cannot be satisfied. We mark it an absurd bounds constraint and we include it in an error report.
    \end{enumerate}
\end{lemma}

\begin{defn}[Functional dependency set]
    We define a space $\mbb {FD}$ of functional dependency sets $FD$.

    Functional dependency set $FD$ on a sequence of type variables $V :: [\mbb V]$ is a set of rules $R$, each of the form $V \to \{\mtt {FROM}, \mtt {TO}, \mtt {UNDEFINED}\}$. This set of rules always contains a ``trivial'' rule $r_0$ such that $r_0(v) = \mtt{FROM}$, this trivial rule is implicit and can be omitted when defining a functional dependency set. If a constraint $C \vect \alpha$ (where $\vect \alpha = V$) follows a functional dependency set $FD$, then, for each proof $C \vect t$ of this constraint and for every corresponding rule $r$ in its set of rules $R$, we generate a new proof of the form $\forall \{v \pipe v \gets \vect \alpha, r(v) = \mtt {TO}\} . \{v \sim t \pipe (v, t) \gets (\mtt {zip}\ \vect \alpha\ \vect t), r(v) = \mtt {TO} \} \To C_r [\mtt {if}\ r(v) = \mtt {FROM}\ \mtt {then}\ t\ \mtt {else}\ v \pipe (v, t) \gets (\mtt {zip}\ \vect \alpha\ \vect t), r(v) \neq \mtt {UNDEFINED}]$. (we borrowed haskell syntax for more succinct definition) This new constraint proof for the variables with $\mtt {FROM}$ variables replaced by types generates new equality constraints on $\mtt {TO}$ variables. We then remove the proof $C \vect t$.

    We then replace each constraint $C \vect t$ with constraints $C_r \vect t_r$ for each corresponding rule $r$, where $t_r$ are subsequences of $t$ generated in a fashion similar to the constraint generation. And we then remove the constraint $C \vect t$.

    In practice, we always simplify the functional dependency sets to minimize the number of constraints. \xxx{fundep simplification}

    The written notation for defining a functional dependency set is a list of statements of the form $\vect \alpha_1 \to \vect \alpha_2$ where $\vect \alpha_1, \vect \alpha_2 \subseteq V$. Each such statement then corresponds to a new rule $r$, where $r(v) := \mtt {FROM}$ if $v \in \alpha_1$, otherwise if $v \in \alpha_2$, then $r(v) := \mtt {TO}$, for each variable $v \in V$.
\end{defn}

\subsection{Type inference preprocessing}

The type inference preprocessing, also called constraint generation takes the AST representation of the source code as its input, annotates it with so-called type holes, and for each of these type holes, generates constraints \xxx{see prev def} as described in \xxx{chap one ref}.

The result of type inference preprocessing is the list of constraints and the annotated AST. The list of constraints is then given to the type inference algorithm and after that the inferred type assignments stored in the state of the inferencer are paired with the annotated AST, resulting in AST with type annotation.

% The algorithm reads the syntactic tree, annotates its nodes (starting from the leaves) is quite complex and the code for it is very lengthy. It is also quite difficult to check for errors. This is why the actual implementation extends the language of constraints with constraint comments.with ``type holes'' and then gives constraints for these type holes.

\begin{defn}[Type holes]
    Type holes are simple structures that contain zero, one or multiple type variables depending on the type of the hole. They serve as the elaborations introduced in \xxx{deferred inference}.

    They help to distinguish, for example, whether an named reference is instantiation of some polymorphic object.
\end{defn}

\subsubsection{Type inference preprocessing algorithm}
\begin{enumerate}
    \item When entering each context (unit, struct, class, instance, function), the type inference preprocessor calls the variable collector. This collector then reports all the newly declared variables inside this context back to the preprocessor.

    \item The preprocessor then generates fresh type variables for these variables and uses them whenever they appear in the currently preprocessed code.

    \xxx{This was later determined to be completely replacible by declaration hoisting in the procedures and preprocessing the program in topological order}.

    \item When preprocessing a piece of code which contains nontrivial subnodes (sub-expressions, sub-statements, etc.), these subnodes are generally preprocessed first, then the algorithm retrieves their type holes and adds the constraints defined by the semantics of the code to the current list of constraints.

    \item The constraints are represented by a list of lists as each context nesting in the code also nests the context of constraints. The decision for a list of list is not completely necessary, but it makes the definition easy to use, future-proof and well-generalizable.

    Closing a constraint just simply pops the first element from this list of lists and gnenerates an implication constraint. This, then, slowly builds a tree of implication constraints to be solved.
\end{enumerate}

\subsection{Type inferencer state}
\label{sec:istate}

During the inference, the algorithm maintains a relatively complex representation of 2 mappings and 2 graphs that representing the relationships between the type variables:

\begin{itemize}
    \item a mapping of renames from variables ``forgotten'' due to unification to their ``alive'' representants
    \item a bijective mapping from type variables that represent types to the primitive patterns that define the types using their immediate subterms of the respective type
    \item a graph with variables as vertices where directed edges represent constness inequalities
    \item a similar graph for data kinds
\end{itemize}

We represent the 4 structures along with the input constraints and some other information about each type, by a set of variables defined in \cref{tab:istate}.

\begin{table}
\scriptsize
\def\arraystretch{1.3}
\begin{tabular}{p{.23\linewidth}p{.7\linewidth}}
\toprule
State variable & Definition \\
\midrule
  $\mathcal{F} \in \mathbb{W}^\ast$ & \textbf{Constraints}\quad
  The complete list of unresolved constraints. In the initial state, it contains the constraints generated by the preprocessor. In subsequent states, it can also contain constraints which are derived from various resolving rules. The algorithm fails if it cannot resolve any of the currently unresolved constraints, and succeeds if $\mathcal{F}$ becomes empty. \\

  $\mathcal{V} \subset \mathbb{V}$ & \textbf{Active type variables}\quad
  Contains the type variables ever encountered by the algorithm and not present in $\mcal G$. The initial value is therefore $\emptyset$. The algorithm then, when generating a ``fresh variable'', uses the smallest possible $v$ such that $\forall v' \in \mcal V. v > v'$. (The implementation actually represents this set via the mapping $u$ and a single integer value.) \\

  $\mathcal{G} \subset \mathbb{V}$ & \textbf{Forgotten type variables}\quad
  This set contains all type variables that were in $\mcal V$ in some past state of the algorithm, but are not in the current state. As above, this state variable is not present in the implementation, but helps describing some invariants. \\

  $\mathcal{P} \subset \mathbb{P}$ & \textbf{Primitive patterns}\quad
  This set contains all type definitions separated encountered or generated by the algorithm separated structurally by type constructors. see \xxx{definition for primitive patterns} \\

  $\mathcal{D} \subset \mathbb{D}$ & \textbf{Type properties}\quad
  This set contains three-valued tuples containing the properties of each type. See \xxx{definition for type properties} \\

  $\mathcal{K} \in \text{Directed graphs on}\ \mathbb{V}$ & \textbf{SubKinds}\quad
  Directional graph that stores the encountered and determined inequalities between data kinds of pairs of type variables. \\

  $b_K \in \mathbb{V} \to \text{interval} \left[\mathbb{K}, \mathbb{K}\right]$ & \textbf{KindBounds}\quad
  Maps each type variable representing the data kind of some types to the known bounds of this data kind. \\

  $\mathcal{C} \in \text{Directed graphs on}\ \mathbb{V}$ & \textbf{SubConsts}\quad
  Similar to $\mcal{K}$, but for constnesses \\

  $b_C \in \mathbb{V} \to \text{interval} \left[\mathbb{C}, \mathbb{C}\right]$ & \textbf{ConstBounds}\quad
  Similar to $b_K$, but for constnesses \\

  $p \in \mathcal{V}_p \to \mathcal{P},\,\mathcal{V}_p \subseteq \mathcal{V}$ & \textbf{Type definition}\quad
  Mapping between type variables with known (encountered or derived) definitions and their respective definitions in terms of immediate subterms. \\

  $d \in \mathcal{V} \to \mathcal{D}$ & \textbf{Type properties}\quad
  Mapping between all type variables and their respective properties. \\

  $u \in \mathcal{G} \to \mathcal{V}$ & \textbf{Result renaming}\quad
  Substitution between forgotten type variables and active type variables resulting from type variables being unified. \\

  $cp \in \mathcal{V}$ & \textbf{Current parent}\quad
  The parent of a currently solved context. \\

  $fs \in \mathcal{V}_f \to \mathbb{W},\,\mcal V_f \subseteq \mcal V$ & \textbf{Schemes}\quad
  The schemes of the closed functions (regular functions, methods, their instances, field accessors) \\

  $cs \in C_{cs} \to [\mathbb{W}],\,C_{cs} \in 2^{\mbb C}$ & \textbf{Class schemes}\quad
  The schemes of the known classes, each of the form $\vect \alpha . F \To C \alpha$, where $C?  \vect \alpha$ is a class constraint and $F \in {\mbb F}^\ast$ is a list of superclasses \\

  $cf \in C_{cf} \to [\mathbb{W}],\,C_{cf} \subseteq C_{cs}$ & \textbf{Class facts}\quad
  The known proofs for each class constraint. Each proof has the form $\vect \alpha . F \To C\ \vect t$, where $C!\ \vect t$.$F$ is a class fact and $F \in {\mcal F}^\ast$ is the list of assumptions of the corresponding proof. \\

  $fd \in C_{fd} \to \mathbb{FD},\,C_{fd} \subseteq C_{cs}$ & \textbf{Functional dependencies}\quad
  Functional dependency sets for each class constraint. They are stored after acquiring a simplified form with subsequent addition of the implicit trivial rule. \\
\bottomrule
\end{tabular}
\caption{Variables maintained as the internal state of the inference algorithm (refer to \cref{sec:istate} for details).}
\label{tab:istate}
\end{table}

In the table, by $<_{\mathcal{K}}$ and $<_{\mathcal{C}}$ we understand that there is a directional path between the given distinct operands in the graphs $\mathcal{K}$ and $\mathcal{C}$, respectively. This is also naturally extended to the other comparison operators --- for example, we say $x =_{\mathcal{K}} y$ if there are paths back and forth between $x$ and $y$ in the graph $\mathcal{K}$, or if $x = y$.


\begin{defn}[simplified notation]
    We expand the function applicability of $b_K$ and $b_C$ to type properties in such a way that for an arbitrary type variable $v \in \mathcal{V}$, we have $b_K (d (v)) = b_K (k (d (v)))$, and similarly for $b_C$. In other words: applying $b_K$ to a type's properties is equivalent to applying it to the type's kind (and similarly for $b_C$ and constness).

    We expand the applicability of $p$ and $d$ in such a way that for an arbitrary type variable $v \in \mathcal{V}_p$ it holds that $d(v) = d(p(v))$ and $p(v) = p(d(v))$.

    And finally, we expand the applicability of $t, k, c$ in such a way that for an arbitrary $v \in \mathcal{V}$ it holds that $t (v) = t (d (v))$, $k (v) = k (d (v))$, and $c (v) = c (d (v))$.
\end{defn}

\subsection{Type inferencer state invariants}

\subsubsection{Intrastate invariants}

The algorithm is designed to have the following intrastate invariants we will :

% TODO: invariant about u

\begin{description}
    \item[Active and forgotten variables] $Var(\mathcal{F}) \cup Var(\mathcal{P}) \cup Var(\mathcal{D}) \cup Var(\mathcal{K}) \cup Var(\mathcal{C}) = Var(\mathcal{V})$ and $Var(\mathcal{V}) \cap Var(\mathcal{G}) = \emptyset$.

    \item[Type properties] $\mathcal{V}$ is projected into $\mathcal{D}$  by the type properties function $d$.

    \item[Type definitions] The subset $\mathcal{V}_p \subseteq \mathcal{V}$ and the set $\mathcal{P}$ are bijected by the type definition function $p$ and its inverse $p^{-1}$. Note that there generally are many types with no known definition.

    \item[Subtype invariants] If we have two type variables $v, v' \in \mathcal{V}$ representing two types, then:
        \begin{description}
            \item[Graph $\mathcal{K}$ has no strongly connected components] $k (v) =_{\mathcal{K}} k (v') \To k (v) = k(v')$

            \item[Graph $\mcal C$ has no strongly connected components] $c (v) =_{\mathcal{C}} c (v') \To c (v) = c(v')$

            \item[Trivially bound by the same data kind] If $b_K (k (v)) = b_K (k (v')) = [k_1, k_1]$ for some kind $k_1 \in \mathbb{K}$, then $k (v) = k (v')$

            \item[Absurdly-bound data kinds considered same] If $b_K (k (v)) = b_K (k (v')) = [k_1, k_2]$ for some kinds $k_1, k_2 \in \mathbb{K}$, $k_2 < k_1$, then $k (v) = k (v')$ and it is an invalid data kind

            \item[Trivially bound by the same constness] If $b_C (c (v)) = b_C (c (v')) = [c_1, c_1]$ for some constness $c_1 \in \mathbb{C}$, then $c (v) = c (v')$

            \item[Absurdly-bound constnesses considered same] If $b_C (c (v)) = b_C (c (v')) = [c_1, c_2]$ for some constnesses $c_1, c_2 \in \mathbb{K}$, $c_2 < c_1$, then $c (v) = c (v')$ and it is an invalid constness

            \item[Data kind bounds propagate transitively] If $k (v) <_{\mathcal{K}} k (v')$, then for bounds $k_1, k_2, k_3, k_4$ such that $b_K (k (v)) = [k_1, k_3]$, and $b_K (k (v')) = [k_2, k_4]$ it holds that $k_1 \leq k_2$ and $k_3 \leq k_4$

            \item[Constness bounds propagate transitively] If $c (v) <_{\mathcal{C}} c (v')$, then for bounds $c_1, c_2, c_3, c_4$ such that $b_C (c (v)) = [c_1, c_3]$, and $b_C (c (v')) = [c_2, c_4]$ it holds that $c_1 \leq c_2$ and $c_3 \leq c_4$
        \end{description}


    \item[Typings propagate with definitions] If a type represented by a type variable $v$ has a known definition (if $v \in \mathcal{V}_p$), then $t (d (v)) = p(v) \left[ \tau := t (d (\tau)) | \tau \in \mathrm{free} (p (d))\right]$. In other words, the typing of the type represented by $v$ follows the typing generated from the typings of the types via which it is defined.
\end{description}

\subsubsection{Interstate invariants}

The algorithm is designed to have the following interstate invariants: (we use the indices to distinguish a successor state variable from its predecessor, we will always assume $n < m \in \mathbb{N}$, we then use these indices in all derived functions too; the list is not exhaustive, these are the ones important to consider when studying the algorithm)

\begin{description}
    \item[Forgotten variables stay forgotten] $v \in \mathcal{G}_n \Rightarrow v \in \mathcal{G}_m$.
    \item[Forgotten variables used to be alive] $v \in \mathcal{G}_m \Rightarrow \exists n . v \in \mathcal{V}_n$.
    \item[All free variables from constraints are considered by the algorithm] $v \in \free {\mathcal{F}_n} \Rightarrow v \in \mathcal{V}_n$.
    \item[Assumed subtype constraints are monotonically restricted] \quad
    \begin{itemize}
        \item For kinds: $v \in {\mathcal{V}}_n \Rightarrow {b_K}_m (k_m(u_m(v))) \subseteq {b_K}_n (k_n(v))$
        \item For constnesses: $v \in {\mathcal{V}}_n \Rightarrow {b_C}_m (c_m(u_m(v))) \subseteq {b_C}_n (c_n(v))$.
    \end{itemize}
    \item[Typings do not get forgotten] $v \in {\mathcal{V}}_n \Rightarrow \exists s . s (t_n(v)) = t_m(u_m (v))$, where $s$ is a substitution
    \item[Explanations do not get forgotten] $v \in {\mathcal{V}_p}_n \Rightarrow \exists s . s(p_n(v)) = p_m(u_m (v))$.
    \item[Results are results of results] $u_m = u_m (u_n)$.
    \item[Schemes do not get forgotten] $fs_n \subseteq fs_m, cs_n \subseteq cs_m, cf_n \subseteq  cf_m, fd_n \subseteq  fd_m$. Note the lack of applying $u$.
\end{description}

\subsubsection{Unifications}

In the scope of this algorithm we distinguish 4 types of unification. The algorithms for each is the same, but their meaning is different.

\begin{enumerate}
    \item Type unification: we unify two type variables that represent types, we then apply the resulting unification to all type variables considered by the algorithm (this unification translates to the other three types as well). \label{tUni}

    This process ends with adding one of the type variables to the set of forgotten variables $\mcal G$ and adding the mapping from one type variable to the other to the function $u$. It also causes the need to update the variables $\mcal P, \mcal D, \mcal K, \mcal C, b_K, b_C,$ and $\mcal F$ so the algorithm still satisfies the required invariants. And any collisions in these variables has to be followed up by further unifications.

    \item Typing unification: we unify the typings of two input types, we then apply the resulting unification to all other typings considered by the algorithm.  \label{tyUni}

    This updates the variable $\mathcal D$.

    \item Kind unification: we unify type variables that represent two input kinds of certain types, we then apply the resulting unification to all other variables representing kinds considered by the algorithm. \label{kUni}

    This updates the variables $\mcal D, \mcal K, b_K$.

    \item Constness unification: we unify two type variables that represent constnesses of certain types, we then apply the resulting unification to all other variables representing constnesses considered by the algorithm. \label{cUni}

    This updates the variables $\mcal D, \mcal C, b_C$.
\end{enumerate}

\subsection{Type inference algorithm primitives}

\begin{defn}[Fresh variable]
    \label{freshVar}
    An identifier for a fresh variable is chosen according to: \linebreak $\inf \left(\mathbb{V} \setminus \mathcal{V} \setminus \mathcal{G}\right)$. It is given a ``generic'' type kinds, which then is unified with whatever kind the fresh variable is assigned to represent. And the parent is set to $cp$, the current parent.
\end{defn}

\begin{defn}[Algorithm repair steps]
    \label{middlesteps}

    After each step of the algorithm that transforms one of the state variables (mainly $\mathcal{G}$ and $\mathcal{P}$) into a new state, we perform the minimal necessary unifications (possibly recursively):

    \begin{enumerate}
        \item To ``repair'' the injections $d$ and bijection $p$ (intrastate invariants about \emph{type properties} and \emph{type definitions}; this translates into using all four unifications).

        \item To collapse any strongly connected components of the graphs $\mathcal{K}$ and  $\mathcal{C}$ and to remove any duplicate images of trivial bounds from $b_K$ and $b_C$ (intrastate \emph{subtype invariants}, using unifications \ref{kUni} and \ref{cUni}).
    \end{enumerate}
\end{defn}

\begin{remark}[Algorithm repair steps]
    The algorithm may, correctly, fail during the repair steps. This can be shown by the observation \ref{trivBoundsObs}.
\end{remark}

\begin{defn}[Introducing a new (fresh) variable]
    \label{introVar}
    We introduce a new (fresh) variable by extending the sets $\mathcal{V}$ and $\mathcal{D}$
    and the type definition function $d$ by the argument $v$ and its image $i = (typing: v, constness: v, kind: v)$, formally: $\mathcal{V}' = \mathcal{V} \cup \{v\}$, $\mathcal{D}' = \mathcal{D} \cup \{i\}$, $d' = d [v := i]$. Where $v$ is the new (fresh) variable.
\end{defn}
\begin{defn}[Algorithm preparation step]
    \label{presteps}

    All the flat constraints generally use complicated types, before each step of the algorithm, we replace the types in the observed constraint with type variables that represent the types. Storing the structure of each input type $t$ to the variable $\mathcal P$ under the mapping $p$, generating a fresh type variable for each immediate subterm not represented in $\mcal P$, recursively.

    The replacement is performed according to the observation \ref{typesObs}, changing the state variables $\mathcal{P}$, $\mathcal{D}$, $\mathcal{V}$, $p$ and $d$ accordingly. We then, starting from the type variables representing the leafs of the structural tree of the type $t$, update the typing of all newly registered subterms of $t$ to adhere to the intrastate invariant about \emph{typings propagation}, each update just collecting the typings of immediate children to their parent. For the (sub)terms types not yet represented by any type variable, we perform the steps explained in definitions \ref{freshVar} and \ref{introVar}.
\end{defn}

\begin{defn}[Algorithm initialization]
    The initial state is: $\mathcal{V} = \mathcal{G} = \mathcal{P} = \mathcal{D} = \mathcal{K} = \mathcal{C} = \emptyset$, $p = d = u = b_K = b_C = \emptyset \to \emptyset$ and $\mathcal{F}$ is the list of constraints to be satisfied.

    Before the algorithm starts, we introduce the variables encountered in $\mathcal{F}$ according to the definition \ref{introVar} and we update the counter of variables accordingly.
\end{defn}

\begin{algorithm}
    \small
    \caption{\xxx{Main loop}}
    \label{main_loop}
    \begin{algorithmic}
        \Require All state variables of the inference algorithm set to initial values
        \State $\mcal F_p \gets \emptyset$ \Comment {deferred constraints}
        \While {$\mcal{F} \neq \emptyset$}
            \State take an arbitrary constraint $f$ from $\mcal F \setminus \mcal F_p$
            \State perform the preparation step, see the \cref{presteps}
            \State $\mcal F \gets \mcal F \setminus \{f\}$ \Comment {$f$ is speculatively removed}
            \If {$f \equiv t_1 \sim t_2$}
                \State unify $t_1$ and $t_2$ by the unification \ref{tUni}
            \ElsIf {$f \equiv t_1 \sim_T t_2$}
                \State unify the typings $t (d (t_1))$ and $t (d (t_2))$ using the unification \ref{tyUni}
            \ElsIf {$f \equiv t_1 \leq_K t_2$}
                \State add an edge $(k(t_1), k(t_2))$ to $\mathcal{K}$
            \ElsIf {$f \equiv t_1 \leq_C t_2$}
                \State add an edge $(c(t_1), c(t_2))$ to $\mathcal{C}$
            \ElsIf {$f \equiv k_1 \leq_K t \leq_K k_2$}
                \State $b_K \gets b_K[ k(t) := [k_1 \cup k_1', k_2 \cap k_2'] ]$ \textbf{where} $[k_1', k_2'] \gets b_K(k(t))$
            \ElsIf {$f \equiv c_1 \leq_C t \leq_C c_2$}
                \State $b_C \gets b_C[ c(t) := [c_1 \cup c_1', c_2 \cap c_2'] ]$ \textbf{where} $[c_1', c_2'] \gets b_C(c(t))$
            \ElsIf {$f \equiv C?\ \vect t$}
                \If {$C$ has a functional dependency set defined in $fd$}
                    \State $f'$ are constraints derived from $C$ according to $fd$
                    \State $\mcal F \gets \mcal F \cup f'$
                \ElsIf {Exists $s$: $s \equiv (\forall \vect \alpha . A \To C \vect t') \in cf(C), t(\vect t) \sqsupseteq t(\vect t')$}
                    \State instantiate $s$ into $s' \equiv A' \To \vect t$ (or \textbf{report unification failure})
                    \State $\mcal F \gets \mcal F \cup A'$
                \Else
                    \State $\mcal F \gets \mcal F \cup \{f\}; \mcal F_p \gets \mcal F_p \cup \{f\}$ \Comment {$f$ is deferred}
                \EndIf
            \ElsIf {$f \equiv C!\ \vect t$}
                \State $cf(C) \gets cf(C) \cup \{\emptyset . \emptyset \To C \vect t\} $
                \State instantiate $s \equiv (\forall \vect \alpha . A \To C \vect t') \gets cs(C)$ into $s' \equiv A' \To \vect t$
                \State $\mcal F \gets \mcal F \cup A''$, where $A''$ are the constraints $A'$ changed from $X?$ to $X!$
            \ElsIf {$f \equiv t_1 \sqsupseteq_M t_2$}
                \If {$fs(t_2)\ \mtt {is\ defined}$}
                    \State instantiate $s \equiv (\forall \vect \alpha . A \To t') \gets fs(t_2)$ into $s' \equiv A' \To t_1$
                    \State $\mcal F \gets \mcal F \cup A'$
                \Else
                    \State $ \mcal F \gets \mcal F \cup \{f\}; \mcal F_p \gets \mcal F_p \cup \{f\}$ \Comment {$f$ is deferred}
                \EndIf
            \EndIf
            \State perform a repair step, see the \cref{middlesteps}
        \EndWhile
        \State \Return $\mcal F_p$
    \end{algorithmic}
\end{algorithm}

\subsection{Type inference algorithm pipeline}

Because of not supporting nested constraints (see the introduction of this section), we use implication constraints solely for expressing schemes: function schemes, class schemes and class facts. We present the algorithm in a simplified form without considering how these schemes are added from the constraint language to the inferencer state as that is an implementation-specific detail not related to the type theory. For a full picture of the algorithm, it is just important to note that $\mcal F$ contains each of the implication constraints representing an \emph{unclosed function}: a function not yet added to the function schemes $fs$.

The type inference pipeline then consists of the following steps:

\begin{enumerate}
    \item We repeatedly perform the \cref{main_loop} while the resulting list $\mcal F_p$ of deferred constraints does not cover the whole $\mcal{F}$.

    \item If there are any instantiation constraints present in $\mcal F$, we know we have an unclosed function. We collect the instantiation constraints and then we construct an ``instantiation'` graph with edges of the form ``(instantiated, instantiated by)''. We then analyze the topological ordering of this graph, identifying the strongly connected components and relations between them.

    \item We then take the functions in the component which does not have any connected edges with the ``instantiated by'' end outside of the component. And we rewrite the instantiation constraints in these functions refering the other functions into equality constraints. We then continue with the \cref{main_loop}. \label{do_inst_graph}

    \item We then collect all the type variables free in the currently solved component, we minimize the subtypes according to the principle \xxx{ref prev chap}. We translate the resulting subtype variable definitions by suprema back into constraints \xxx{we do not recommend this. Instead, use type-like variable-length constructors that unify by unionization}, and then we collect the leftover constraints on the free variables of the component, we then also include the constraints derived from the type descriptions (namely typing constraints, two-way subkind and subconst constraints derived from identical properties) and subtyping bounds, kind-bound and const-bound constraints. We close the component, generating a scheme for each function of this component, quantifying the free variables and assuming all the collected constraints.

    \item If there are more components in the instantiation graph, we construct a new instantiation graph by removing the currently closed component, we then continue with the step \ref{do_inst_graph}

    \item If there are none, we determine whether there are some unresolved constraints and if there are, we report an error. If there are none, the code is ready to be monomorphized.
\end{enumerate}

\xxx{maybe remove or move to conclusion}. We would like to point out that, ideally, there should be a phase between type inference and monomorphization that would annotate the code with the inferred schemes, which would have an effect of simplifying the the monomorphization implementation and decreasing the space for errors as the ast would be annotated with clear indications whether the singular nodes of ast are polymorphic or monomorphic (the monomorphic nodes would have no free variables, they are monotyped).

Note that, in the included implementation, this is part of the monomorphization phase, so all the primitives of this new layer are already demonstrated.

\section{Monomorphization}

The monomorphization algorithm is a two-phase processes which is performed on type-inferred code with type handles present in an annotation of each node of the AST.

\begin{enumerate}
    \item In the first phase, we go through the top level definitions in the program and identify which ones are monomorphic. We add them to the queue of code to be instantiated. If we encounter any polymorphic functions marked \li{foreign "C"}, we report an error.

    \item We remove the first top level definition to be instantiated from the queue, prepending it to the resulting program, and then we add all references to  definitions not yet instantiated to the desired type to the queue. We repeat this step as long as the queue is not empty or we do not reach some predefined monomorphization limit.
\end{enumerate}

\xxx{add example for monomorphization}

If there was no limit on monomorphization, it could potentially continue forever.

\begin{ex}[non-terminating monomorphization]
    Suppose the following program:

    \begin{lstlisting}
        class C a {
            c() -> auto (a);
        }

        instance C ptr(auto(a)) => C auto (a) {
            c() {
                auto x;
                auto y;

                x = c();
                y = [x];

                return (y);
            }
        }
    \end{lstlisting}

    Then the program is semantically valid, but a successful monomorphization of its use would require an infinite set of subsequent definitions for the instances of c.

    \xxx{An interesting piece of information about this program is that it does not have any uninitialized values even though the variable $x$ cannot be proven to receive any value, yet it passes its value to some instruction.}

\end{ex}

The demonstration algorithm differs from the above description in that it also updates type handles \xxx{ref to preprocessing and inference} of each node of the currently instantiated top level definition. It also separates the queue into waves. The zeroth wave consists of monomorphic the top level definitions, then the first consist of the top levels being instantiated from references in the zeroth level, etc. It does not really differ from the approach with the linear queue. \xxx{it really doesn't?}

\xxx{if the AST contained type information with no external state, the monomorphization could be implemented via generics.}


\subsection{Monomorphization postprocessing}

The monomorphization algorithm does not change the code, it simply copies the top level definitions and changes their type annotations so that they contain the correct monotypes.

\subsubsection{Type hole filling}

This phase reads the type annotations of all types encountered in the monomorphized code and replaces the AST nodes with representations of the inferred types.

The resulting code then contains only explicitly typed declarations (no \li{auto}).

\subsubsection{Name mangling}

This phase renames all functions not marked with the \li{foreign "C"} specialization. According to their types. The name mangling used in this implementation is not very space-efficient as it is designed to be human-readable for easier exploration and debugging.

\begin{defn}[Name mangling]
    The mangled name consists of the \li{_M\$} prefix (standing for ``mangled'') followed by the name and then by the result of applying the mangle function to a type which, for primitive types, returns their name, and for aggregate types, returns the prefix corresponding to the aggregate type followed by the arguments enclosed in \li{\$L} and \li{R\$} (for functions, the arguments and the returns are enclosed separately).
\end{defn}


\xxx{example}

\section{Translation}

The final translation of the code (or code generation) is performed on the postprocessed code with type annotations. It uses \xxx{reference} llvm-hs-pure as a framework for llvm AST generation. The output of translation is then a code interpretable by the low-level virtual machine compiler llc. for an example, see \cref{lst:transl}.

\begin{listing}
    \caption{Example source code before and after the translation to llvm.}
    \label{lst:transl}
    \begin{center}
    %\begin{minipage}[t]{0.5\linewidth}
    \begin{minipage}{0.5\linewidth}
    \begin{lstlisting}
// original code
foreign "C" f(bits32 x,
              bits32 y) {
  x, y = y, x;
  return (x, y);
}
    ...\end{lstlisting}
    \end{minipage}%
    \begin{minipage}{0.5\linewidth}
    \begin{lstlisting}
// translated code
define external ccc
<{i32, i32}> @f(i32  %x_0,
                i32  %y_0) {
LR$procedure0:
  %0 = insertvalue <{i32, i32}>
            undef, i32 %y_0, 0
  %1 = insertvalue <{i32, i32}>
            %0, i32 %x_0, 1
  ret <{i32, i32}> %1
}
    ...\end{lstlisting}
    \end{minipage}
    \end{center}
\end{listing}

Apart from the postprocessed code, it takes three variables from the blockifier:

\begin{enumerate}
    \item Control flow: The edges of the control flow graph
    \item Block data: The mapping from each block's index and variable name relevant in the context of the corresponding block to a two-valued tuple \linebreak \li{(writes, requests alive)} where each entry is a boolean
    \item Blocks table: The mapping from each block's index to its name so we can use the blocks' actual names in the generated code.
\end{enumerate}

The translator first restructures the code so that all the struct definitions (since they do not ever call any procedures) are at the top and the procedures are at the bottom. Then it collects the definitions of all structs and represents them in the pairs \li{([(field_alias, field_index)], [field_type])}. The second value in each pair corresponds to how llvm-hs-pure represents the structures, while the first maps each field accessor to the corresponding index, again, usable when generating the relevant LLVM instructions. After the translator collects all the struct definitions, it starts translating the procedures:

\begin{algorithm}
    \caption{Procedure translation}
    \label{trans_proc}
    \begin{algorithmic}[1]
        \State $r \gets \emptyset$ \Comment{mapping from names to stack-allocated objects}
        \State Open the entry basic block
        \State Generate operands for all formal arguments
        \State $f \gets$ a mapping from variables, initially just formals, to their operands
        \For{datum $d \in$ \li{stackdata} declaration $in$ all procedure's declarations}
            \State Generate \li{alloca} for $d$ resulting in $d'$
            \State $r \gets r[a := d']$ for each alias $a$ of $d$. \Comment{add the appropriate pairs to the $r$ mapping}
        \EndFor
        \State break the list of body statements into basic blocks $B$ according to annotations left by the blockifier
        \State from $B$, take the entry block $E$ and the rest $R$
        \State generate $ph$ a mapping of placeholder operands for each record in the block data \xxx{see ...} without the entry block such that \li{writes} or \li{requests alive} is set
        \State translate the statements (\cref{transl_stmt}) in the entry block $E$, each given $f$, while updating the mapping $f$ accordingly after each statement translation
        \For{block $b \in R$}
            \State translate the block using \cref{transl_block} with the index of the basic block, list of its statements, $r$, $f$ and $ph$
        \EndFor
        \State Collect the \textbf{return}ed mappings from each block into $exports$
        \State Create a mapping $s$ from the placeholders in $ph$ to their corresponding images in $exports$
        \State use $s$ to rewrite all placeholders in all phi nodes generated in each block in $R$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{Statement translation}
    \label{transl_stmt}
    \begin{algorithmic}[1]
        \Require $r$ \Comment{mapping from names to stack-allocated object operands}
        \Require $vars$ \Comment{mapping from variables to operands}
        \Require $s$ \Comment{the statement}
        \If {$s$ is a label}
            \State open a new LLVM basic block
        \Else
            \State transcribe the semantics of $s$ into LLVM
            \State \Comment {for any write to a variable, update $vars$ accordingly}
            \State \Comment {for any reference, look up its corresponding operand in $vars$ or $r$, and if it is not present in either, generate an operand refering a global object}
        \EndIf
        \Return $vars$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{Block translation}
    \label{transl_block}
    \begin{algorithmic}[1]
        \Require $idx$ \Comment{the index of the basic block}
        \Require $r$ \Comment{mapping from names to stack-allocated object operands}
        \Require $f$ \Comment{mapping from entry block's variables to operands}
        \Require $ph$ \Comment{mapping from \li{(blockIdx, varName)} to placeholder operands}
        \Require $B$ \Comment{the list of consecutive statements of the basic block}
        \State translate the first statement (\cref{transl_stmt}) of $B$ \Comment{opens a LLVM basic block, variables not relevant here}
        \State $vars \gets \emptyset$ a mapping from variable names to their corresponding operands
        \For {$v$ a variable requested alive}
            \State generate a phi node $\varphi$
            \For {$(idx', idx)$ in the control flow graph}
                \State add an operand from $f$ (if the corresponding block $idx'$ is the entry block) or $ph$ (otherwise) labeled by the name of the block $idx'$
            \EndFor
            \State $vars \gets vars[v := \varphi]$ \Comment{add the phi node to the $vars$ mapping}
        \EndFor
        \State translate the statements of $B$ (\cref{transl_stmt}), starting from the second, each given $vars$, while updating $vars$ accordingly after each statement translation
        \State \Return $vars$
    \end{algorithmic}
\end{algorithm}


\section{Interaction with external calling conventions}

The program interacts with the external world through functions marked with the \li{foreign "C"} specification. This makes a function follow the "C" calling convention. In the context of type system, it puts a constraint on all arguments and return values that they have to be runtime values (constraint on their constness dimensions) and their data kinds are constrained by the specifications of the call convention.
