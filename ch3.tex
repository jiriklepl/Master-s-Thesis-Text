\chapter{Design of the prototype compiler}

\label{chap3}

\begin{figure}
\caption{Overview schema of the architecture of the prototype compiler.}
\label{fig:arch}
    \includegraphics[width=\linewidth]{img/out/arch.pdf}
\end{figure}

\section{Overview}

In this chapter, we demonstrate how we implemented the prototype compiler so it supported the introduced language and type system extensions we discussed in \cref{chap2} and translated the source code into a form interpretable by the LLVM backend for correct programs, while providing specific error messages to programs that are recognized as malformed due to various reasons (ranging from syntax errors to type errors).

The overall architecture of the prototype implementation is shown in \cref{fig:arch}. The data flow diagram shows the transformation of the input through the components (depicted by light blue rectangles) of the compiler, each somehow transforming the intermediate representation (AST; depicted by light green parallelograms) and/or generating some error information (depicted by, slightly darker, orange parallelograms), into the resulting LLVM representation.

We will describe the purpose and the function of each of the components of the compiler pipeline separately. The description and introduced primitives of each component should be the main takeaway from this chapter rather than the specific implementation itself.

\section{Flattening and blockifying}

Flattening and blockifying phase convert a possibly nested \cmm source code into code with no nesting that is separated into simple blocks of continuous execution (corresponding to the LLVM basic blocks as defined by \cite{llvmBB}). These blocks form a semi-sequential representation of the code organized in a graph structure that reflects the program's control flow. Later, this allows for a straightforward transcription to the SSA form of LLVM.

\begin{remark}
    It is unusual to perform flattening and blockifying before type inference, but it is necessary as these two steps generate calls to the \li{drop} function for each resource object (see \cref{RAII}) and these calls' types then have to be type-inferred in order to perform the desired resource management actions.
\end{remark}

LLVM basic block is defined as a sequence of consecutive (non-terminating, if not malformed) statements that starts with a label (or an entry point of a procedure) and ends with a `terminator' statement, such as a branch, a return, an explicitly marked unreachable statement, or an implicit fallthrough to another basic block. Notably, basic blocks can not contain labels other than the initial ones.

For \cmm, we work with even more restricted definition of a basic block that only allows branch statements at the end of the basic blocks and we transform the code to specifically this form.

\subsection{Flattening}

Flattening phase transforms all nested statements into trivial single-command statements that are simpler to convert into LLVM basic blocks.

An example of the flattening phase transformation is shown in \cref{lst:flat}.

\begin{listing}
\begin{center}
%\begin{minipage}[t]{0.5\linewidth}
\begin{minipage}{0.5\linewidth}
\begin{lstlisting}
// original code
while:
if w != 0 {
    r = w;
    w = u % w;
    u = r;
goto while;
}
...\end{lstlisting}
\end{minipage}%
\begin{minipage}{0.5\linewidth}
\begin{lstlisting}
// flattened code
while:
if w != 0 {
    goto F_then_1;
} else {
    goto F_fi_1;
}
F_then_1:
r = w;
w = u % w;
u = r;
goto while;
goto F_fi_1;
F_fi_1:
...\end{lstlisting}
\end{minipage}
\end{center}
\caption{Example source code before and after the flattening phase.}
\label{lst:flat}
\end{listing}

Flattening moves the nested bodies of each \li{if} statement to the same level as the \li{if} statement, replacing them with a single \li{goto} statement and adding other \li{goto}s where needed to preserve the semantics. Deeply nested code is flattened recursively. For consistency in subsequent processing, it also generates an explicit \li{goto} for each fallthrough.

The resulting code structure can be directly transcribed into LLVM (see \cref{translation}). After flattening, each of \li{if} statements' nested bodies (notably, else body is always present) consist of just a single branch expression. This makes the \li{if} statements directly related to LLVM conditional branches. We perform the transformation similarly for \li{switch} statements. Although \cmm does not specify any other control flow statements with nested bodies, the flattening would be applicable even for \li{while} statements and other, more complex conditional execution.

After transforming all function bodies into flat forms, the flattening phase also reorders the bodies of functions so that all declarations are at the top. This is not necessary, but it improves the readability of the generated code for programmers, and also simplifies the implementation of several other parts of the compiler pipeline. Note that the order of declarations in procedure bodies is irrelevant and the names are visible throughout the whole procedure \cite{ramsey2005c}.

\subsubsection{Filling in calls to the \li{drop} function}

One more task for the flattening phase in the scope of this programming language is that it studies the presence of any resource-representing objects (stack allocated objects carrying \li{new} specification; see \cref{RAII}) and puts calls to the \li{drop} function accordingly to the function's exit points. This is not a usual goal for the flattening phase, but it fitted its algorithm well enough, so there was no necessity for an extra phase. In the case of further similar extensions, we advise considering separation of such concerns into more phases.

\subsection{Blockifying}
\label{sec:blockify}

The blockifying phase follows the flattening phase and it annotates each statement of functions with block annotations while collecting some further information we will describe shortly. The annotated code can be seen in \cref{lst:blockified}.

\begin{defn}[Block annotations]
    Block annotations, given to each statement of a procedure, state whether the statement:

    \begin{enumerate}
        \item starts a basic block, given by index
        \item is part of a basic block given by index
        \item is not a part of any basic block (and for such, generates warnings).

        A statement is not a part of any basic block, for example, when it follows a \li{goto} statement and is not preceded by any label.
    \end{enumerate}
\end{defn}

\begin{listing}
    \caption{An example of a code annotated by blockifying}
    \label{lst:blockified}
    \begin{lstlisting}
while: // starts 1
if x == 0 { goto _then; } // p. of 1; reads "x"; cf += (1, 2)
else      { goto _else; } // ... cf += (1, 3)
_then: // starts 2
x = 5; // part of 2; writes "x"
y = x; // part of 2; writes "y" (does not read "x" before w.)
goto _fi; // part of 2; cf += (2, 4)
_else: // starts 3
x = x + 5; // part of 3; writes "x", reads "x"
goto _fi; // part of 3; cf += (3, 4)
_fi: // starts 4
... // part of 4
    \end{lstlisting}

    The control flow graph contains: $\{(1,2), (1,3), (2,4), (3,4)\}$
\end{listing}

For the identified basic blocks, there is generated an accompanying control flow graph, a blocks table and blocks data.

\begin{description}
    \item[Control flow graph] is a graph on the set of basic blocks of the given procedure with edges specified by branches (represented by \li{goto}s, possibly in \li{if} statements and \li{switch} statements) in pairs $(\mtt{fromIdx}, \mtt{toIdx})$ and a special basic block called \emph{entry block} which begins the procedure and is never branched to.

    \item[Blocks table] is a mapping from each basic block index to its respective name. This allows representing the blocks with unique identifiers while also later allowing them to have the original names in the translated representation.

    \item[Blocks data] is a mapping from each pair \li{(blockIdx, varName)} to a triple \li{(reads before write, writes, requests alive)}. The first two values of the triples are filled in by the blockifier (they are given syntactically) and the last one by the subsequent live-range analysis, which we will describe later in this section.

    This mapping, together with the control flow graph, is then used in translation (\cref{translation}) for rewriting the code into SSA form required by LLVM, more specifically, for generating `phi nodes'.
\end{description}

\emph{Phi nodes} are operands introduced by the \emph{phi} instruction. This instruction takes arguments in \li{(operand, label)} pairs, where the first value of each pair is the operand stored in the phi node during execution in the case the execution was preceded by the basic block specified (using its label) in the second value of the given pair.

\subsubsection{Blockifying algorithm}

Blockifying is performed by reading the code sequentially while annotating the statements with the block annotations, generating the control flow graph, blocks table as described by \cref{alg:blockifying} (for simplicity, we assume just one procedure in the algorithm; for more procedures, assign unique identifier to each entry block as well). This is then followed by live-range analysis, which updates blocks table.

\begin{algorithm}[t]
    \caption{Simplified blockifying algorithm for a single procedure}
    \label{alg:blockifying}
    \begin{algorithmic}
        \Require $S$ \Comment {the list of statements}
        \State $BB \gets \{0\}$ \Comment {the set of basic blocks, containing just the entry block}
        \State $CF \gets \emptyset$ \Comment {edges of the control flow graph}
        \State $\mtt{blocks\_table} \gets \emptyset$ \Comment {a mapping from block names to indexes}
        \State $\mtt{blocks\_data} \gets \emptyset$ \Comment {triples \li{(reads before write, writes, requests alive)}}
        \State $\mtt{current\_block} \gets 0$ \Comment {the current block implicitly set to entry block}
        \State $\forall \mtt{formal\ argument}\ f . \mtt{blocks\_data} \gets \mtt{blocks\_data} [ (0, f) := (\bot, \top, \bot) ]$
        \While {$S \neq \emptyset$}
            \State pop the first statement $s$ from $S$
            \If {$\mtt{current\_block} \neq \emptyset$ }
                \State \Comment {For both, if the record does not yet exist, it is defaulted to $(\bot,\bot, \bot)$}
                \State $\forall v . s\ \mtt{reads}\ v \To \mtt{blocks\_data} [ (\mtt{current\_block}, v) := (r \lor \neg w, w, l) ]$
                \State \quad\quad $\mtt{where}\ (r, w, l) = \mtt{blocks\_data}(\mtt{current\_block}, v)$
                \State $\forall v . s\ \mtt{writes}\ v \To \mtt{blocks\_data} [ (\mtt{current\_block}, v) := (r, \top, l) ]$
                \State \quad\quad $\mtt{where}\ (r, \_, l) = \mtt{blocks\_data}(\mtt{current\_block}, v)$
                \State \Comment {In both, we consider only local variables $v$}
            \EndIf
            \If {$s$ is a label $n:$}
                \State $\mtt{current\_block} \gets \mtt{GetOrAssignFresh}(BB, \mtt{blocks\_table},n)$
            \ElsIf {$s \text{ is a branch with targets } T \land \mtt{current\_block} \neq \emptyset$}
                \State $\forall t \in T: CF \gets \cdots$  (next line)
                \State $CF \cup (\mtt{current\_block}, \mtt{GetOrAssignFresh}(BB, \mtt{blocks\_table},t))$
                \State $\mtt{current\_block} \gets \emptyset$
            \EndIf
            \State \Return $(\mtt{inverted } \mtt{blocks\_table}, \mtt{blocks\_data}, BB, CF)$
        \EndWhile
    \end{algorithmic}
\end{algorithm}

Not shown in the algorithm, blockifier also identifies duplicate declarations (mainly variables and labels), and illegal fallthroughs (fallthroughs to continuation statements and fallthroughs outside of the procedure) and generates errors for such.

\subsubsection{Live-range analysis}


After the blockifier builds the whole control flow graph and blocks data, live-range analysis (originally described by \citet{Aho2006compilers} as live-variable analysis, we use simplified version described by \citet{bednarek2008compilers}) measures live-ranges of all variables, gives errors for uninitialized variables (variables requested alive in the entry block) and warnings for unreachable basic blocks (basic blocks not in the dfs closure generated from the entry block).


\section{Type System}

We implemented the type system as defined in \cref{sec:typesystem}. During the development, we identified many flaws of the practical implementations of the simple original design, which we briefly comment in this section. First, and the most significant flaw, was assuming that the types are equal if they are equal in every subtype dimension and their typing. This fatal flaw was removed from the system, but we include it here as a warning for any attempt at a similar design. \cref{inconsistency} shows an inconsistency caused by such an assumption:

\begin{lemma}Type system from \cref{sec:typesystem} is generally inconsistent if equality of all typing dimensions dimension implies type equality
    \label{inconsistency}
    \begin{proof}
        Let us assume that the types $t$ and $t'$ are equal in every subtype dimension and the typing dimension. That is, $t =_s t'$ for every subtype dimension $s$ and also $t =_T t'$ (see \cref{def:typing,def:stDim}).

        More specifically, if they are type applications $\alpha \beta$ and $\alpha' \beta'$, respectively, then from $t =_T t'$, it also follows that $\alpha =_T \alpha'$ and $\beta =_T \beta'$, but it does not follow that $\alpha =_s \alpha'$ and $\beta =_s \beta'$ for a general subtype dimension. We will offer a counterexample.

        Let us say, $t$ and $t'$ are pointers to functions that take a 32-bit bitvector and return the same type, they are both compile-time constants and the both pointers are stored on the same types of registers (they have the same data kind). This means that $t$ and $t'$ follow the assumptions listed above. But one of them can be constrained to be a pointer to a function that takes an integer and returns an integer, while the other can be constrained to be a pointer to a function that takes a float number and returns a float number (can be shown by a function defined similarly to $id$ known from Haskell). If we assumed that $t = t'$, we would have to solve the constraint $\mtt{float} = \mtt{int}$, where both operands are constants. This would lead to a contradiction as defined by \cref{def:contra}.
    \end{proof}
\end{lemma}

Note that there can be systems with different subtype dimensions, in which the two relations (equality of types, equality of all dimensions of types) can indeed be defined as equivalent. An example of such a system would be a system with no subtype dimensions.

\subsection{Type system limitations}

Consider \cref{congruence}, which describes why subtypes do not commute with type algebra, and \cref{inconsistency}, which describes why we do not consider two types having the same dimensions necessarily the same.

Notice that if we have two function types $t \equiv \alpha \to \beta$ and $t' \equiv \alpha' \to \beta'$ and $t \leq t'$, then the only derivable constraints on $\alpha, \alpha', \beta, \beta'$ are $\alpha \sim_T \alpha'$ and $\beta \sim_T \beta'$. This means that if we assign a reference to a function to a variable, the subtype constraints on its arguments are ``forgotten''. In the prototype implementation, this does not pose any problems as we consider only direct references to functions in call statements. But, In general case, it should be addressed as we describe here:

In \cref{def:stDim}, we introduced that subtype dimension are non-affecting type class resolution. For any implementation, which considers assignments of references to functions, the relationship of $\alpha$ and $\alpha'$ and $\beta$ and $\beta'$ can be ensured by introducing appropriate constraints after deciding the typings according to the desired semantics (for example, $\alpha \leq \alpha' \land \beta' \leq \beta$).

\section{Type Inference}

In the modified \cmm language, we do not support existentials (as discussed in \cref{sys_defer}) and all polymorphic definitions have to be at top level (or they are easily representable as such: definitions of methods and their instances). This allows the solver to solve more eagerly, since there is no need for context levels, deferring the constraints only on two cases (see \cref{main_loop}).

\subsection{Type inference preliminaries}

The basic idea of the implementation of type inference used in the prototype compiler is that each type is represented by a type variable. And the semantic value of this variable is defined by the state of inferencer, structure that controls the inference algorithm.

In the prototype compiler, the type inferencer keeps track of the type definitions of all known types in the terms of a type constructor applied to variables representing each of its immediate subterms (introducing so-called \emph{primitive patterns}). For each type variable, the inferencer then contains a handle, which (in contrast to primitive patterns) stores its fully substituted values of all dimensions (typings stored in a similar form to Algorithm W resulting unifications - see \cite{damas1982principal}, constnesses and data kinds). In addition to these two ``databases'' of \emph{alive variables}, it also keeps the mapping from each variable's original name (as it appears in the AST with so-called \emph{elaborations}, see \cref{preprocessing}). This mapping is necessary as the algorithm performs many unifications of the alive type variables and their corresponding records in aforementioned ``databases'' may not be stored under the original type variables.

The reason behind such intricate design was the necessity to capture the semantics of the various dimensions, most notably the typing dimension (see \cref{def:typing}, focus on the requirement that typings commute with type structure). The primitive patterns ensure that the inference algorithm can model these semantics - specifically type application and type decomposition. In \cref{new_approach}, we introduce a simpler, more practical, design more akin to the design of Deferred Solving described in \cref{defer_solve}. The prototype design can serve as a basis for a theoretic model, which models the inference of the proposed type system.

\subsubsection{Representation of type-related values}

\xxx{fix dis}
\label{def:tyvar}
\label{typing_gram}
\label{def:primpat}
\label{const_kind_def}
\label{def:properties}

\begin{lang}
  \def\grammarP{0.6}
  \begin{grammar}
    \firstcaseP{\mathbb{T}}{\mathbb{V}}{Variable}
    \otherformP{\left[\mathbb{T}\right] \to \mathbb{T}}{Function (each function takes a certain number of argument and returns a type - usually a tuple)}
    \otherformP{\left[\mathbb{T}\right]}{Tuple (function return type)}
    \otherformP{\mathbb{T}\ \mathbb{T}}{Type application (for structs and for constraints)}
    \otherformP{\mtt{Addr}\ \mathbb{T}}{address type (representing a pointer) }
    \otherformP{\cdots}{Primitive types (such as \texttt{bits\_n})}
  \end{grammar}
  \caption{The language $\mathbb{T}$ of typings and types}
  \label{lang:typing}
\end{lang}

\begin{description}
    \item[Type Variable] Uniquely represents a type inferred by the type inference. Each type variable is represented by an unique integer identifier, its type kind, and a reference to a parent type variable, if it has any.

    We call the space of type variables $\mathbb{V}$.

    \emph{Type variable identifier} is a unique number generated by incrementing a specific counter in the preprocessor state and then later in the inferencer state.

    \emph{Reference to a parent type variable} is a reference to a type variable, which uniquely represents the nested scope in which the variable was generated. The references to parent variables are a generalization of the idea of context levels in the Deferred Inference algorithm (see \cref{defer_solve}). They are used mainly for deciding whether a given type variable is defined in the scope of a certain procedure (this is specific to the prototype implementation, but it demonstrates a possible usefulness of this generalization of a context level).

    We call \emph{alive variables} the variables, which are not renamed to another variable by the algorithm. Renaming is performed every time two distinct variables are unified.

    \item[Typing and types] Typings are the main dimension of each type as described in \cref{sec:typesystem}. Type inference represents them by the space $\mathbb{T}$, which is defined inductively by \cref{lang:typing}.

    We use the same space to specify types.

    \item[Primitive patterns] Primitive patterns $\mathbb{P}$ follow the same grammar as typings (see \cref{typing_gram}) but without the very first grammar case ($\mathbb{V}$) and with type variables ($\mbb{V}$) appearing in the grammar cases in place of typings ($\mbb{T}$) wherever applicable.

    Primitive patterns represent the definition of types broken up into single applications of type constructors.

    \item[Subtype dimensions] The algorithm recognizes two subtype dimensions, \emph{constness dimension} and \emph{data kind dimension}:

    \begin{itemize}
        \item  The \emph{constness dimension}, represented by space of constness constants $\mathbb{C}$ represents the bounds of constraints on when the value of each variable has to be known (upper-bound), and, when it can be known (lower-bound). The constness constants are defined as follows:

\begin{center}\begin{grammar}
  \firstcase{\mathbb{C}}{\mathtt{Regular}}{regular runtime variables}
  \otherform{\mathtt{Linkexpr}}{variables known at linkage}
  \otherform{\mathtt{Constexpr}}{variables known at compilation}
\end{grammar}\end{center}

        We define linear ordering on constnesses:
        $$\bot = \mathtt{Constexpr} < \mathtt{Linkexpr} < \mathtt{Regular} = \top$$

        \item The \emph{data kind dimension} represents the bounds of constraints on on which registers (register types) the variables can be stored. For the types in returned tuples, for example, this can specify whether the variable is integral or a floating point number.

        $\mathbb{K}$ is the space of data kinds, each data kind is a subset of the power set of the set of registers on the given architecture. The kinds are ordered by the relation of being a subset. The concrete specification of its constants is architecture-specific.
    \end{itemize}

    \item[Type properties] Type properties represent the various dimension of a type. The functions that project objects from $\mbb{D}$ to the respective properties are called $t$, $c$ and $k$.

    $\mathbb{D}$ is the space of type properties given as tuples:
    $$(typing: \mathbb{T}, constness: \mathbb{V}, kind: \mathbb{V})$$


    \item[Bijection between type variables and primitive patterns]
    A bijective mapping from type variables to primitive patterns is used to:

    \begin{itemize}
        \item reconstruct a type represented by a variable (recursively)
        \item lookup the variables representing each subterm of a type
        \item lookup the variable which represents the given type
    \end{itemize}

    Keeping a consistent mapping of primitive patterns is quite costly, but it allows for many experiments in type inference and it can also help in debugging (it allows for lookup of subterms of a given type, all represented by type variables).
\end{description}

\subsubsection{Type reconstruction}

\emph{Type reconstruction} is evaluated by substituting primitive patterns for type variables iteratively until no type variable is possible to be substituted with a primitive pattern. The algorithm starts with a single type variable.

\begin{lemma}[Primitive patterns]
    \label{typesObs}
    Every type $t$ can be equivalently represented by a type variable, by its primitive pattern (if it has any), and by its reconstructed type acquired by type reconstruction.

    \begin{proof}
        For types, the theory assumes that  $\alpha \beta = \alpha' \beta' \GetsTo \alpha = \alpha' \land \beta = \beta'$  (generalized to all constructors listed in \cref{lang:typing}). And then $t = \alpha \beta \land t' = \alpha' \beta' \Leftrightarrow t = t'$. The above statement is a direct consequence of these assumptions; for reconstruction, shown by induction.
    \end{proof}
\end{lemma}


\subsubsection{Modifications to the constraints language of deferred inference}

$\mbb{W}$ Stands for a flat constraint or an implication constraint similarly to \cref{def:defer_constr}, the space of flat constraints $\mathbb{F}$ is defined inductively by a grammar as \cref{lang:flat}.

\begin{lang}
  \caption{The language $\mathbb{F}$ of flat constraints and their basic semantics.}
  \label{lang:flat}
  \def\grammarP{0.6}
  \def\arraystretch{1.4}
  \begin{grammar}
    \firstcaseP{\mathbb{F}}{\mathbb{T} \sim \mathbb{T}}{Equality constraint (the types are identical)}
    \otherformP{\mathbb{T} \sim_T \mathbb{T}}{Typing equality (the types are identical in the typing dimension)}
    \otherformP{\mathbb{T} \leq_K \mathbb{T}}{SubKind (the right-hand operand has more general data kind than the left-hand operand)}
    \otherformP{\mathbb{T} \leq_C \mathbb{T}}{SubConst (the right-hand operand has more general constness than the left-hand operand)}
    \otherformP{\mathbb{K} \leq_K \mathbb{T} \leq_K \mathbb{K}}{Kind bounds (the type has a data kind from the given range)}
    \otherformP{\mathbb{C} \leq_C \mathbb{T} \leq_C \mathbb{C}}{Const bounds (the type has a constness from the given range)}
    \otherformP{C?\ \mathbb{T}^\ast}{Class constraint (the given types have to satisfy the given constraint - requires an instantiation of a proof)}
    \otherformP{C!\ \mathbb{T}^\ast}{Class fact (the given types are known to satisfy the given constraint - provides the proof to $C?\ \mathbb{T}^\ast$)}
    \otherformP{\mathbb{T} \sqsupseteq_M \mathbb{T}}{Instantiation constraint (the left-hand operand is a monotype instance of the polytype right-hand operand)}
  \end{grammar}
\end{lang}

\pagebreak % TODO: remove this as needed

In the context of constraints, we use some specific constructs and naming:
\begin{description}
    \item[Derived constraints] Sometimes we use derived constraints like $\mathbb{T} \leq_K \mathbb{K}$, $\mathbb{T} =_K \mathbb{T}$, etc. These are either special cases of the aforementioned constraints, or compositions of multiple constraints. The most complicated derived constraint we use is the \emph{SubType} constraint, $\mathbb{T} \leq \mathbb{T}$, which is a combination of $\mathbb{T} \sim_T \mathbb{T}$, $\mathbb{T} \leq_K \mathbb{T}$ and $\mathbb{T} \leq_C \mathbb{T}$.

    \item[Impossible constraints] It should be noted that, within the defined language, we cannot state (or anyhow derive) the constraints $\mathbb{T} \not\sim_T \mathbb{T}$, $\mathbb{T} \not\leq_K \mathbb{T}$, $\mathbb{T} \not\leq_C \mathbb{T}$ and $\mathbb{T} \not\sqsupseteq_M \mathbb{T}$ and thus these (and similar ones) are not considered derived constraints.

    \item[Bounds] We call the right-hand operand of a bounds constraint the upper-bound and the left-hand operand the lower-bound.

    \item[Trivial bounds] If a constraint $F$ states $k_1 \leq_K \tau \leq_K k_2$ and $k_2 \leq k_1$, or $c_1 \leq_C \tau \leq_C c_2$ and $c_2 \leq c_1$, we call such a bounds constraint trivial.

    If the bounds of a trivial bounds constraint are equal, the type constnesses of the types constrained by such a constraint have to be the same and we can unify them.

    If, for a bounds constraint, the upper-bound is lower than the lower-bound, the constraint cannot be satisfied. We mark it an \emph{absurd bounds constraint} and we include it in an error report.

    \item[Functional dependency set] We define a space $\mbb {FD}$ of functional dependency sets $FD$.

    Functional dependency set $FD$ on a sequence of type variables $V \in 2^{\mbb V}$ is a set of rules, each a mapping $V \to \{\mtt {FROM}, \mtt {TO}, \mtt {UNDEFINED}\}$. This set of rules always contains a \emph{trivial rule} $r_0$ such that $\forall v \in V . r_0(v) = \mtt{FROM}$, this trivial rule is implicit and can be omitted when defining a functional dependency set. If a constraint $C \vect \alpha$ (where $\vect \alpha = V$) follows a functional dependency set $FD$ (this is always explicitly specified), then, for each proof $C! \vect t$ (for simplicity, written without any assumptions; otherwise, include them appropriately in the following) of this constraint and for every corresponding rule $r \in FD$, we generate a new proof of the form $\forall \{v \pipe v \gets \vect \alpha, r(v) = \mtt {TO}\} . \{v \sim t \pipe (v, t) \gets (\mtt {zip}\ \vect \alpha\ \vect t), r(v) = \mtt {TO} \} \To C_r! [\mtt {if}\ r(v) = \mtt {FROM}\ \mtt {then}\ t\ \mtt {else}\ v \pipe (v, t) \gets (\mtt {zip}\ \vect \alpha\ \vect t), r(v) \neq \mtt {UNDEFINED}]$ (informally: the \li{TO} types in the proof are replaced with type variables, which are then unified with the original types in the assumptions of the proof, the type variables are then added to the quantification of the proof; we borrowed haskell syntax for more succinct formal definition). The generated proofs then replace the proof $C \vect t$.

    We then, similarly to proofs, replace each constraint $C? \vect t$ with constraints $C_r? \vect t_r$ for each corresponding rule $r$.

    The written notation for defining a functional dependency set $FD$ for a certain class is a list of statements of the form $\vect \alpha_1 \to \vect \alpha_2$ where $\vect \alpha_1, \vect \alpha_2 \subseteq V$, $V$ is the set of variables of the corresponding class. Each such statement then corresponds to a new rule $r$, where $r(v) := \mtt {FROM}$ if $v \in \alpha_1$, otherwise if $v \in \alpha_2$, then $r(v) := \mtt {TO}$, for each variable $v \in V$. $FD := FD \cup \{r\}$

\end{description}

\subsection{Type inference preprocessing}
\label{preprocessing}

The type inference preprocessing, also called \emph{constraint generation}, takes the AST representation of the source code as its input, annotates each node with \emph{elaborations}, and for each of these elaborations, generates constraints (described in \cref{lang:flat}) capturing the semantics of the code. We will not describe the whole process in detail as it is language-specific and the cases, albeit simple, are quite numerous. An example of the result of this process is demonstrated by \cref{lst:elab}.

\begin{listing}
    \caption{Generated elaborations and constraints for a call statement}
    \label{lst:elab}

    \begin{lstlisting}
x = f(a->b); // the considered call statement
    \end{lstlisting}

    Elaborations for subexpressions of the call statement:
    \begin{itemize}
        \item \li{x} elaboration: $v_1$

        \item \li{f} elaboration: $\mtt instance\ v_2 v_f$ (pair instance scheme)

        \item \li{a} elaboration: $v_3$

        \item \li{a->b} elaboration: $\mtt member\ v_4 v_5 v_{getB}$ (triple return instance scheme)
    \end{itemize}

    The generated constraints (for a version without subtyping, replace all $\leq$ with $=$):
    \begin{itemize}
        \item \li{f}: $\{v_2 \sqsupseteq_M v_f\}$ ($v_f$ is the variable representing the function scheme)

        \item \li{a}: $\emptyset$

        \item \li{a->b}: $\{v5 \sqsupseteq_M v_{getB}; v_5 \sim v_8 \to v_9; v_3 \leq v_8; v_9 \leq v_4\}$ (describes the field accessor and two assignments)

        \item \li{x = f(a->b)}: $\{v_2 \sim v_6 \to v_7; v_4 \leq v_6; v_7 \leq v_1\}$ (describes the function and two assignments)
    \end{itemize}
\end{listing}

The output of type inference preprocessing is the list of constraints and the \emph{elaborated AST}. The list of constraints is then given to the type inference algorithm and after that the inferred type assignments stored in the state of the inferencer are paired with the annotated AST, resulting in AST with type annotation.

\begin{description}
    \item[Elaborations] Elaborations are simple structures that contain zero, one or multiple type variables depending on the type of the elaboration (zero if the node has no type semantics). They help to distinguish, for example, whether an named reference is an instantiation of some polymorphic object (as can be seen in \cref{lst:elab}).

    \item[Fresh type variable] A fresh type variable is a type variable (see \cref{def:tyvar}) with the given kind and parent and with a new unique identifier (in our implementation, chosen by a specific counter).
\end{description}

\subsubsection{Type inference preprocessing algorithm}

\begin{enumerate}
    \item When entering each context (unit, struct, class, instance, function), the type inference preprocessor calls the variable collector. This collector then reports all the newly declared variables inside this context back to the preprocessor.

    \item The preprocessor then generates fresh type variables for these variables and uses them whenever they appear in the currently preprocessed code.

    \item When preprocessing a piece of code which contains nontrivial subnodes (sub-expressions, sub-statements, etc.), these subnodes are preprocessed first, then the algorithm retrieves their elaborations and adds the constraints defined by the semantics of the piece of code to the current list of constraints, often generating more fresh type variables.

    \item The constraints are, during the constraint generation, represented by a list of lists, as each polymorphic context nesting in the code also nests the context of constraints. The decision for a list of list is not completely necessary (the nesting is always finite, maximal for instance definitions), but it makes the definition easy to use and generalizable to deeper nestings.

    Closing an nested code context pops the first element from this list of lists and generates an implication constraint to the new head.
\end{enumerate}

\subsection{Type inferencer state}
\label{sec:istate}

During the inference, the algorithm maintains a relatively complex representation of 2 mappings and 2 graphs that represent the relationships between the type variables:

\begin{description}
    \item[Result renaming:]a mapping from variables \emph{forgotten} (due to unification) to their \emph{alive} representants
    \item[Primitive patterns] a bijective mapping from type variables that represent types to the primitive patterns that define the types using their immediate subterms of the respective type
    \item[SubConsts] a graph with variables as vertices where directed edges represent constness inequalities
    \item[SubKinds] a similar graph for data kinds
\end{description}

We represent the 4 structures along with the input constraints and some other information about each type, by a set of variables defined in \cref{tab:istate}.

\begin{table}
\scriptsize
\def\arraystretch{1.3}
\begin{tabular}{p{.23\linewidth}p{.7\linewidth}}
\toprule
State variable & Definition \\
\midrule
  $\mathcal{F} \in \mathbb{W}^\ast$ & \textbf{Constraints}\quad
  The complete list of unresolved constraints. In the initial state, it contains the constraints generated by the preprocessor. In subsequent states, it also contains constraints that are derived from various resolving rules. The algorithm fails if it cannot resolve any of the currently unresolved constraints (or encounters a contradiction), and succeeds if $\mathcal{F}$ becomes empty. \\

  $\mathcal{V} \subset \mathbb{V}$ & \textbf{Active type variables}\quad
  Contains the type variables ever encountered by the algorithm and not present in $\mcal G$. The initial value is therefore $\emptyset$. The algorithm then, when generating a \emph{fresh variable}, uses the smallest possible $v$ such that $\forall v' \in \mcal V. v > v'$. (The implementation actually represents this set via the mapping $u$ and a single integer value.) \\

  $\mathcal{G} \subset \mathbb{V}$ & \textbf{Forgotten type variables}\quad
  This set contains all type variables that were in $\mcal V$ in some past state of the algorithm, but are not in the current state. As above, this state variable is not present in the implementation, but helps describing some invariants. \\

  $\mathcal{P} \subset \mathbb{P}$ & \textbf{Primitive patterns}\quad
  This set contains all type definitions separated encountered or generated by the algorithm separated structurally by type constructors, see \cref{def:primpat}. \\

  $\mathcal{D} \subset \mathbb{D}$ & \textbf{Type properties}\quad
  This set contains three-valued tuples containing the properties of each type, see \cref{def:properties}. \\

  $\mathcal{K} \in \text{Directed graphs on}\ \mathbb{V}$ & \textbf{SubKinds}\quad
  Directional graph that stores the encountered and determined inequalities between data kinds of pairs of type variables. \\

  $b_K \in \mathbb{V} \to \text{interval} \left[\mathbb{K}, \mathbb{K}\right]$ & \textbf{KindBounds}\quad
  Maps each type variable representing the data kind of some types to the known bounds of this data kind. \\

  $\mathcal{C} \in \text{Directed graphs on}\ \mathbb{V}$ & \textbf{SubConsts}\quad
  Similar to $\mcal{K}$, but for constnesses. \\

  $b_C \in \mathbb{V} \to \text{interval} \left[\mathbb{C}, \mathbb{C}\right]$ & \textbf{ConstBounds}\quad
  Similar to $b_K$, but for constnesses. \\

  $p \in \mathcal{V}_p \to \mathcal{P},\,\mathcal{V}_p \subseteq \mathcal{V}$ & \textbf{Type definition}\quad
  Mapping between type variables with known (encountered or derived) definitions and their respective definitions in terms of a type constructor applied to immediate subterms (see \cref{def:primpat}). \\

  $d \in \mathcal{V} \to \mathcal{D}$ & \textbf{Type properties}\quad
  Mapping between all type variables and their respective properties. \\

  $u \in \mathcal{G} \to \mathcal{V}$ & \textbf{Result renaming}\quad
  Substitution between forgotten type variables and active type variables resulting from type variables being unified. \\

  $cp \in \mathcal{V}$ & \textbf{Current parent}\quad
  The parent of a currently solved context. \\

  $fs \in \mathcal{V}_f \to \mathbb{W},\,\mcal V_f \subseteq \mcal V$ & \textbf{Schemes}\quad
  The schemes of the closed functions (regular functions, methods, their instances, field accessors) \\

  $cs \in C_{cs} \to \mathbb{W}^\ast,\,C_{cs} \in 2^{\mbb C}$ & \textbf{Class schemes}\quad
  The schemes of the known classes, each of the form $\forall \vect \alpha . F \To C?  \vect \alpha$, where $C?  \vect \alpha$ is a class constraint and $F \in {\mbb F}^\ast$ is a list of superclasses \\

  $cf \in C_{cf} \to \mathbb{W}^\ast,\,C_{cf} \subseteq C_{cs}$ & \textbf{Class facts}\quad
  The known proofs for each class constraint. Each proof has the form $\forall \vect \alpha . F \To C!\ \vect t$, where $C!\ \vect t$ is a class fact and $F \in {\mbb F}^\ast$ is the list of assumptions of the corresponding proof. \\

  $fd \in C_{fd} \to \mathbb{FD},\,C_{fd} \subseteq C_{cs}$ & \textbf{Functional dependencies}\quad
  Functional dependency sets for each class constraint. They are stored after acquiring a simplified form (for this, see $\mtt{FunDeps.hs}$ in \cite{klepl2022compiler}; not relevant to the type system, but might be of interest) with subsequent addition of the implicit trivial rule. \\
\bottomrule
\end{tabular}
\caption{Variables maintained as the internal state of the inference algorithm (refer to \cref{sec:istate} for details).}
\label{tab:istate}
\end{table}

By $<_{\mathcal{K}}$ and $<_{\mathcal{C}}$ we understand that there is a directional path between the given distinct operands in the graphs $\mathcal{K}$ and $\mathcal{C}$, respectively. And we extended this notation the other comparison operations --- for example, we say $x =_{\mathcal{K}} y$ if there are paths back and forth between $x$ and $y$ in the graph $\mathcal{K}$, or if $x = y$.


\begin{defn}[Simplified notation]
    We extend the functions $b_K$ and $b_C$ to type properties in such a way that for an arbitrary type variable $v \in \mathcal{V}$, we have $b_K (d (v)) = b_K (k (d (v)))$, and similarly for $b_C$. In other words: applying $b_K$ to a type's properties is equivalent to applying it to the type's kind (and similarly for $b_C$ and constness).

    We extend the functions $p$ and $d$ in such a way that for an arbitrary type variable $v \in \mathcal{V}_p$ it holds that $d(v) = d(p(v))$ and $p(v) = p(d(v))$.

    And finally, we extend the functions $t, k, c$ in such a way that for an arbitrary $v \in \mathcal{V}$ it holds that $t (v) = t (d (v))$, $k (v) = k (d (v))$, and $c (v) = c (d (v))$.
\end{defn}

\subsection{Type inferencer state invariants}
\label{sec:invariants}

\subsubsection{Intrastate invariants}

The prototype type inference algorithm is designed to have the following intrastate invariants (the list is nonexhaustive, these will be used in various descriptions of the algorithm's steps):

\begin{description}
    \item[Active and forgotten variables]
    Active type variables $\mcal V$ contain all type variables that represent the current state and they do not overlap with forgotten variables $\mcal G$.

    $$Var(\mathcal{F}) \cup Var(\mathcal{P}) \cup Var(\mathcal{D}) \cup Var(\mathcal{K}) \cup Var(\mathcal{C}) = Var(\mathcal{V})$$ $$Var(\mathcal{V}) \cap Var(\mathcal{G}) = \emptyset$$

    \item[Type properties] $\mathcal{V}$ is projected into $\mathcal{D}$  by the type properties function $d$.

    \item[Type definitions] The subset $\mathcal{V}_p \subseteq \mathcal{V}$ and the set $\mathcal{P}$ are bijected by the type definition function $p$ and its inverse $p^{-1}$. Note that there generally are many types with no known definition.

    \item[Subtype invariants] For any type variables $v, v' \in \mathcal{V}$ representing two types:
        \begin{itemize}
            \item Graph $\mathcal{K}$ has no strongly connected components:
            $$k (v) =_{\mathcal{K}} k (v') \To k (v) = k(v')$$

            \item Graph $\mcal C$ has no strongly connected components:
            $$c (v) =_{\mathcal{C}} c (v') \To c (v) = c(v')$$

            \item Trivially bound by the same data kind:

            If $b_K (k (v)) = b_K (k (v')) = [k_1, k_1]$ for some kind $k_1 \in \mathbb{K}$, \\
            then $k (v) = k (v')$.

            \item Absurdly-bound data kinds considered same:

            If $b_K (k (v)) = b_K (k (v')) = [k_1, k_2]$ for kinds $k_1, k_2 \in \mathbb{K}$, $k_2 < k_1$, \\
            then $k (v) = k (v')$ and it is an invalid data kind.

            \item Trivially bound by the same constness:

            If $b_C (c (v)) = b_C (c (v')) = [c_1, c_1]$ for some constness $c_1 \in \mathbb{C}$, \\
            then $c (v) = c (v')$.

            \item Absurdly-bound constnesses considered same:

            If $b_C (c (v)) = b_C (c (v')) = [c_1, c_2]$ for constnesses $c_1, c_2 \in \mathbb{K}$, $c_2 < c_1$, \\
            then $c (v) = c (v')$ and it is an invalid constness.

            \item Data kind bounds propagate transitively:

            If $k (v) <_{\mathcal{K}} k (v')$, \\
            then, for bounds $k_1, k_2, k_3, k_4$ such that $
            \left(\begin{array}{r}b_K (k (v)) = [k_1, k_3] \\ \text{and}\ b_K (k (v')) = [k_2, k_4]\end{array}\right)$, \\
            it holds that $k_1 \leq k_2$ and $k_3 \leq k_4$.

            \item Constness bounds propagate transitively:

            If $c (v) <_{\mathcal{C}} c (v')$, \\
            then, for bounds $c_1, c_2, c_3, c_4$ such that $\left(\begin{array}{r}b_C (c (v)) = [c_1, c_3] \\\text{and}\ b_C (c (v')) = [c_2, c_4]\end{array}\right)$, \\
            it holds that $c_1 \leq c_2$ and $c_3 \leq c_4$.
        \end{itemize}

    \item[Typings propagate with definitions] The typing of the type represented by $v$ follows the typing generated from the typings of the types via which it is defined.

    If a type represented by a type variable $v$ has a known definition (if $v \in \mathcal{V}_p$), \\
    then $t (d (v)) = p(v) \left[ \tau := t (d (\tau)) | \tau \in \mathrm{free} (p (d))\right]$.
\end{description}

\subsubsection{Interstate invariants}

The algorithm is designed to have the following interstate invariants: (we use the indices to distinguish a successor state variable from its predecessor, we will always assume $n < m \in \mathbb{N}$, we then use these indices in all derived functions too; the list is not exhaustive, these are the ones important to consider when studying the algorithm)

\begin{itemize}
    \item Forgotten variables stay forgotten:
    $$v \in \mathcal{G}_n \Rightarrow v \in \mathcal{G}_m$$

    \item Forgotten variables used to be alive:
    $$v \in \mathcal{G}_m \Rightarrow \exists n . v \in \mathcal{V}_n$$

    \item All free variables from constraints are considered by the algorithm:
    $$v \in \free {\mathcal{F}_n} \Rightarrow v \in \mathcal{V}_n$$

    \item Assumed subtype constraints are monotonically restricted: \quad
    \begin{itemize}
        \item For kinds: $v \in {\mathcal{V}}_n \Rightarrow {b_K}_m (k_m(u_m(v))) \subseteq {b_K}_n (k_n(v))$
        \item For constnesses: $v \in {\mathcal{V}}_n \Rightarrow {b_C}_m (c_m(u_m(v))) \subseteq {b_C}_n (c_n(v))$.
    \end{itemize}

    \item Typings do not get forgotten:
    $$v \in {\mathcal{V}}_n \Rightarrow \exists s . s (t_n(v)) = t_m(u_m (v));\quad s \text{ is a substitution}$$

    \item Explanations do not get forgotten:
    $$v \in {\mathcal{V}_p}_n \Rightarrow \exists s . s(p_n(v)) = p_m(u_m (v))$$

    \item Results are results of results: $u_m = u_m (u_n)$.

    \item Schemes do not get forgotten: (note the lack of applying $u$)
    $$fs_n \subseteq fs_m, cs_n \subseteq cs_m, cf_n \subseteq  cf_m, fd_n \subseteq  fd_m$$
\end{itemize}

\subsubsection{Unifications}
\label{unifications}

In the scope of the prototype algorithm we distinguish 4 types of unification. The algorithms for each is the same, but their meaning is different and each of them is followed by some further actions.

\begin{enumerate}
    \item Type unification: we unify two type variables that represent types, we then apply the resulting unification to all type variables considered by the algorithm (this unification translates to the other three types as well). \label{tUni}

    This process ends with adding one of the type variables to the set of forgotten variables $\mcal G$ and adding the mapping from this type variable to the other to the function $u$.

    This is followed by updating the variables $\mcal P, \mcal D, \mcal K, \mcal C, b_K, b_C$ and $\mcal F$. And any collisions in these variables (and occurrences of strongly connected components in $\mcal K, \mcal C$ or trivial bounds in $b_K, b_C$) have to be followed up (recursively) by further unifications so the required invariants are satisfied. Note that any unification of primitive patterns or typings can report failure (by unifying distinct type constructors, or by occurs check - which checks whether a typing variable is unified with a typing it occurs in, if it does, the unification reports a failure).

    \item Typing unification: we unify the typings of two input types (with an occurs check). We then apply the resulting unification to all other typings in considered by the algorithm. \label{tyUni}

    This updates the variable $\mathcal D$.

    \item Kind unification: we unify type variables that represent two input kinds of certain types, we then apply the resulting unification to all other variables representing kinds considered by the algorithm. \label{kUni}

    This updates the variables $\mcal D, \mcal K, b_K$.

    \item Constness unification: we unify two type variables that represent constnesses of certain types, we then apply the resulting unification to all other variables representing constnesses considered by the algorithm. \label{cUni}

    This updates the variables $\mcal D, \mcal C, b_C$.
\end{enumerate}

\subsection{Type inference algorithm routines}

\begin{description}
    \item[\textsc{Fresh variable}] An identifier for a fresh variable is chosen according to: \linebreak $\min \left(\mathbb{V} \setminus \mathcal{V} \setminus \mathcal{G}\right)$. It is given a ``generic'' type kind, which then is unified with whatever kind the fresh variable is assigned to represent. And the parent is set to $cp$, the current parent.

    \item[\textsc{Repair step}] After each step of the algorithm that transforms one of the state variables (mainly $\mathcal{G}$ and $\mathcal{P}$, $\mcal{D}$) into a new state, we perform the minimal necessary unifications (possibly recursively):

    \begin{enumerate}
        \item To ``repair'' the injections $d$ and bijection $p$ (intrastate invariants about \emph{type properties} and \emph{type definitions}; this translates into using all four unifications).

        \item To collapse any strongly connected components of the graphs $\mathcal{K}$ and  $\mathcal{C}$ and to remove any duplicate images of trivial bounds from $b_K$ and $b_C$ (intrastate \emph{subtype invariants}, using unifications \ref{kUni} and \ref{cUni}).
    \end{enumerate}

    This directly relates to subsection \emph{Unifications} in \cref{unifications}, see it for more details.

    The algorithm may, \emph{correctly}, fail during the repair steps. It can determine an absurd bounds constraint or fail by unification errors specified in subsection \emph{Unifications} in \cref{unifications}. This is followed up by generating an error.

    \item[\textsc{Introduction of a new variable}] We introduce a new variable (not necessarily fresh) by extending the sets $\mathcal{V}$ and $\mathcal{D}$ and the type definition function $d$ by the argument $v$ and its image $i = (typing: v, constness: v, kind: v)$, formally: $\mathcal{V}' = \mathcal{V} \cup \{v\}$, $\mathcal{D}' = \mathcal{D} \cup \{i\}$, $d' = d [v := i]$, where $v$ is the new variable.

    \item[\textsc{Preparation step} for a given constraint] All the flat constraints generally use complicated types, before each step of the algorithm, we replace the types in the given constraint with type variables that represent the types. Storing the structure of each input type $t$ to the variable $\mathcal P$ under the mapping $p$, generating a fresh type variable for each immediate subterm not represented in $\mcal P$, recursively.

    The replacement is performed according to the observation \ref{typesObs}, changing the state variables $\mathcal{P}$, $\mathcal{D}$, $\mathcal{V}$, $p$ and $d$ accordingly. We then, starting from the type variables representing the leafs of the structural tree of the type $t$, update the typing of all newly registered subterms of $t$ to adhere to the intrastate invariant about \emph{typings propagation}, each such update just collecting the typings of immediate children (from $\mcal D$), applying appropriate type constructor and updating the parent's record in $\mcal D$. For the (sub)terms' types not represented by any type variable before this step, we generate \textsc{Fresh variable} and then we perform the \textsc{Introduction of a new variable}.

    \item[\textsc{Algorithm initialization}] The initial state is: $\mathcal{V} = \mathcal{G} = \mathcal{P} = \mathcal{D} = \mathcal{K} = \mathcal{C} = p = d = u = b_K = b_C = cp = fs = cs = cf = fd = \emptyset$ and $\mathcal{F}$ is the list of constraints to be satisfied.

    Before the algorithm starts, we perform \textsc{Introduction of a new variable} to introduce each of the variables encountered in $\mathcal{F}$ and we update the counter of variables so the next fresh variable has an unique identifier.
\end{description}



\begin{algorithm}
    \small
    \caption{Solution loop}
    \label{main_loop}
    \begin{algorithmic}
        \Require All state variables of the inference algorithm set to initial values
        \State $\mcal F_p \gets \emptyset$ \Comment {deferred constraints}
        \While {$\mcal{F} \neq \emptyset$}
            \State take an arbitrary constraint $f$ from $\mcal F \setminus \mcal F_p$
            \State perform the \textsc{Preparation step}
            \State $\mcal F \gets \mcal F \setminus \{f\}$ \Comment {$f$ is speculatively removed}
            \If {$f \equiv t_1 \sim t_2$}
                \State unify $t_1$ and $t_2$ by the unification \ref{tUni}
            \ElsIf {$f \equiv t_1 \sim_T t_2$}
                \State unify the typings $t (d (t_1))$ and $t (d (t_2))$ using the unification \ref{tyUni}
            \ElsIf {$f \equiv t_1 \leq_K t_2$}
                \State add an edge $(k(t_1), k(t_2))$ to $\mathcal{K}$
            \ElsIf {$f \equiv t_1 \leq_C t_2$}
                \State add an edge $(c(t_1), c(t_2))$ to $\mathcal{C}$
            \ElsIf {$f \equiv k_1 \leq_K t \leq_K k_2$}
                \State $b_K \gets b_K[ k(t) := [k_1 \cup k_1', k_2 \cap k_2'] ]$ \textbf{where} $[k_1', k_2'] \gets b_K(k(t))$
            \ElsIf {$f \equiv c_1 \leq_C t \leq_C c_2$}
                \State $b_C \gets b_C[ c(t) := [c_1 \cup c_1', c_2 \cap c_2'] ]$ \textbf{where} $[c_1', c_2'] \gets b_C(c(t))$
            \ElsIf {$f \equiv C?\ \vect t$}
                \If {$C$ has a functional dependency set defined in $fd$}
                    \State $f'$ are constraints derived from $C$ according to $fd$
                    \State $\mcal F \gets \mcal F \cup f'$
                \ElsIf {Exists $s$: $s \equiv (\forall \vect \alpha . A \To C \vect t') \in cf(C), t(\vect t) \sqsupseteq t(\vect t')$}
                    \State instantiate $s$ into $s' \equiv A' \To \vect t$ (or \textbf{report unification failure})
                    \State $\mcal F \gets \mcal F \cup A'$
                \Else
                    \State $\mcal F \gets \mcal F \cup \{f\}; \mcal F_p \gets \mcal F_p \cup \{f\}$ \Comment {$f$ is deferred}
                \EndIf
            \ElsIf {$f \equiv C!\ \vect t$}
                \State $cf(C) \gets cf(C) \cup \{\emptyset . \emptyset \To C \vect t\} $
                \State instantiate $s \equiv (\forall \vect \alpha . A \To C \vect t') \gets cs(C)$ into $s' \equiv A' \To \vect t$
                \State $\mcal F \gets \mcal F \cup A''$, where $A''$ are the constraints $A'$ changed from $X?$ to $X!$
            \ElsIf {$f \equiv t_1 \sqsupseteq_M t_2$}
                \If {$fs(t_2)\ \mtt {is\ defined}$}
                    \State instantiate $s \equiv (\forall \vect \alpha . A \To t') \gets fs(t_2)$ into $s' \equiv A' \To t_1$
                    \State $\mcal F \gets \mcal F \cup A'$
                \Else
                    \State $ \mcal F \gets \mcal F \cup \{f\}; \mcal F_p \gets \mcal F_p \cup \{f\}$ \Comment {$f$ is deferred}
                \EndIf
            \EndIf
            \State perform the \textsc{Repair step}
        \EndWhile
        \State \Return $\mcal F_p$
    \end{algorithmic}
\end{algorithm}

\subsection{Type inference algorithm pipeline}

Because of not supporting nested constraints (see the introduction of this section), we use implication constraints solely for expressing schemes: function schemes, class schemes and class facts. We present the prototype algorithm in a simplified form without considering how these schemes are added from the constraint language to the inferencer state as that is an implementation-specific detail not related to the type theory. For a full picture of the prototype algorithm, it is just important to note that $\mcal F$ contains each of the implication constraints representing an \emph{unclosed function}: a function not yet added to the function schemes $fs$.

The type inference pipeline then consists of the following steps:

\begin{enumerate}
    \item We perform the \textsc{Algorithm initialization}

    \item We then repeatedly perform \cref{main_loop} until the resulting list $\mcal F_p$ of deferred constraints covers the whole $\mcal{F}$.

    \item If there are any instantiation constraints present in $\mcal F$, we know we have an unclosed function (at least one). We collect the instantiation constraints and then we construct an ``instantiation'' graph with edges of the form \li{(instantiated, instantiated by)}. We then analyze the topological ordering of this graph, identifying the strongly connected components and relations between them.

    \item We then collect the functions in the component that do not have any connected edges with the \li{instantiated by} end outside of the component. And we rewrite the instantiation constraints in these functions referring the other functions into equality constraints. We then continue with \cref{main_loop}. \label{do_inst_graph}

    \item We then collect all the type variables free in the currently solved component, we minimize the subtypes according to \cref{sec:inferSub}. We translate the resulting subtype variable definitions by suprema back into constraints, and we collect the unsolved constraints on the free variables of the component, we also include, for each free type variable, the constraints derived from the type descriptions (records in $\mcal{D}$) and subtype bounds (edges from/to the type variable in $\mcal K, \mcal C$), kind-bound (from $\mcal b_K$) and const-bound (from $\mcal b_C$) constraints. We close the component, generating a scheme for each function of this component, quantifying the free variables and assuming all the collected constraints. \label{closing}

    This demonstrates the impracticality of keeping information about each type in state variables - this information has to be then translated back into constraints.

    \item If there are more components in the instantiation graph, we construct a new instantiation graph by removing the currently closed component, we then continue with the step \ref{do_inst_graph}

    \item If there are none, we determine whether there are some unresolved constraints and if there are, we report an error. If there are none, the code is ready to be monomorphized.
\end{enumerate}

The algorithm, if implemented entirely, is correct as described in \cref{defer_solve} and \cref{sec:typesystem}. The various variables ($\mcal D, \mcal P, \mcal C, \mcal K, \dots$) represent the input constraints (and derived constraints) in a more centralized, equivalent, form (demonstrated by the step \ref{closing}, which translates this form back into constraints), while the invariants (see \cref{sec:invariants}) ensure the updates to the variables are equivalent to rewriting used in Deferred Solving. The invariants also ensure that the information is updated whenever possible. Note that because of non-nested contexts, there are no skolem constants nor context levels in the algorithm.

\section{Monomorphization}

The monomorphization algorithm is a two-phase processes that is performed on a type-inferred code with elaborations present in an annotation of each node of the AST. It outputs a code, which is generated by a static interpretation of the callgraph (generalized to all references to top-level definitions) of the original code, starting from all monomorphic definitions, for each reference to a polymorphic definition, creating a monomorphic copy of the polymorphic definition, and continuing with references from the copy, recursively. The input and the output of this phase can be seen in \cref{lst:mono}.

The elaborations of the input AST contain directly interpretable type information inferred by the type inference. In the prototype implementation, this interpretation is tied to the inferencer state introduced in \cref{sec:istate}, but we recommend the elaborations be independent on any outside information as described in \cref{new_approach}.

The two phases of monomorphization are:

\begin{enumerate}
    \item We go through the top level definitions in the program and identify which ones are monomorphic. We add them to the queue of code to be instantiated. If we encounter any polymorphic functions marked \li{foreign "C"}, we report an error (note that \li{foreign "C"}-specified definitions are outside-world bindings - callable from code written in the C language, for example; they are required to be monomorphic).

    \item We remove the first top level definition to be instantiated from the queue introduced in the first step, prepending the definition to the resulting program (initially empty). After that, we add all references to  definitions not yet instantiated to the desired types (types of the references) to the queue, the corresponding records consisting of pairs (polytype definition, type of the reference). We repeat this step as long as the queue is not empty or we do not reach some predefined monomorphization limit. \label{steptwo}
\end{enumerate}

\begin{listing}
    \caption{Example of code before and after monomorphization. For the specific ``mangled'' names given to the copies of \li{id}, see \cref{sec:mangling}.}
    \label{lst:mono}
    \begin{center}
    %\begin{minipage}[t]{0.5\linewidth}
    \begin{minipage}{0.4\linewidth}
    \begin{lstlisting}
// original code
id (auto x) {
    return (x);
}

foreign "C"
f(bits32 x){
    x = id(x);
    return (x);
}

foreign "C"
g(bits64 x){
    x = id(x);
    return (x);
}
\end{lstlisting}
    \end{minipage}%
    \begin{minipage}{0.6\linewidth}
    \begin{lstlisting}
// monomorphized code
_Mid_$f$Lb32R$$Lt$Lb32R$R$(bits32 x){
  return (x);
}
_Mid_$f$Lb64R$$Lt$Lb64R$R$(bits64 x){
  return (x);
}
foreign "C" f(bits32 x) {
  x = _Mid_$f$Lb32R$$Lt$Lb32R$R$(x);
  return (x);
}
foreign "C" g(bits64 x) {
  x = _Mid_$f$Lb64R$$Lt$Lb64R$R$(x);
  return (x);
}
\end{lstlisting}
    \end{minipage}

    \end{center}
\end{listing}

Note that if there was no limit on monomorphization (observe step \ref{steptwo}), it could potentially continue forever as seen in \cref{lst:nonTermination}. We usually choose the limit so it is safely bigger than the monomorphization depth of majority of practical programs; this is precedented, for example, by Haskell \cite{haskell2010}.

\begin{listing}
    \caption{Non-terminating monomorphization}
    \label{lst:nonTermination}

    \begin{lstlisting}
class C a {
    c() -> auto (a);
}

instance C ptr(auto(a)) => C auto (a) {
    c() {
        auto x;
        auto y;

        x = c();
        y = [x];

        return (y);
    }
}
    \end{lstlisting}

    The program is semantically valid, but a successful monomorphization of its use would require an infinite set of subsequent definitions for the instances of \li{c}.
\end{listing}

In the prototype implementation, the monomorphization queue is separated into waves. The zeroth wave consists of monomorphic top level definitions, then the first consist of the top level definitions being instantiated from references in the zeroth level, etc. This allows the monomorphization depth be checked by a simple counter (without waves, we can check the monomorphization depth by adding an extra argument to each record in the queue).

\subsection{Monomorphization postprocessing}

In the prototype implementation, the monomorphization algorithm does not change the code, it simply copies the top level definitions and changes their annotations so that they contain elaborations interpreted as the correct monotypes (it does so by applying a substitution from each definition's polytype to the copy's monotype to the elaboration of each node inside the definition).

The monomorphization phase is then followed by two postprocessing phases, \emph{elaboration filling} and \emph{name mangling}, which change the some code depending on its elaboration. These two phases replace type variables appearing in the code to their appropriate types (stored in the corresponding elaborations) and ``mangle'' names of definitions of polytypes and their references. These concerns can be included in monomorphization itself, but implementing them separately turned out to simplify their implementation (note that this is specific to the prototype implementation).

\subsubsection{Elaboration filling}

This phase takes an elaborated and monomorphized AST, reads the type elaborations of all type nodes encountered in the AST, and replaces the nodes with representations of the inferred types. The resulting code then contains only explicitly typed declarations (no \li{auto}).

\subsubsection{Name mangling}
\label{sec:mangling}

This phase takes an elaborated and monomorphized AST. It renames all functions not marked with the \li{foreign "C"} specialization according to their types. The name mangling used in this implementation is not very space-efficient as it is designed to be human-readable for easier exploration and debugging. See \cref{lst:mono} for an example of a mangled function name.

\begin{defn}[Name mangling]
    The mangled name consists of the \li{_M\$} prefix (standing for ``mangled'') followed by the name and then by the result of applying the mangle function to a type. The mangle function, for primitive types (\li{bits32}, etc.), returns their shortened name (for \li{bits32}, \li{b32}, for example), and for composite types, returns the prefix corresponding to the top-most type constructor followed by its mangled arguments enclosed in \li{\$L} and \li{R\$} (for functions, the arguments and the returns are enclosed separately).
\end{defn}

\section{Translation}
\label{translation}

The final translation of the code (or code generation) is performed on the postprocessed monomorphized code with type elaborations and block annotations (see \cref{sec:blockify} and ). We use llvm-hs-pure (see \cite{llvmHSpure}) as a framework for LLVM AST generation and llvm-hs-pretty for subsequent prettyprinting \cite{llvmHSpretty}. The prettyprinted output of translation is then a LLVM assembly code interpretable by the low-level virtual machine compiler \emph{llc}. For an example, see \cref{lst:transl}.

\begin{listing}
    \caption{Example source code before and after the translation to LLVM.}
    \label{lst:transl}
    \begin{center}
    %\begin{minipage}[t]{0.5\linewidth}
    \begin{minipage}{0.5\linewidth}
    \begin{lstlisting}
// original code
foreign "C" f(bits32 x,
              bits32 y) {
  x, y = y, x;
  return (x, y);
}
    ...\end{lstlisting}
    \end{minipage}%
    \begin{minipage}{0.5\linewidth}
    \begin{lstlisting}
// translated code
define external ccc
<{i32, i32}> @f(i32  %x_0,
                i32  %y_0) {
LR$procedure0:
  %0 = insertvalue <{i32, i32}>
            undef, i32 %y_0, 0
  %1 = insertvalue <{i32, i32}>
            %0, i32 %x_0, 1
  ret <{i32, i32}> %1
}
    ...\end{lstlisting}
    \end{minipage}
    \end{center}
\end{listing}

Apart from the postprocessed monomorphized code, translation takes three variables from the \emph{blockifier} (see \cref{sec:blockify}):

\begin{description}
    \item[\textsc{Control flow}] The edges of the control flow graph, for each procedure.
    \item[\textsc{Block data}] The mapping from each block's index and variable name relevant in the context of the corresponding block to a two-valued tuple \linebreak \li{(writes, requests alive)} where each entry is a boolean (note that in \cref{sec:blockify}, we introduced this variable as a triple, we siply strip the first value of each record away).
    \item[\textsc{Blocks table}] The mapping from each block's index to its name, so we can use the blocks' actual names in the generated code.
\end{description}

The translator first restructures the code so that all the struct definitions are at the top and the procedures are at the bottom. Then it collects the definitions of all structs and represents each of them using a pair  \li{([(field_alias, field_index)], [field_type])}. The second value (list of a structure's fields) in each such pair corresponds to how llvm-hs-pure represents the structures, while the first maps each field accessor name to the index of the corresponding field, again, directly usable when generating the relevant LLVM instructions. After the translator collects all the struct definitions, it starts translating the procedures as described in \cref{trans_proc}.

\begin{algorithm}
    \caption{Procedure translation}
    \label{trans_proc}
    \begin{algorithmic}[1]
        \Require $S$ \Comment{List of the procedure's statements}
        \State $r \gets \emptyset$ \Comment{mapping from names to stack-allocated objects}
        \State Open the entry basic block
        \State $f \gets \emptyset$ \Comment{a mapping from variable names to their operands}
        \State $\forall \mtt{formal\ argument}\ \alpha . f \gets f [ \alpha := \mtt{newOperand}]$
        \For {$\mtt{datum}\ d \in$ \li{stackdata \{ ... \}} $\in \mtt{declarationsOfProcedure}$}
            \State Generate \li{alloca} for $d$, resulting in $d'$
            \State $\forall  a\ \mtt{aliasOf}\ d .  r \gets r[a := d']$. \Comment{add the appropriate pairs to $r$ mapping}
        \EndFor
        \State $B \gets \mtt{breakIntoBasicBlocks} (S)$ \Comment{according to blockifier's annotations}
        \State $E,R \gets \mtt {entryAndOtherBasicBlocks}(B)$
        \State $ph \gets$ placeholder operands for each record in the \textsc{block data} (variable from the blockifier) without the entry block such that \li{writes} or \li{requests alive} is set
        \For {$s \in E$}
            \State $f \gets \mtt {translate}(r, f, s)$ \Comment{\cref{transl_stmt}}
        \EndFor
        \State $exports \gets \emptyset$ \Comment{Actual values for the placeholders in $ph$}
        \For{block $b \in R$}
            \State $vars \gets \mtt {translate}(idx(b), r, f, ph, b)$ \Comment{\cref{transl_block}}
            \State $exports \gets exports \cup (idx(b),vars)$
        \EndFor
        \State Create a mapping $s$ from the placeholders in $ph$ to their corresponding images in $exports$
        \State use $s$ to rewrite all placeholders in all phi nodes generated in each block in $R$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{Statement translation}
    \label{transl_stmt}
    \begin{algorithmic}[1]
        \Require $r$ \Comment{mapping from names to stack-allocated object operands}
        \Require $vars$ \Comment{mapping from variable names to operands}
        \Require $s$ \Comment{the statement}
        \If {$s$ is a label}
            \State open a new LLVM basic block
        \Else
            \State transcribe the semantics of $s$ into LLVM
            \State \Comment {for any write to a variable, update $vars$ accordingly}
            \State \Comment {for any reference, look up its corresponding operand in $vars$ or $r$, and if it is not present in either, generate an operand refering a global object}
        \EndIf
        \State \Return $vars$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{Block translation}
    \label{transl_block}
    \begin{algorithmic}[1]
        \Require $idx$ \Comment{the index of the basic block}
        \Require $r$ \Comment{mapping from names to stack-allocated object operands}
        \Require $f$ \Comment{mapping from entry block's variables to operands}
        \Require $ph$ \Comment{mapping from \li{(blockIdx, varName)} to placeholder operands}
        \Require $B$ \Comment{the list of consecutive statements of the basic block}
        \State $l, S \gets \mtt {firstAndOtherStatements}(B)$
        \State $\emptyset \gets \mtt{translate}(r, \emptyset, s)$ \Comment{\cref{transl_stmt}; opens a LLVM basic block}
        \State $vars \gets \emptyset$ \Comment{a mapping from variable names to their corresponding operands}
        \For {$v$ a variable requested alive}
            \State generate a phi node $\varphi$
            \For {$(idx', idx)$ in the \textsc{control flow} graph from the blockifier}
                \If {$idx'$ is the entry block}
                    \State $\varphi \gets \varphi \cup (f(v), \text{\textsc{blocks table}}(idx'))$
                \Else
                    \State $\varphi \gets \varphi \cup (ph(idx', v), \text{\textsc{blocks table}}(idx'))$
                \EndIf \Comment {\text{\textsc{blocks table}} is from the blockifier}
            \EndFor
            \State $vars \gets vars[v := \varphi]$ \Comment{add the phi node to the $vars$ mapping}
        \EndFor
        \For {$s \in S$}
            \State $vars \gets \mtt {translate} (r, vars, s)$ \Comment {\cref{transl_stmt}}
        \EndFor
        \State \Return $vars$
    \end{algorithmic}
\end{algorithm}


\section{Interaction with external calling conventions}

The program interacts with the external world through functions marked with the \li{foreign "C"} specification. This makes a function follow the "C" calling convention. In the context of type system, it puts a constraint on all arguments and return values that they have to be runtime values (constraint on their constness dimensions) and their data kinds are constrained by the specifications of the call convention.
