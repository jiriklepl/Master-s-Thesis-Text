\chapter{Design}

\todo{PROC BC JO ALE PROC ZLEPSIT}

\todo{VRSTVY}

\includegraphics[width=\linewidth]{img/out/arch.pdf}

\section{Flattening and blockifying}

Flattening phase and blockifying phases together are important parts of the algorithm if we are interested either in translating the code into an SSA intermediate form or llvm backend. And at the same time they analyze the control flow of the program.

\subsection{Flattening}

Flattening phase is much simpler and it just transcribes the function bodies and other nested control flow structures into flat forms. Flat forms are characterized by all if statements being of the form:

\xxx{write a trivial if statement}.

This if statements directly relates to conditional branches. And then similarly for switch statements and other similar structures (switch statements would be a little bit more complicated, but we still first want them be in this flattened form). Since \cmm does not have other control flow statements with nested bodies, the principle ends here, but a similar principle would apply to them as well.

It then reorders the bodies of functions so that all declarations are at the very top, this is unnecessary, but it improves the readability of the functions in case of any compiler debugging being necessary and allows for easier implementation of other parts of the compiler pipeline.

One more work for the flattening phase in the scope of this programming language is that it studies the presence of any RAII contracts and puts calls to drop functions accordingly. This is not a usual goal for the flattening phase, but it fitted its algorithm well enough.

\subsection{Blockifying}

The blockifying phase follows the flattening phase and it annotated the nodes of AST with annotations stating whether the node starts a basic block, is part of a certain basic block, or that it is not a part of any basic block (is trivially unreachable). It then gives warnings for encountering these unreachable statements.

Basic blocks are defined as the maximum sequence of consecutive statements such that it starts with a label and ends with a (conditional) branch or a return statement (and apart from these two instances, it contains neither label statements nor any control flow statements), fallthrough to an another basic block is regarded as implicit branch.

It also analyzes illegal fallthroughs (fallthroughs to continuation statements and fallthroughs outside of the procedure).

After the blockifier annotates all basic blocks in a procedure, it then measures live ranges of all variables and gives errors for uninitialized variables. It also finds out unreachable labels (and thus finds nontrivially unreachable code as well).

The state it keeps is usable for translating the code into an SSA form. For each basic block, it keeps the list of the basic blocks it is preceded by in the control flow of the corresponding procedure and for each variable present in the block or alive in its scope, it contains whether the variable is required to be alive at the basic block's entry, whether it is written to, and whether it is being read from.

The SSA form can be trivially generated by a two-phase algorithm where the first phase prepares the relevant phi nodes with placeholders for variables being exported from the preceding basic blocks and then the second phase fills in these placeholders with actual names of the exported variables (they will likely not match the original names of the variables as we have to distinguish between different writes to them).

\section{Flow analysis and how it connects to RAII}

\section{Type System}

We derived the type system from the description in \xxx{ref prev section}. But there were many flaws in the original design. First, and the most significant flaw, was assuming that the types are equal if they are equal in every subtype dimension and their typing.

\begin{ex}[Inconsistency of assuming type equality equivalent to combination of typing equality and subtype equality in all dimensions]
    Let us assume that the types $t$ and $t'$ are equal in every subtype dimension and the typing dimension. That is, $t =_s t'$ for every subtype dimension $s$ and $t =_T t'$ \xxx{according to def. ref here}.

    Then if they are type applications $\alpha \beta$ and $\alpha' \beta'$, respectively, then from $t =_T t'$, it also follows that $\alpha =_T \alpha'$ and $\beta =_T \beta'$, but it does not follow that $\alpha =_s \alpha'$ and $\beta =_s \beta'$ for every subtype dimension. We will offer a counterexample.

    Let us say, $t$ and $t'$ are pointers to functions that take a 32-bit bitvector and return the same type, they are both constant expressions and the both pointers are stored on the same register (they have the same data kind). This means that $t$ and $t'$ follow the assumptions listed above. But one of them can be constrained to be a pointer to a function that takes an integer and returns an integer, while the other can be constrained to be a pointer to a function that takes a float number and returns a float number. If we assumed that $t = t'$, we would have to solve the constraint $\mtt{float} = \mtt{int}$, where both operands are constants.

    Note that there can be systems with different subtype dimensions, in which the two relations can indeed be defined as equivalent. An example of such a system would be a system with no subtype dimensions.
\end{ex}

This fatal flaw was removed from the system.

\section{Inference}



\subsection{Kind inference}

\todo{add text from kinds.tex}

\subsection{Type inference}

The basic idea of the implementation of type inference used in our solution is that each type is represented by a type variable. And the semantic value of this variable is defined by the state of inferencer, structure that controls the inference algorithm.

In our solution, the type inferencer contains keeps track of the type definitions of all known types in the terms of a type constructor applied to variables representing each of its immediate subterms (so called ``primitive patterns''). And for each type variable it contains a handle which stores its subtype dimensions (fully substituted \xxx{?}, contrasting the primitive patterns). In addition to these two databases of ``alive variables'', it also keeps the mapping from each variable's original name (as it appears in annotated AST \xxx{preprocessing}) as the algorithm performs many unifications and substitutions which often causes the variables being renamed.

\begin{defn}[Type variables]
    Each type variable is represented by its identifier, an unique number generated by incrementing a specific counter in the preprocessor state \xxx{ref} and then later in the inferencer state. Each variable then contains an information about the type kind of the type represented by the variable \xxx{ref to type kinds} and a reference to the ``parent variable'', a variable which uniquely represents the nested scope in which the variable was generated. The references to parent variables are a generalization of the idea presented by the deferring inference algorithm  \xxx{ref to deferring inference algorithm}.
    
    We will call the space of defined type variables $\mathbb{V}$.

    We will call the variables not being renamed to another variable by the algorithm ``alive variables''. Renaming is performed every time two variables are to be unified.
\end{defn}

\begin{defn}[Typings]
    Typings are the main dimension of types. They are used to resolve type class instantiations.

    We will call the space of typings $\mathbb{T}$, and every typing is defined in the terms of the following grammar:

    \begin{table}[H]
        \begin{grammar}{\mathbb{T} \Rightarrow }{Typing}
            \mathbb{V} & Variable (usually if the typing depends on a quantified variable of it is not fully resolved)\\
            \left[\mathbb{T}\right] \to \mathbb{T} & Function (each function takes a certain number of argument and returns a type - usually a tuple)\\
            \mid \left[\mathbb{T}\right] & Tuple (function return type)\\
            \mid \mathbb{T}\ \mathbb{T} & Type application (for structs and for constraints)\\
            \mid \mtt{Addr}\ \mathbb{T} & address type (representing a pointer) \\
            \cdots & Primitive types (for example, $bits_n$)
        \end{grammar}
    \end{table}
\end{defn}

\begin{defn}[Primitive patterns]
    Primitive patterns represent the definition of types, they can be used to reconstruct a type represented by a variable, to lookup the variables representing each subterm, or to lookup the variable which represents the given type \xxx{`simplify' function}, since the inferencer state keeps track of bijection $\mtt{variable} \leftrightarrow \mtt{primitive\ pattern}$. This is quite costly, but it allows for many experiments in type inference and it can also help in debugging.

    Primitive patterns $\mathbb{P}$ are defined by the following grammar rules:

    \begin{table}[H]
        \begin{grammar}{\mathbb{P} \Rightarrow }{Primitive pattern}
            \left[\mathbb{V}\right] \to \mathbb{V} & Function \\
            \mid \left[\mathbb{V}\right] & Tuple \\
            \mid \mathbb{V}\ \mathbb{V} & Type application \\
            \mid \mtt{Addr}\ \mathbb{T} & address type \\
            \cdots & Primitive types
        \end{grammar}
    \end{table}

    Notice how it reflects the grammar for typings \xxx{ref}.
\end{defn}

\begin{defn}[Type reconstruction]
    Type reconstruction is perfomed by substituting primitive patterns for type variables iteratively until no type variable is possible to be substituted with a primitive pattern. The algorithm starts with a single type variable.
\end{defn}

\begin{lemma}[Primitive patterns]
    \label{typesObs}
    Every type $t$ can be equivalently represented by a type variable, by its primitive pattern, and by its reconstruction 
\end{lemma}

\begin{proof}
    For types, the theory assumes that  $\alpha \beta = \alpha' \beta' \Leftrightarrow \alpha = \alpha' \land \beta = \beta'$  (generalized to all constructors listed in \xxx{ref}, \xxx{ref}). And then $t = \alpha \beta \land t' = \alpha' \beta' \Leftrightarrow t = t'$. FThe above statement is a direct consequence of these assumptions; for reconstruction, shown by induction.
\end{proof}

\begin{defn}[Subtype dimensions]
    The algorithm recognizes two subtype dimensions, constness dimension and data kind dimension.

    The constness dimension represents the bounds of constraints on when the value of each variable has to be known, and also, when it can be know.

    The data kind dimension represents the bounds of constraints on on which registers (register types) the variables can be stored. For the types in returned tuples, for example, this can specify whether the variable is integral or a floating point number.
\end{defn}

\begin{defn}[Constnesses]
    $\mathbb{C}$ is the space of constness constants given as follows:
    \begin{table}[H]
        \begin{grammar}{\mathbb{C} \Rightarrow }{Constness}
            \mathtt{Regular} & \textit{for regular runtime variables} \\
            \mid \mathtt{Linkexpr} & \textit{for variables that are to be known during linkage} \\
            \mid \mathtt{Constexpr} & \textit{for variables that are to be known during compilation}
        \end{grammar}
    \end{table}

    We define the ordering on constnesses: $\bot = \mathtt{Constexpr} < \mathtt{Linkexpr} < \mathtt{Regular} = \top$. And the whole system of constnesses follows a linear ordering.
\end{defn}

\begin{defn}[Data kinds]
    $\mathbb{K}$ is the space of data kinds, each data kind is a subset of the power set of the set of registers on the given architecture.

    The kinds are ordered by the relation of being a subset. We put $\mathtt{GenericKind} = \top$ as the superset of all possible kinds and $\mathtt{EmptyKind} = \bot$ as the subset of all possible kinds.
\end{defn}

\begin{defn}[Type properties]
    $\mathbb{D}$ is the space of type properties given as follows:

    \begin{algorithmic}
        \State{$\mathbb{D} \Rightarrow (typing: \mathbb{T}, constness: \mathbb{V}, kind: \mathbb{V})$}
    \end{algorithmic}

    This represents that a type has a certain typing, and then constness and data kind same as a types represented by some type variables.

    The functions that project objects from $\mathcal{D}$ to the respective properties are called $t$, $c$ and $k$.
\end{defn}

\begin{defn}[Facts]
    $\mathbb{F}$ is the space of flat facts given as follows:
    \begin{table}[H]
        \begin{grammar}{\mathbb{F} \Rightarrow }{Flat facts}
            \mathbb{T} \sim \mathbb{T} & Equality constraint: \textit{specifies that the types are identical} \\
            \mathbb{T} \sim_T \mathbb{T} & Typing unification: \textit{specifies that the types are identical in the typing dimension} \\
            \mathbb{T} \leq_K \mathbb{T} & SubKind: \textit{specifies that the right-hand operand has more general data kind than the left-hand operand} \\
            \mathbb{T} \leq_C \mathbb{T} & SubConst: \textit{specifies that the right-hand operand has more general constness than the left-hand operand} \\
            \mathbb{K} \leq_K \mathbb{T} \leq_K \mathbb{K} & Kind bounds: \textit{specifies that the type has a data kind from the given range} \\
            \mathbb{C} \leq_C \mathbb{T} \leq_C \mathbb{C} & Const bounds: \textit{specifies that the type has a constness from the given range} \\
            C\ \mathbb{T} & Class constraint: \textit{specifies that the given constraint type (application of a class constraint constructor to a certain number of types - unless nullary) is instantiable} \\
            \mathbb{T} \geq_M \mathbb{T} & Instantiation constraint: \textit{specifies that the left-hand operand is monotype instance of the polytype right-hand operand; note: this is satisfiable only if the right-hand operand is indeed a polytype} \\
        \end{grammar}
    \end{table}

    Sometimes we will use derived facts like $\mathbb{T} \leq_K \mathbb{K}$, $\mathbb{T} =_K \mathbb{T}$, etc. These are either special cases of the aforementioned facts, or compositions of multiple facts. The most complicated derived fact we will use is the ``SubType'' fact, $\mathbb{T} \leq \mathbb{T}$, which is a combination of $\mathbb{T} \sim_T \mathbb{T}$, $\mathbb{T} \leq_K \mathbb{T}$ and $\mathbb{T} \leq_C \mathbb{T}$.

    It should be noted that, within the defined language, we cannot state the facts $\mathbb{T} \not\sim_T \mathbb{T}$, $\mathbb{T} \not\leq_K \mathbb{T}$, $\mathbb{T} \not\leq_C \mathbb{T}$ and $\mathbb{T} \not\leq_M \mathbb{T}$ and thus these (and similar ones) are not even derived facts.

    $\mathbb{W}$ is the space of facts given as follows:

    \begin{table}[H]
        \begin{grammar}{\mathbb{W} \Rightarrow }{Facts}
            \mathbb{F} & Flat fact \\
            \forall \alpha_1, \dots \alpha_n . [\mathbb{F}] \Rightarrow [\mathbb{W}] & Nested facts: \textit{specifies that satisfying the given nested facts (listed after the double arrow) requires the assumed facts (listed before the double arrow) to be satisfied as well}
        \end{grammar}
    \end{table}
\end{defn}

\begin{defn}[Bounds]
    We call the right-hand operand of a bounds fact the upper-bound and the left-hand operand the lower-bound.
\end{defn}

\begin{defn}[Trivial bounds]
    If a fact $F$ states $k_1 \leq_K \tau \leq_K k_2$ and $k_2 \leq k_1$, or $c_1 \leq_C \tau \leq_C c_2$ and $c_2 \leq c_1$, we call such a bounds fact trivial.

\end{defn}

\begin{lemma}[Trivial bounds]
    \label{trivBoundsObs}
    \begin{enumerate}
        \item If the bounds of a trivial bounds fact are equal, the type constesses of the types constrained by such a fact have to be the same and we can unify them.
        \item If, for a bounds fact, the upperbound is lower than the lowerbound, the fact cannot be satisfied.
    \end{enumerate}
\end{lemma}

\begin{defn}[Algorithm state]
    The algorithm state consists of the following finite variables
    % TODO: explain each
    \begin{itemize}
        \item Facts: $\mathcal{F} :: [\mathbb{W}]$
        \item Active type variables: $\mathcal{V} \subset \mathbb{V}$
        \item Forgotten type variables: $\mathcal{G} \subset \mathbb{V}$
        \item primitive patterns: $\mathcal{P} \subset \mathbb{P}$
        \item Type properties: $\mathcal{D} \subset \mathbb{D}$
        \item SubKinds: $\mathcal{K} :: \text{Directional graph} on\ \mathbb{V}$
        \item KindBounds: $b_K :: \mathbb{V} \to \text{interval} \left[\mathbb{K}, \mathbb{K}\right]$
        \item SubConsts: $\mathcal{C} :: \text{Directional graph on}\ \mathbb{V}$
        \item ConstBounds: $b_C :: \mathbb{V} \to \text{interval} \left[\mathbb{C}, \mathbb{C}\right]$
        \item Type explanation: $p :: \mathcal{V}_p \to \mathcal{P}; \mathcal{V}_p \subseteq \mathcal{V}$
        \item Type definition: $d :: \mathcal{V} \to \mathcal{D}$
        \item Result function: $u :: \mathcal{G} \to \mathcal{V}$
    \end{itemize}

    By $<_{\mathcal{K}}$ and $<_{\mathcal{C}}$,
    we understand that there exist a directional path between the given distinct operands in the respective graphs.
    We naturally extend this definitions to the other comparison operators
    (for example: $x =_{\mathcal{K}} y$ if there are paths back and forth between $x$ and $y$ in the graph $\mathcal{K}$ or $x = y$). % TODO: not that they can be equal

\end{defn}

\begin{defn}[Algorithm intrastate invariants]
    The algorithm is designed to have the following intrastate invariants:

    % TODO: invariant about u

    \begin{enumerate}
        \item Active and forgotten variables: $Var(\mathcal{F}) \cup Var(\mathcal{P}) \cup Var(\mathcal{D}) \cup Var(\mathcal{K}) \cup Var(\mathcal{C}) = Var(\mathcal{V})$ and $Var(\mathcal{V}) \cap Var(\mathcal{G}) = \emptyset$. \label{invVar}

        \item $\mathcal{V}$ and $\mathcal{D}$ are bijected by the type definition function $d$ with its inverse $d^{-1}$. The subset $\mathcal{V}_p \subseteq \mathcal{V}$ and the set $\mathcal{P}$ are bijected by the type explanation function $p$ and its inverse $p^{-1}$. Note that there generally are many types with no known explanation. \label{invPD}

        We expand the function applicability of $b_K$ and $b_C$ to type definitions is such a way that for an arbitrary type variable $v \in \mathcal{V}$, we have $b_K (d (v)) = b_K (c (d (v)))$, and similarly for $b_C$. In other words: applying $b_K$ to a type definition is equivalent to applying it to the type's kind (and similarly for $b_C$).

        We expand the applicability of $p$ and $d$ in such a way that for an arbitrary type variable $v \in \mathcal{V}_p$ it holds that $d(v) = d(p(v))$ and $p(v) = p(d(v))$.

        And finally, we expand the applicability of $t, k, c$ in such a way that for an arbitrary $v \in \mathcal{V}$ it holds that $t (v) = t (d (v))$, $k (v) = k (d (v))$, and $c (v) = c (d (v))$.

        \item 

        \item \label{invG} If we have two type variables $v, v' \in \mathcal{V}$ representing two types, then:
            \begin{enumerate}
                \item If $k (v) =_{\mathcal{K}} k (v')$, then: $k (v) = k(v')$. In other words, graph $\mathcal{K}$ has no strongly connected components (this will be true for $\mathcal{C}$ as well).
                \item If $c (v) =_{\mathcal{C}} c (v')$, then: $c (v) = c(v')$.
                \item If $b_K (k (v)) = b_K (k (v')) = [k_1, k_1]$ for some kind $k_1 \in \mathbb{K}$, then $k (v) = k (v')$.
                \item If $b_K (k (v)) = b_K (k (v')) = [k_1, k_2]$ for some kinds $k_1, k_2 \in \mathbb{K}$, $k_2 < k_1$, then $k (v) = k (v')$ and it is an invalid kind.
                \item If $b_C (c (v)) = b_C (c (v')) = [c_1, c_1]$ for some constness $c_1 \in \mathbb{C}$, then $c (v) = c (v')$.
                \item If $b_C (c (v)) = b_C (c (v')) = [c_1, c_2]$ for some constnesses $c_1, c_2 \in \mathbb{K}$, $c_2 < c_1$, then $c (v) = c (v')$ and it is an invalid constness.
                \item If $k (v) <_{\mathcal{K}} k (v')$, then for bounds $k_1, k_2, k_3, k_4$ such that $b_K (k (v)) = [k_1, k_3]$, and $b_K (k (v')) = [k_2, k_4]$ it holds that $k_1 \leq k_2$ and $k_3 \leq k_4$. Note that for new (and any non-stored) variables we will assume the least restrictive bounds, so adhering to this invariant means we update the bounds monotonically.
                \item If $c (v) <_{\mathcal{C}} c (v')$, then for bounds $c_1, c_2, c_3, c_4$ such that $b_C (c (v)) = [c_1, c_3]$, and $b_C (c (v')) = [c_2, c_4]$ it holds that $c_1 \leq c_2$ and $c_3 \leq c_4$.
            \end{enumerate}

        \item If a type represented by a type variable $v$ is explained ($v \in \mathcal{V}_p$), then $t (d (v)) = p(v) \left[ \tau := t (d (\tau)) | \tau \in \mathrm{free} (p (d))\right]$. In other words, the typing of the type represented by $v$ follows the typing generated from the typings of the types via which it is explained. \label{invT}
    \end{enumerate}
\end{defn}

\begin{defn}[Algorithm interstate invariants]
    The algorithm is designed to have the following interstate invariants: (we use the indices to distinguish a successor state variable from its predecessor, we will always assume $n < m \in \mathbb{N}$, we then use these indices in all derived functions too).

    \begin{enumerate}
        \item Forgotten variables stay forgotten: $v \in \mathcal{G}_n \Rightarrow v \in \mathcal{G}_m$.
        \item Forgotten variables used to be alive: $v \in \mathcal{G}_m \Rightarrow \exists n . v \in \mathcal{V}_n$.
        \item All free variables from facts are considered by the algorithm: $v \in \mathrm{free} (\mathcal{F}_0) \Rightarrow v \in \mathcal{V}_0$.
        \item Assumed subtype constraints are monotonically getting restricted. For kinds: $v \in {\mathcal{V}}_n \Rightarrow {b_K}_m (k_m(u_m(v))) \subseteq {b_K}_n (k_n(v))$ and for constnesses: $v \in {\mathcal{V}}_n \Rightarrow {b_C}_m (c_m(u_m(v))) \subseteq {b_C}_n (c_n(v))$.
        \item Typings do not get forgotten: $v \in {\mathcal{V}}_n \Rightarrow \exists s . s(t_n(v)) = t_m(u_m (v))$.
        \item Explanations do not get forgotten: $v \in {\mathcal{V}_p}_n \Rightarrow \exists s . s(p_n(v)) = p_m(u_m (v))$.
        \item Results are results of results: $u_m = u_m (u_n)$.
        % TODO: add more constraints
    \end{enumerate}
\end{defn}

\begin{defn}[Unifications]
    In the scope of this algorithm we distinguish 4 types of unification. The algorithms for each will be the same, but their meaning is different.

    \begin{enumerate}
        \item Type unification: we unify two type variables that represent types, we then apply the resulting unification to all type variables considered by the algorithm. \label{tUni}
        \item Typing unification: we unify two typings of certain types, we then apply the resulting unification to all other typings considered by the algorithm.
        \item Kind unification: we unify two type variables that represent kinds of certain types, we then apply the resulting unification to all other variables representing kinds considered by the algorithm. \label{kUni}
        \item Constness unification: we unify two type variables that represent constnesses of certain types, we then apply the resulting unification to all other variables representing constnesses considered by the algorithm. \label{cUni} 
    \end{enumerate}
\end{defn}

\begin{defn}[Fresh variable]
    \label{freshVar}
    A fresh variable is chosen according to: \linebreak $\inf \left(\mathbb{V} \setminus \mathcal{V} \setminus \mathcal{G}\right)$.
\end{defn}

\begin{defn}[Algorithm middlesteps]
    \label{middlesteps}
    After each step of the algorithm that transforms one of the state variables (mainly $\mathcal{G}$,$\mathcal{P}$ or $\mathcal{D}$) into a new state, we perform the minimal necessary unifications to ``repair'' the bijections $d$ and $p$ (intrastate invariant \ref{invPD}, all unifications), to collapse any strongly connected components of the graphs $\mathcal{K}$ and  $\mathcal{C}$ and to remove any duplicate images of trivial bounds from $b_K$ and $b_C$ (intrastate invariant \ref{invG}, unifications \ref{kUni} and \ref{cUni}). This will be often performed recursively. We then apply the resulting combined unification (limited to just unifications \ref{tUni}) to the function $u$ so it satisfies the intrastate invariant \ref{invVar}. % TODO: fix wording
\end{defn}

\begin{lemma}[Algorithm middlesteps]
    The algorithm may (correctly) fail during the middlesteps. This can be shown by the observation \ref{trivBoundsObs}.
\end{lemma}

\begin{defn}[Introducing new (fresh) variable]
    \label{introVar}
    We introduce a new (fresh) variable by extending the sets $\mathcal{V}$ and $\mathcal{D}$
    and the type definition function $d$ by the argument $v$ and its image $i = (typing: v, constness: v, kind: v)$, formally: $\mathcal{V}' = \mathcal{V} \cup \{v\}$, $\mathcal{D}' = \mathcal{D} \cup \{i\}$, $d' = d [v := i]$. Where $v$ is the new (fresh) variable.
\end{defn}
\begin{defn}[Algorithm presteps]
    \label{presteps}
    All the (flat) facts generally use complicated types, before each step of the algorithm we replace the types in the observed fact with type variables that represent the types.

    The transformation is performed according to the observation \ref{typesObs}, changing the state variables $\mathcal{P}$, $\mathcal{D}$, $\mathcal{V}$, $p$ and $d$ accordingly and we set the typing of the type to adhere to the intrastate invariant \ref{invT}. For the yet unnamed types we perform the steps explained in definitions \ref{freshVar} and \ref{introVar}.
\end{defn}

\begin{defn}[Algorithm initialization]
    The initial state is: $\mathcal{V} = \mathcal{G} = \mathcal{P} = \mathcal{D} = \mathcal{K} = \mathcal{C} = \emptyset$, $p = d = u = b_K = b_C = \emptyset \to \emptyset$ and $\mathcal{F}$ is the list of facts to be satisfied.

    Before the algorithm starts, we introduce the variables encountered in $\mathcal{F}$ according to the definition \ref{introVar}.
\end{defn}

\begin{defn}[Algorithm steps]
    After the prestep (definition \ref{presteps}), we retrieve the first fact $f$ of $\mathcal{F}$ and then:

    \begin{itemize}
        \item If $f \equiv t_1 \sim t_2$, we unify $t_1$ and $t_2$
        \item If $f \equiv t_1 \sim_T t_2$, we unify the typings $t (d (t_1))$ and $t (d (t_2))$, resulting in the unification $m$ which we then apply to all typings in $\mathcal{D}$.
        \item If $f \equiv t_1 \leq_K t_2$, we add an edge $(t_1, t_2)$ to $\mathcal{K}$.
        \item If $f \equiv t_1 \leq_C t_2$, we add an edge $(t_1, t_2)$ to $\mathcal{C}$.
    \end{itemize}

    Then we perform the middlestep (definition \ref{middlesteps}) and continue to the next step.
\end{defn}

\section{Monomorphization}

\section{Translation}
