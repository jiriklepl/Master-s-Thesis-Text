\chapter{Design of the prototype compiler}
 \label{chap3}

\begin{figure}
    \includegraphics[width=\linewidth]{img/out/arch.pdf}
\caption{Overview schema of the architecture of the prototype compiler.}
\label{fig:arch}
\end{figure}

\section{Overview}

The overall architecture of the prototype implementation is shown in \cref{fig:arch}. The data flow diagram shows the transformation of the input through the components (depicted by light blue rectangles) of the compiler, each somehow transforming the intermediate representation (AST; depicted by light green parallelograms) and/or generating some error information (depicted by, slightly darker, orange parallelograms).

In this chapter, we will describe the purpose and function of each of the components separately.

\section{Flattening and blockifying}

Flattening phase and blockifying phase together are important parts of the algorithm especially if we are interested either in translating the code into an SSA intermediate form (for example for the llvm backend) or any other form which assumes semi-sequential code (a graph of basic blocks).

Basic blocks are defined as the maximum sequence of consecutive statements such that it starts with a label (or the procedure's entry point) and ends with a (conditional) branch or a return statement (and apart from these two instances, it contains neither label statements nor any control flow statements), fallthrough to another basic block is regarded as an implicit branch.

The result of the combination of these two phases is semi-sequential representation of the code annotated with the information of starts and ends of singular basic blocks and the control flow of the program analyzed.

\subsection{Flattening}

Flattening phase is much simpler and it just transcribes the function bodies and other nested control flow structures into flat forms. Flat forms are characterized by all if statements being of the form:

\xxx{write a trivial if statement}.

This if statements directly relates to conditional branches. And then similarly for switch statements and other similar structures (switch statements would be a little bit more complicated, but we still first want them be in this flattened form). Since \cmm does not have other control flow statements with nested bodies, the principle ends here, but a similar principle would apply to them as well.

It then reorders the bodies of functions so that all declarations are at the very top, this is unnecessary, but it improves the readability of the functions in case of any compiler debugging being necessary and allows for easier implementation of other parts of the compiler pipeline.

One more work for the flattening phase in the scope of this programming language is that it studies the presence of any RAII contracts and puts calls to drop functions accordingly. This is not a usual goal for the flattening phase, but it fitted its algorithm well enough.

\subsection{Blockifying}

The blockifying phase follows the flattening phase and it annotated the nodes of AST with annotations stating whether the node starts a basic block, is part of a certain basic block, or that it is not a part of any basic block (is trivially unreachable). It then gives warnings for encountering these unreachable statements.

It also analyzes illegal fallthroughs (fallthroughs to continuation statements and fallthroughs outside of the procedure).


\xxx{move this to the next subsection}

After the blockifier annotates all basic blocks in a procedure, it then measures live ranges of all variables and gives errors for uninitialized variables \xxx{link bednarek}. It also finds out unreachable labels (and thus finds nontrivially unreachable code as well).

The state it keeps is usable for translating the code into an SSA form. For each basic block, it keeps the list of the basic blocks it is preceded by in the control flow of the corresponding procedure and for each variable present in the block or alive in its scope, it contains whether the variable is required to be alive at the basic block's entry, whether it is written to, and whether it is being read from.

The SSA form can be trivially generated by a two-phase algorithm where the first phase prepares the relevant phi nodes with placeholders for variables being exported from the preceding basic blocks and then the second phase fills in these placeholders with actual names of the exported variables (they will likely not match the original names of the variables as we have to distinguish between different writes to them).

\section{Flow analysis and how it connects to RAII}

\section{Type System}

We derived the type system from the description in \xxx{ref prev section}. But there were many flaws in the original design. First, and the most significant flaw, was assuming that the types are equal if they are equal in every subtype dimension and their typing.

\begin{ex}[Inconsistency of assuming type equality equivalent to combination of typing equality and subtype equality in all dimensions]
    Let us assume that the types $t$ and $t'$ are equal in every subtype dimension and the typing dimension. That is, $t =_s t'$ for every subtype dimension $s$ and $t =_T t'$ \xxx{according to def. ref here}.

    Then if they are type applications $\alpha \beta$ and $\alpha' \beta'$, respectively, then from $t =_T t'$, it also follows that $\alpha =_T \alpha'$ and $\beta =_T \beta'$, but it does not follow that $\alpha =_s \alpha'$ and $\beta =_s \beta'$ for every subtype dimension. We will offer a counterexample.

    Let us say, $t$ and $t'$ are pointers to functions that take a 32-bit bitvector and return the same type, they are both constant expressions and the both pointers are stored on the same register (they have the same data kind). This means that $t$ and $t'$ follow the assumptions listed above. But one of them can be constrained to be a pointer to a function that takes an integer and returns an integer, while the other can be constrained to be a pointer to a function that takes a float number and returns a float number. If we assumed that $t = t'$, we would have to solve the constraint $\mtt{float} = \mtt{int}$, where both operands are constants.

    Note that there can be systems with different subtype dimensions, in which the two relations can indeed be defined as equivalent. An example of such a system would be a system with no subtype dimensions.
\end{ex}

This fatal flaw was removed from the system.

\section{Type Inference}

\subsection{Type inference preliminaries}

The basic idea of the implementation of type inference used in our solution is that each type is represented by a type variable. And the semantic value of this variable is defined by the state of inferencer, structure that controls the inference algorithm.

In our solution, the type inferencer keeps track of the type definitions of all known types in the terms of a type constructor applied to variables representing each of its immediate subterms (so called ``primitive patterns''). And for each type variable it contains a handle which stores its subtype dimensions (fully substituted \xxx{?}, contrasting the primitive patterns). In addition to these two databases of ``alive variables'', it also keeps the mapping from each variable's original name (as it appears in annotated AST \xxx{preprocessing}) as the algorithm performs many unifications and substitutions which often causes the variables being renamed.

\subsubsection{Representaition of type-related values}

\begin{defn}[Type variables]
    We will call the space of type variables $\mathbb{V}$.

    Each type variable is represented by an unique integer identifier, its type kind and a reference to a parent type variable, if it has any.
\end{defn}

\begin{remark}[Type variable identifier]
    Type variable identifier is an unique number generated by incrementing a specific counter in the preprocessor state \xxx{ref} and then later in the inferencer state.
\end{remark}

\begin{remark}[Reference to a parent type variable]
    A reference a variable, which uniquely represents the nested scope in which the variable was generated. The references to parent variables are a generalization of the idea presented by the deferred inference algorithm  \xxx{ref to deferred inference algorithm}. They are used mainly for debugging.
\end{remark}

\begin{defn}[Alive type variable]
    We will call the variables not being renamed to another variable by the algorithm ``alive variables''. Renaming is performed every time two distinct variables are to be unified.
\end{defn}

The representation of types follows the one introduced in the definition \ref{typing_def}, but extended by the necessary extra constructions.

\begin{defn}[Typings]
    \label{typing_gram}
    $\mathbb{T}$ is a space of typings defined inductively by \cref{lang:typing}.
\end{defn}

\begin{lang}
  \def\grammarP{0.6}
  \begin{grammar}
    \firstcaseP{\mathbb{T}}{\mathbb{V}}{Variable}
    \otherformP{\left[\mathbb{T}\right] \to \mathbb{T}}{Function (each function takes a certain number of argument and returns a type - usually a tuple)}
    \otherformP{\left[\mathbb{T}\right]}{Tuple (function return type)}
    \otherformP{\mathbb{T}\ \mathbb{T}}{Type application (for structs and for constraints)}
    \otherformP{\mtt{Addr}\ \mathbb{T}}{address type (representing a pointer) }
    \otherformP{\cdots}{Primitive types (such as \texttt{bits\_n})}
  \end{grammar}
  \caption{The language $\mathbb{T}$ of flat typings and the basic semantics of types.}
  \label{lang:typing}
\end{lang}

\begin{defn}[Primitive patterns]
    Primitive patterns $\mathbb{P}$ follow the same grammar as typings, see the definition \ref{typing_gram} but, without the very first grammar case and with type variables ($\mbb{V}$) appearing in the grammar cases in place of typings ($\mbb{T}$).

    Primitive patterns represent the definition of types broken up into single applications of type constructors.
\end{defn}

\begin{remark}[Bijection between type variables and primitive patterns]
    A bijective mapping from type variables to primitive patterns is used to:

    \begin{itemize}
        \item reconstruct a type represented by a variable (recursively)
        \item lookup the variables representing each subterm of a type
        \item lookup the variable which represents the given type \xxx{`simplify' function}
    \end{itemize}

    Keeping a consistent mapping of primitive patterns is quite costly, but it allows for many experiments in type inference and it can also help in debugging.
\end{remark}

\begin{defn}[Type reconstruction]
    Type reconstruction is evaluated by substituting primitive patterns for type variables iteratively until no type variable is possible to be substituted with a primitive pattern. The algorithm starts with a single type variable.
\end{defn}

\begin{lemma}[Primitive patterns]
    \label{typesObs}
    Every type $t$ can be equivalently represented by a type variable, by its primitive pattern (if it has any), and by its reconstructed type acquired by type reconstruction.
\end{lemma}

\begin{proof}
    For types, the theory assumes that  $\alpha \beta = \alpha' \beta' \Leftrightarrow \alpha = \alpha' \land \beta = \beta'$  (generalized to all constructors listed in \xxx{ref}, \xxx{ref}). And then $t = \alpha \beta \land t' = \alpha' \beta' \Leftrightarrow t = t'$. FThe above statement is a direct consequence of these assumptions; for reconstruction, shown by induction.
\end{proof}

\begin{defn}[Subtype dimensions]
    The algorithm recognizes two subtype dimensions, \textbf{constness} dimension and \textbf{data kind} dimension:

    \begin{itemize}
        \item  The constness dimension, represented by space of constness constants $\mathbb{C}$ represents the bounds of constraints on when the value of each variable has to be known, and, when it can be known. The constness constants are defined as follows:

\begin{center}\begin{grammar}
  \firstcase{\mathbb{C}}{\mathtt{Regular}}{regular runtime variables}
  \otherform{\mathtt{Constexpr}}{variables known at compilation}
  \otherform{\mathtt{Linkexpr}}{variables known at linkage}
\end{grammar}\end{center}

        We define the ordering on constnesses: $\bot = \mathtt{Constexpr} < \mathtt{Linkexpr} < \mathtt{Regular} = \top$. And the whole system of constnesses follows a linear ordering.

        \item The data kind dimension represents the bounds of constraints on on which registers (register types) the variables can be stored. For the types in returned tuples, for example, this can specify whether the variable is integral or a floating point number.

        $\mathbb{K}$ is the space of data kinds, each data kind is a subset of the power set of the set of registers on the given architecture. The kinds are ordered by the relation of being a subset.
    \end{itemize}
\end{defn}

\begin{defn}[Type properties]
    Type properties represent the various dimension of a type. The functions that project objects from $\mbb{D}$ to the respective properties are called $t$, $c$ and $k$.

    $\mathbb{D}$ is the space of type properties given as tuples: $(typing: \mathbb{T}, constness: \mathbb{V}, kind: \mathbb{V})$
\end{defn}

\subsubsection{Modifications to the constraints language of deferred inference}

\begin{defn}[Constraints]
    With $\mbb{W}$ standing for a flat constraint or an implication constraint, the space of flat constraints $\mathbb{F}$ is defined inductively by a grammar as \cref{lang:flat}.
\end{defn}

\begin{lang}
  \def\grammarP{0.6}
  \def\arraystretch{1.4}
  \begin{grammar}
    \firstcaseP{\mathbb{F}}{\mathbb{T} \sim \mathbb{T}}{Equality constraint (the types are identical)}
    \otherformP{\mathbb{T} \sim_T \mathbb{T}}{Typing equality (the types are identical in the typing dimension)}
    \otherformP{\mathbb{T} \leq_K \mathbb{T}}{SubKind (the right-hand operand has more general data kind than the left-hand operand)}
    \otherformP{\mathbb{T} \leq_C \mathbb{T}}{SubConst (the right-hand operand has more general constness than the left-hand operand)}
    \otherformP{\mathbb{K} \leq_K \mathbb{T} \leq_K \mathbb{K}}{Kind bounds (the type has a data kind from the given range)}
    \otherformP{\mathbb{C} \leq_C \mathbb{T} \leq_C \mathbb{C}}{Const bounds (the type has a constness from the given range)}
    \otherformP{C?\ \mathbb{T}}{Class constraint (the constraint in the type has to be instantiable)}
    \otherformP{C!\ \mathbb{T}}{Class fact (the given constraint is known to be instantiable)}
    \otherformP{\mathbb{T} \sqsupseteq_M \mathbb{T}}{Instantiation constraint (the left-hand operand is a monotype instance of the polytype right-hand operand)}
  \end{grammar}
  \caption{The language $\mathbb{F}$ of flat constraints and their basic semantics.}
  \label{lang:flat}
\end{lang}

\begin{remark}[Derived constraints]
    Sometimes we will use derived constraints like $\mathbb{T} \leq_K \mathbb{K}$, $\mathbb{T} =_K \mathbb{T}$, etc. These are either special cases of the aforementioned constraints, or compositions of multiple constraints. The most complicated derived constraint we will use is the ``SubType'' constraint, $\mathbb{T} \leq \mathbb{T}$, which is a combination of $\mathbb{T} \sim_T \mathbb{T}$, $\mathbb{T} \leq_K \mathbb{T}$ and $\mathbb{T} \leq_C \mathbb{T}$.
\end{remark}

\begin{remark}[Impossible constraints]
    It should be noted that, within the defined language, we cannot state the constraints $\mathbb{T} \not\sim_T \mathbb{T}$, $\mathbb{T} \not\leq_K \mathbb{T}$, $\mathbb{T} \not\leq_C \mathbb{T}$ and $\mathbb{T} \not\sqsupseteq_M \mathbb{T}$ and thus these (and similar ones) are not even derived constraints.
\end{remark}


\begin{defn}[Bounds]
    We call the right-hand operand of a bounds constraint the upper-bound and the left-hand operand the lower-bound.
\end{defn}

\begin{defn}[Trivial bounds]
    If a constraint $F$ states $k_1 \leq_K \tau \leq_K k_2$ and $k_2 \leq k_1$, or $c_1 \leq_C \tau \leq_C c_2$ and $c_2 \leq c_1$, we call such a bounds constraint trivial.

\end{defn}

\begin{lemma}[Trivial bounds]
    \label{trivBoundsObs}
    \begin{enumerate}
        \item If the bounds of a trivial bounds constraint are equal, the type constesses of the types constrained by such a constraint have to be the same and we can unify them.
        \item If, for a bounds constraint, the upperbound is lower than the lowerbound, the constraint cannot be satisfied. We mark it an absurd bounds constraint and we include it in an error report.
    \end{enumerate}
\end{lemma}

\begin{defn}[Functional dependency set]
    We define a space $\mbb {FD}$ of functional dependency sets $FD$.

    Functional dependency set $FD$ on a sequence of type variables $V :: [\mbb V]$ is a set of rules $R$, each of the form $V \to \{\mtt {FROM}, \mtt {TO}, \mtt {UNDEFINED}\}$. This set of rules always contains a ``trivial'' rule $r_0$ such that $r_0(v) = \mtt{FROM}$, this trivial rule is implicit and can be omitted when defining a functional dependency set. If a constraint $C \vect \alpha$ (where $\vect \alpha = V$) follows a functional dependency set $FD$, then, for each proof $C \vect t$ of this constraint and for every corresponding rule $r$ in its set of rules $R$, we generate a new proof of the form $\forall \{v \pipe v \gets \vect \alpha, r(v) = \mtt {TO}\} . \{v \sim t \pipe (v, t) \gets (\mtt {zip}\ \vect \alpha\ \vect t), r(v) = \mtt {TO} \} \To C_r [\mtt {if}\ r(v) = \mtt {FROM}\ \mtt {then}\ t\ \mtt {else}\ v \pipe (v, t) \gets (\mtt {zip}\ \vect \alpha\ \vect t), r(v) \neq \mtt {UNDEFINED}]$. (we borrowed haskell syntax for more succinct definition) This new constraint proof for the variables with $\mtt {FROM}$ variables replaced by types generates new equality constraints on $\mtt {TO}$ variables. We then remove the proof $C \vect t$.

    We then replace each constraint $C \vect t$ with constraints $C_r \vect t_r$ for each corresponding rule $r$, where $t_r$ are subsequences of $t$ generated in a fashion similar to the constraint generation. And we then remove the constraint $C \vect t$.

    In practice, we always simplify the functional dependency sets to minimize the number of constraints. \xxx{fundep simplification}

    The written notation for defining a functional dependency set is a list of statements of the form $\vect \alpha_1 \to \vect \alpha_2$ where $\vect \alpha_1, \vect \alpha_2 \subseteq V$. Each such statement then corresponds to a new rule $r$, where $r(v) := \mtt {FROM}$ if $v \in \alpha_1$, otherwise if $v \in \alpha_2$, then $r(v) := \mtt {TO}$, for each variable $v \in V$.
\end{defn}

\subsection{Type inference preprocessing}

The type inference preprocessing, also called constraint generation takes the AST representation of the source code as its input, annotates it with so-called type holes, and for each of these type holes, generates constraints \xxx{see prev def} as described in \xxx{chap one ref}.

The result of type inference preprocessing is the list of constraints and the annotated AST. The list of constraints is then given to the type inference algorithm and after that the inferred type assignments stored in the state of the inferencer are paired with the annotated AST, resulting in AST with type annotation.

% The algorithm reads the syntactic tree, annotates its nodes (starting from the leaves) is quite complex and the code for it is very lengthy. It is also quite difficult to check for errors. This is why the actual implementation extends the language of constraints with constraint comments.with ``type holes'' and then gives constraints for these type holes.

\begin{defn}[Type holes]
    Type holes are simple structures that contain zero, one or multiple type variables depending on the type of the hole. They serve as the elaborations introduced in \xxx{deferred inference}.

    They help to distinguish, for example, whether an named reference is instantiation of some polymorphic object.
\end{defn}

\subsubsection{Type inference preprocessing algorithm}
\begin{enumerate}
    \item When entering each context (unit, struct, class, instance, function), the type inference preprocessor calls the variable collector. This collector then reports all the newly declared variables inside this context back to the preprocessor.

    \item The preprocessor then generates fresh type variables for these variables and uses them whenever they appear in the currently preprocessed code.

    \xxx{This was later determined to be completely replacible by declaration hoisting in the procedures and preprocessing the program in topological order}.

    \item When preprocessing a piece of code which contains nontrivial subnodes (sub-expressions, sub-statements, etc.), these subnodes are generally preprocessed first, then the algorithm retrieves their type holes and adds the constraints defined by the semantics of the code to the current list of constraints.

    \item The constraints are represented by a list of lists as each context nesting in the code also nests the context of constraints. The decision for a list of list is not completely necessary, but it makes the definition easy to use, future-proof and well-generalizable.

    Closing a constraint just simply pops the first element from this list of lists and gnenerates an implication constraint. This, then, slowly builds a tree of implication constraints to be solved.
\end{enumerate}

\subsection{Type inferencer state}

The main idea of the inferencer state is that it keeps a representation of 2 mappings and 2 graphs representing the relationships between the type variables:

\begin{itemize}
    \item a mapping for renames from variables ``forgotten'' due to unification to their ``alive'' representants
    \item a bijective mapping from some type variables representing a type to the primitive pattern defining the type in terms of immediate subterms of the respective type.
    \item a graph with edges from each variable representing a constness of some types to the variables representing the types with higher (more restricting) constnesses
    \item a graph with edges from each variable representing a data kind of some types to the variables representing the types with higher (more restricting) data kinds
\end{itemize}

We represent the 4 structures and some other information about each type, along with the input constraints, by the following finite variables, which we will use when discussing the algorithm's invariants:

\begin{itemize}
    \item Constraints: $\mathcal{F} :: [\mathbb{W}]$

    This is the complete list of unresolved constraints. In the initial state, it contains the constraints generated by the preprocessor. In subsequent states, it can also contain constraints which are derived from various resolving rules.

    It defines the termination state. The algorithm terminates if it cannot resolve any of the currently unresolved constraints. The algorithm terminates with success if the list $\mcal F$ is empty.

    \item Active type variables: $\mathcal{V} \subset \mathbb{V}$

    This set contains the type variables ever encountered by the algorithm and not present in $\mcal G$. The initial value is therefore $\emptyset$. The algorithm then, when generating a ``fresh variable'', uses the smallest possible $v$ such that $\forall v' \in \mcal V. v > v'$.

    This is only used when describing the algorithm, the actual implementation represents this set via the mapping $u$ and a single integer value

    \item Forgotten type variables: $\mathcal{G} \subset \mathbb{V}$

    This set contains all type variables that were in $\mcal V$ in some past state of the algorithm, but are not in the current state.

    Same as above, this state variable is not present in the implementation, but helps describing some invariants.

    \item primitive patterns: $\mathcal{P} \subset \mathbb{P}$

    This set contains all type definitions separated encountered or generated by the algorithm separated structurally by type constructors. see \xxx{definition for primitive patterns}

    \item Type properties: $\mathcal{D} \subset \mathbb{D}$

    This set contains three-valued tuples containing the properties of each type. See \xxx{definition for type properties}

    \item SubKinds: $\mathcal{K} :: \text{Directional graph on}\ \mathbb{V}$

    Directional graph that stores the encountered and determined inequalities between data kinds of pairs of type variables.

    \item KindBounds: $b_K :: \mathbb{V} \to \text{interval} \left[\mathbb{K}, \mathbb{K}\right]$

    This mapping maps each type variable representing the data kind of some types to the known bounds of this data kind.

    \item SubConsts: $\mathcal{C} :: \text{Directional graph on}\ \mathbb{V}$

    Similar to $\mcal{K}$, but for constnesses

    \item ConstBounds: $b_C :: \mathbb{V} \to \text{interval} \left[\mathbb{C}, \mathbb{C}\right]$

    Similar to $b_K$, but for constnesses

    \item Type explanation: $p :: \mathcal{V}_p \to \mathcal{P}; \mathcal{V}_p \subseteq \mathcal{V}$

    Mapping between type variables with known (encountered or derived) definitions and their respective definitions in terms of immediate subterms.

    \item Type properties: $d :: \mathcal{V} \to \mathcal{D}$

    Mapping between all type variables and their respective properties.

    \item Result renaming: $u :: \mathcal{G} \to \mathcal{V}$

    Substitution between forgotten type variables and active type variables resulting from type variables being unified.

    \item Current parent: $cp :: \mathcal{V}$

    The parent of a currently solved context.

    \item Schemes: $fs :: \mathcal{V}_f \to \mathbb{W}; \mcal V_f \subseteq \mcal V$

    The schemes of the closed functions (regular functions, methods, their instances, field accessors)

    \item Class schemes: $cs :: C_{cs} \to [\mathbb{W}]; C_{cs} :: \{\mbb C\}$

    The schemes of the known classes, each of the form $\vect \alpha . F \To C \alpha$, where $C?  \vect \alpha$ is a class constraint and $F :: [\mbb F]$ is a list of superclasses

    \item Class facts: $cf :: C_{cf} \to [\mathbb{W}]; C_{cf} \subseteq C_{cs}$

    The known proofs for each class constraint. Each proof has the form $\vect \alpha . F \To C \vect t$, where $C! \vect t$ $F$ is a class fact and $F :: [\mcal F]$ is the list of assumptions of the corresponding proof.

    \item Functional dependencies: $fd :: C_{fd} \to \mathbb{FD}; C_{fd} \subseteq C_{cs}$

    Functional dependency sets for each class constraint. They are stored after acquiring a simplified form with subsequent addition of the implicit trivial rule.
\end{itemize}

By $<_{\mathcal{K}}$ and $<_{\mathcal{C}}$,
we understand that there exist a directional path between the given distinct operands in the graphs $\mathcal{K}$ and $\mathcal{C}$, respectively.

We naturally extend this definitions to the other comparison operators
(for example: $x =_{\mathcal{K}} y$ if there are paths back and forth between $x$ and $y$ in the graph $\mathcal{K}$ or $x = y$).


\begin{defn}[simplified notation]
    We expand the function applicability of $b_K$ and $b_C$ to type properties in such a way that for an arbitrary type variable $v \in \mathcal{V}$, we have $b_K (d (v)) = b_K (k (d (v)))$, and similarly for $b_C$. In other words: applying $b_K$ to a type's properties is equivalent to applying it to the type's kind (and similarly for $b_C$ and constness).

    We expand the applicability of $p$ and $d$ in such a way that for an arbitrary type variable $v \in \mathcal{V}_p$ it holds that $d(v) = d(p(v))$ and $p(v) = p(d(v))$.

    And finally, we expand the applicability of $t, k, c$ in such a way that for an arbitrary $v \in \mathcal{V}$ it holds that $t (v) = t (d (v))$, $k (v) = k (d (v))$, and $c (v) = c (d (v))$.
\end{defn}

\subsection{Type inferencer state invariants}

\subsubsection{Intrastate invariants}

The algorithm is designed to have the following intrastate invariants:

% TODO: invariant about u

\begin{enumerate}
    \item Active and forgotten variables: $Var(\mathcal{F}) \cup Var(\mathcal{P}) \cup Var(\mathcal{D}) \cup Var(\mathcal{K}) \cup Var(\mathcal{C}) = Var(\mathcal{V})$ and $Var(\mathcal{V}) \cap Var(\mathcal{G}) = \emptyset$. \label{invVar}

    \item $\mathcal{V}$ is projected to $\mathcal{D}$  by the type properties function $d$. The subset $\mathcal{V}_p \subseteq \mathcal{V}$ and the set $\mathcal{P}$ are bijected by the type explanation function $p$ and its inverse $p^{-1}$. Note that there generally are many types with no known explanation. \label{invPD}

    \item \label{invG} If we have two type variables $v, v' \in \mathcal{V}$ representing two types, then:
        \begin{enumerate}
            \item If $k (v) =_{\mathcal{K}} k (v')$, then: $k (v) = k(v')$. In other words, graph $\mathcal{K}$ has no strongly connected components (this will be true for $\mathcal{C}$ as well). If such strongly connected components is created by registering an edge representing a subtyping constraint, then the variables inside this new strongly connected component are unified to preserve this invariant.
            \item If $c (v) =_{\mathcal{C}} c (v')$, then: $c (v) = c(v')$. Again, with the same resolution on encountering a newly created strongly connected component. We resolve the violation of this invariant similarly to the previous one.
            \item If $b_K (k (v)) = b_K (k (v')) = [k_1, k_1]$ for some kind $k_1 \in \mathbb{K}$, then $k (v) = k (v')$. The type variables representing the data kinds of $v$ and $v'$ are unified whenever it is derived they have the same value.
            \item If $b_K (k (v)) = b_K (k (v')) = [k_1, k_2]$ for some kinds $k_1, k_2 \in \mathbb{K}$, $k_2 < k_1$, then $k (v) = k (v')$ and it is an invalid kind. We resolve the violation of this invariant similarly to the previous one.
            \item If $b_C (c (v)) = b_C (c (v')) = [c_1, c_1]$ for some constness $c_1 \in \mathbb{C}$, then $c (v) = c (v')$. We resolve the violation of this invariant similarly to the previous one.
            \item If $b_C (c (v)) = b_C (c (v')) = [c_1, c_2]$ for some constnesses $c_1, c_2 \in \mathbb{K}$, $c_2 < c_1$, then $c (v) = c (v')$ and it is an invalid constness. We resolve the violation of this invariant similarly to the previous one.
            \item If $k (v) <_{\mathcal{K}} k (v')$, then for bounds $k_1, k_2, k_3, k_4$ such that $b_K (k (v)) = [k_1, k_3]$, and $b_K (k (v')) = [k_2, k_4]$ it holds that $k_1 \leq k_2$ and $k_3 \leq k_4$. Note that for n variables we always assume the least restrictive bounds. Which are well defined as constnesses and data kinds are both bounded lattices.

            The violations of this invariant are resolved by propagating the values. The data kind bounds are, for each variable, always updated monotonically.
            \item If $c (v) <_{\mathcal{C}} c (v')$, then for bounds $c_1, c_2, c_3, c_4$ such that $b_C (c (v)) = [c_1, c_3]$, and $b_C (c (v')) = [c_2, c_4]$ it holds that $c_1 \leq c_2$ and $c_3 \leq c_4$. The violations for this invariant are resolved similarly to the previous one.
        \end{enumerate}


    \item If a type represented by a type variable $v$ has a know definition (if $v \in \mathcal{V}_p$), then $t (d (v)) = p(v) \left[ \tau := t (d (\tau)) | \tau \in \mathrm{free} (p (d))\right]$. In other words, the typing of the type represented by $v$ follows the typing generated from the typings of the types via which it is defined. \label{invT}
\end{enumerate}

\subsubsection{Interstate invariants}

The algorithm is designed to have the following interstate invariants: (we use the indices to distinguish a successor state variable from its predecessor, we will always assume $n < m \in \mathbb{N}$, we then use these indices in all derived functions too).

\begin{enumerate}
    \item Forgotten variables stay forgotten: $v \in \mathcal{G}_n \Rightarrow v \in \mathcal{G}_m$.
    \item Forgotten variables used to be alive: $v \in \mathcal{G}_m \Rightarrow \exists n . v \in \mathcal{V}_n$.
    \item All free variables from constraints are considered by the algorithm: $v \in \mathrm{free} (\mathcal{F}_0) \Rightarrow v \in \mathcal{V}_0$. And $v \in \mathrm{free} (\mathcal{F}_n) \Rightarrow v \in \mathcal{V}_n$.
    \item Assumed subtype constraints are monotonically getting restricted. For kinds: $v \in {\mathcal{V}}_n \Rightarrow {b_K}_m (k_m(u_m(v))) \subseteq {b_K}_n (k_n(v))$ and for constnesses: $v \in {\mathcal{V}}_n \Rightarrow {b_C}_m (c_m(u_m(v))) \subseteq {b_C}_n (c_n(v))$.
    \item Typings do not get forgotten: $v \in {\mathcal{V}}_n \Rightarrow \exists s . s(t_n(v)) = t_m(u_m (v))$, where $s$ is a substitution
    \item Explanations do not get forgotten: $v \in {\mathcal{V}_p}_n \Rightarrow \exists s . s(p_n(v)) = p_m(u_m (v))$.
    \item Results are results of results: $u_m = u_m (u_n)$.
    \item Schemes do not get forgotten: $fs_n \subseteq fs_m, cs_n \subseteq cs_m, cf_n \subseteq  cf_m, fd_n \subseteq  fd_m$. Note the lack of applying $u$.
    % TODO: add more constraints
\end{enumerate}

\subsubsection{Unifications}

In the scope of this algorithm we distinguish 4 types of unification. The algorithms for each is the same, but their meaning is different.

\begin{enumerate}
    \item Type unification: we unify two type variables that represent types, we then apply the resulting unification to all type variables considered by the algorithm (this unification translates to the other three types as well). \label{tUni}

    This process ends with adding one of the type variables to the set of forgotten variables $\mcal G$ and adding the mapping from one type variable to the other to the function $u$. It also causes the need to update the variables $\mcal P, \mcal D, \mcal K, \mcal C, b_K, b_C,$ and $\mcal F$ so the algorithm still satisfies the required invariants. And any collisions in these variables has to be followed up by further unifications.

    \item Typing unification: we unify the typings of two input types, we then apply the resulting unification to all other typings considered by the algorithm.  \label{tyUni}

    This updates the variable $\mathcal D$.

    \item Kind unification: we unify type variables that represent two input kinds of certain types, we then apply the resulting unification to all other variables representing kinds considered by the algorithm. \label{kUni}

    This updates the variables $\mcal D, \mcal K, b_K$.

    \item Constness unification: we unify two type variables that represent constnesses of certain types, we then apply the resulting unification to all other variables representing constnesses considered by the algorithm. \label{cUni}

    This updates the variables $\mcal D, \mcal C, b_C$.
\end{enumerate}

\subsubsection{The type inference algorithm itself}

\begin{defn}[Fresh variable]
    \label{freshVar}
    An identifier for a fresh variable is chosen according to: \linebreak $\inf \left(\mathbb{V} \setminus \mathcal{V} \setminus \mathcal{G}\right)$. It is given a ``generic'' type kinds, which then is unified with whatever kind the fresh variable is assigned to represent. And the parent is set to $cp$, the current parent.
\end{defn}

\begin{defn}[Algorithm repair steps]
    \label{middlesteps}

    After each step of the algorithm that transforms one of the state variables (mainly $\mathcal{G}$ and $\mathcal{P}$) into a new state, we perform the minimal necessary unifications (possibly recursively):

    \begin{enumerate}
        \item To ``repair'' the injections $d$ and bijection $p$ (intrastate invariant \ref{invPD}; this translates into using all four unifications).

        \item To collapse any strongly connected components of the graphs $\mathcal{K}$ and  $\mathcal{C}$ and to remove any duplicate images of trivial bounds from $b_K$ and $b_C$ (intrastate invariant \ref{invG}, using unifications \ref{kUni} and \ref{cUni}).
    \end{enumerate}
\end{defn}

\begin{remark}[Algorithm repair steps]
    The algorithm may, correctly, fail during the repair steps. This can be shown by the observation \ref{trivBoundsObs}.
\end{remark}

\begin{defn}[Introducing a new (fresh) variable]
    \label{introVar}
    We introduce a new (fresh) variable by extending the sets $\mathcal{V}$ and $\mathcal{D}$
    and the type definition function $d$ by the argument $v$ and its image $i = (typing: v, constness: v, kind: v)$, formally: $\mathcal{V}' = \mathcal{V} \cup \{v\}$, $\mathcal{D}' = \mathcal{D} \cup \{i\}$, $d' = d [v := i]$. Where $v$ is the new (fresh) variable.
\end{defn}
\begin{defn}[Algorithm preparation step]
    \label{presteps}

    All the flat constraints generally use complicated types, before each step of the algorithm, we replace the types in the observed constraint with type variables that represent the types. Storing the structure of each input type $t$ to the variable $\mathcal P$ under the mapping $p$, generating a fresh type variable for each immediate subterm not represented in $\mcal P$, recursively.

    The replacement is performed according to the observation \ref{typesObs}, changing the state variables $\mathcal{P}$, $\mathcal{D}$, $\mathcal{V}$, $p$ and $d$ accordingly. We then, starting from the type variables representing the leafs of the structural tree of the type $t$, update the typing of all newly registered subterms of $t$ to adhere to the intrastate invariant \ref{invT}, each update just collecting the typings of immediate children to their parent. For the (sub)terms types not yet represented by any type variable, we perform the steps explained in definitions \ref{freshVar} and \ref{introVar}.
\end{defn}

\begin{defn}[Algorithm initialization]
    The initial state is: $\mathcal{V} = \mathcal{G} = \mathcal{P} = \mathcal{D} = \mathcal{K} = \mathcal{C} = \emptyset$, $p = d = u = b_K = b_C = \emptyset \to \emptyset$ and $\mathcal{F}$ is the list of constraints to be satisfied.

    Before the algorithm starts, we introduce the variables encountered in $\mathcal{F}$ according to the definition \ref{introVar} and we update the counter of variables accordingly.
\end{defn}

\subsubsection{Algorithm steps}

\begin{enumerate}
    \item We perform a preparation step (definition \ref{presteps}) \label{prestep_step}

    \item Constraint solving step: we retrieve the first constraint $f$ of $\mathcal{F}$ and then:

    \begin{itemize}
        \item If $f \equiv t_1 \sim t_2$, we unify $t_1$ and $t_2$ by the unification \ref{tUni}

        \item If $f \equiv t_1 \sim_T t_2$, we unify the typings $t (d (t_1))$ and $t (d (t_2))$ using the unification \ref{tyUni}.

        \item If $f \equiv t_1 \leq_K t_2$, we add an edge $(k(t_1), k(t_2))$ to $\mathcal{K}$.

        \item If $f \equiv t_1 \leq_C t_2$, we add an edge $(c(t_1), c(t_2))$ to $\mathcal{C}$.

        \item If $f \equiv k_1 \leq_K t \leq_K k_2$, we update the bounds for $k(t)$ in the variable $b_K$ in such a way that given $b_K(k(t)) = [k_1', k_2']$, we set $b_K$ to $b_K[ k(t) := [k_1 \cup k_1', k_2 \cap k_2'] ]$.

        \item If $f \equiv c_1 \leq_C t \leq_C c_2$, we update the bounds for $c(t)$ in the variable $b_C$ in such a way that given $b_C(c(t)) = [c_1', c_2']$, we set $b_C$ to $b_C[ c(t) := [\max (c_1, c_1'), \min (c_2,  c_2')] ]$.

        \item If $f \equiv C?\ \vect t$. Then if $C$ has a functional dependency set defined in $fd$, we perform rewrites of the constraint according to \xxx{add ref}.

        Otherwise we lookup the class scheme proofs for $C$ in $cf(C)$ for a proof such that $t(t) \sqsupseteq t(cf(C))$ (checked by unification algorithm which always prefers substituting the right-hand variables, otherwise it fails, trying the next proof).

        If we successfully lookup a proof for $C \vect t$, then we simply replace the constraint by assumptions of the proof, otherwise we put it aside and continue with the next constraint.

        Since we require all proofs to be disjunctive on their results, we can stop the lookup at the first successful find.

        \item If $f \equiv C!\ \vect t$, we store the proof $C \vect t$ to the list of proofs $cf(C)$.

        \item If $f \equiv t_1 \supseteq t_2$, we check whether we have a scheme present for $t_1$. If so, we instantiate the scheme, replacing all its quantified variables.

        \item If $f \equiv t_1 \supseteq t_2$, we check whether we have a function scheme present for $t_1$. If so, we instantiate the scheme, replacing all its quantified variables with fresh variables. We then replace this constraint with an equality constraint with the fresh scheme's result type for the left-hand side operand (leaving $t_2$ as the right-hand side operand, unchanged) and the assumptions of the scheme as new constraints. Otherwise, we put it aside.

        Note that since the lookup is dependent on the type, we have to make sure never to change $t_1$ in this constraint. By either providing an exception to the preparation step or by using a special type (variable) not present in other constraints.

        \item \xxx{ nested constraint}
    \end{itemize}

    \item Then we perform the repair step (definition \ref{middlesteps}) and continue to the next constraint. If no further constraint can be solved we continue with the constraints which have been put aside (recursively), unless it is the same set of constraints as the one before, if it is, we continue to the next step.

    \item If we have put aside any instantiation constraints, then we know we have an unclosed function. Since, if it was closed, there would be a corresponding scheme present in $fs$. We collect the instantiation constraints and we construct an ``instantiation'` graph with edges of the form ``(instantiated, instantiated by)'' and we analyze the topological ordering of this graph, identifying the strongly connected components and relations between them.

    \item If we have constructed the instantiation graph, we then take the functions in the component which does not have any connected edges with the ``instantiated by'' end outside of the component. And we return the instantiation constraints originating in these functions back to their respective nested constraints while rewriting them into equality constraints. We then continue with the step \ref{prestep_step}. \label{do_inst_graph}

    \item We then collect all the type variables free in the currently solved component, we minimize the subtypes according to the principle \xxx{ref prev chap}. We translate the resulting subtype variable definitions by suprema back into constraints (we do not recommend this. Instead, use type-like variable-length constructors that unify by unionization), and then we collect the leftover constraints on the free variables of the component, we then also include the constraints derived from the type descriptions (namely typing constraints, two-way subkind and subconst constraints derived from identical properties) and subtyping bounds, kind-bound and const-bound constraints. We close the component, generating a scheme for each function, quantifying the free variables of the component and assuming all the listed constraints.

    \item If there are more components in the instantiation graph, we construct a new instantiation graph by removing the currently closed component, we then continue with the step \ref{do_inst_graph}

    \item If there are none, we determine whether there are some unresolved constraints and if there are, we report an error. If there are none, the code is ready to be monomorphized.
\end{enumerate}

\xxx{maybe remove or move to conclusion}. We would like to point out that, ideally, there should be a phase between type inference and monomorphization that would annotate the code with the inferred schemes, which would have an effect of simplifying the the monomorphization implementation and decreasing the space for errors as the ast would be annotated with clear indications whether the singular nodes of ast are polymorphic or monomorphic (the monomorphic nodes would have no free variables, they are monotyped).

Note that, in the included implementation, this is part of the monomorphization phase, so all the primitives of this new layer are already demonstrated.

\section{Monomorphization}

The monomorphization algorithm is a two-phase processes which is performed on type-inferred code with type handles present in an annotation of each node of the AST.

\begin{enumerate}
    \item In the first phase, we go through the top level definitions in the program and identify which ones are monomorphic. We add them to the queue of code to be instantiated.

    \item We remove the first top level definition to be instantiated from the queue, prepending it to the resulting program, and then we add all statements causing instantiations to the queue. We repeat this step as long as the queue is not empty or we do not reach some predefined monomorphization limit.
\end{enumerate}

\xxx{add example for monomorphization}

If there was no limit on monomorphization, it could potentially continue forever.

\begin{ex}[non-terminating monomorphization]
    Suppose the following program:

    \begin{lstlisting}
        class C a {
            c() -> auto (a);
        }

        instance C ptr(auto(a)) => C auto (a) {
            c() {
                auto x;
                auto y;

                x = c();
                y = [x];

                return (y);
            }
        }
    \end{lstlisting}

    Then the program is semantically valid, but a successful monomorphization of its use would require an infinite set of subsequent definitions for the instances of c.

    \xxx{An interesting piece of information about this program is that it does not have any uninitialized values even though the variable $x$ cannot be proven to receive any value, yet it passes its value to some instruction.}

\end{ex}

The demonstration algorithm differs from the above description in that it also updates type handles \xxx{ref to preprocessing and inference} of each node of the currently instantiated top level definition. It also separates the queue into waves. The zeroth wave consists of monomorphic the top level definitions, then the first consist of the top levels being instantiated from references in the zeroth level, etc. It does not really differ from the approach with the linear queue. \xxx{it really doesn't?}

\xxx{if the AST contained type information with no external state, the monomorphization could be implemented via generics.}


\subsection{Monomorphization postprocessing}

The monomorphization algorithm does not change the code, it simply copies the top level definitions and changes their type annotations so that they contain the correct monotypes.

\subsubsection{Type hole filling}

This phase reads the type annotations of all types encountered in the monomorphized code and replaces the AST nodes with representations of the inferred types.

The resulting code then contains only explicitly typed declarations (no \li{auto}).

\subsubsection{Name mangling}

This phase renames all functions not marked with the \li{foreign "C"} specialization. According to their types. The name mangling used in this implementation is not very space-efficient as it is designed to be human-readable for easier exploration and debugging.

\begin{defn}[Name mangling]
    The mangled name consists of the \li{_M\$} prefix (standing for ``mangled'') followed by the name and then by the result of applying the mangle function to a type which, for primitive types, returns their name, and for aggregate types, returns the prefix corresponding to the aggregate type followed by the arguments enclosed in \li{\$L} and \li{R\$} (for functions, the arguments and the returns are enclosed separately).
\end{defn}


\xxx{example}

\section{Translation}

The final translation of the code (or code generation) is performed on the postprocessed code with type annotations. It uses \xxx{reference} llvm-hs-pure as a framework for llvm AST generation.

Apart from the postprocessed code, it takes three variables from the blockifier:


\xxx{define the variables in the blockifier}
\begin{enumerate}
    \item Control flow: The edges of the control flow graph
    \item Block data: The mapping from each block's index and variable name appearing in the context of the corresponding block to a three-valued tuple \linebreak \li{(reads, writes, requests alive) where each entry is a boolean}
    \item Blocks table: The mapping from each block's index to its name so we can use the blocks' actual names in the generated code.
\end{enumerate}

The output of translation is then a code interpretable by the low-level virtual machine compiler llc.

It first restructures the code so that all the struct definitions (since they do not ever call any procedures) are at the top and the procedures are at the bottom.

Then it collects the definitions of all structs and represents them in the pairs \linebreak \li{([(field_alias, field_index)], [field_type])}. The second field then corresponds to how llvm-hs-pure represents the structures.

After the translator collects all the struct definitions, it starts translating the procedures:

\begin{enumerate}
    \item It generates argument variables for all formal arguments

    \item It hoists the declarations inside the procedure to the beginning of the procedure and generates an \li{alloca} instruction for each struct datum declared in the procedure's \li{stackdata} declarations and stores the mapping from their aliases to the operands representing the \li{alloca}'ted memory.

    \item Then it breaks the list of statements into consecutive blocks. The blocks are already annotated by the blockifier, so it just collects the ones annotated by the same block index into one group.

    \item Then it finalizes the entry block (started implicitly when beginning to translate the procedure). It gives the translation procedure the mapping from formal arguments to operands.


    The translation of the entry block then returns this mapping with added entries for assignments performed in the entry block. These new entries can override the old ones and the current mapping always affects the translation of the currently translated statement \xxx{we will return to that}. We will call this mapping \li{nvars}

    Note that one variable corresponds to multiple operands \xxx{example where we assign to `x' multiple times}. \xxx{define llvm's phi nodes somewhere}

    \item After the procedure entry is done, it generates a list \li{placeholders} of placeholders used as operands for phi nodes at the entry of each block, these placeholders are indexed by the pair \li{(idx, name)} where \li{idx} is the index of the block the operand is from and \li{name} is the name of the variable the operand corresponds to. These placeholders are generated for each block's (with the exception of the entry block) potentially exported variable (every variable $v$ such that \tit{block data} for the block and $v$ have the \li{writes} and/or \li{requests alive} set).

    \item Then, it translates each block, giving it the \li{nvars} mapping and the indexed list of placeholders. We will assume the block is indexed \li{idx}.

    \begin{enumerate}
        \item The block translation begins by generating the list \li{vars} of variables $v$ to be imported (via phi nodes). These are the ones, for which \tit{block data} maps the blocks's \li{idx} and the variable $v$ to a tuple with the \li{requests alive} entry set.

        \item Then it generates the list \li{from} of blocks to generate the variables from. This is done by collecting the out-nodes $f$ of the edges $e$ in the control flow list such that $e = (f, idx)$.

        \item Then it translates the first statement of the block (this block is a label statement).

        \item Now, it finally generates the phi nodes to import each variable from the \li{vars} list from the blocks in the \li{from} list.

        It does so by generating a phi node for each $v$ in \li{vars}. Each of these phi nodes is then given an argument for each $f$ in \li{from}, taking a value from \li{nvars} corresponding to $v$ if $f$ corresponds to the entry block, otherwise a placeholder from the \li {placeholders} list indexed by the pair $(f, v)$. \xxx{lemma: quadratic number of imports}

        \xxx{does this deserve an example?}

        It then adds the entry pairs for each $v$ and its corresponding phi node to the  mapping $bvars$.

        \item Then we translate the statements in the block, each given a mapping from the variables to the corresponding operands that represent their current values. The assignment statements and call statements can (and usually will) change this mapping and this new mapping is then given to the next statement, etc. The last such mapping serves as the \li{exports} list of the current block
    \end{enumerate}

    \item We collect the \li{exports} lists of each block into one list and zip the collected list with the list of \li{placeholders} \xxx{lemma: they have the same length} and from this we create a mapping \li{(placeholder, export)}. We then use this mapping to replace all placeholders in phi nodes with the actual operands that are to be exported from the corresponding blocks. \xxx{lemma: the placeholders appear only in phi nodes}
\end{enumerate}
