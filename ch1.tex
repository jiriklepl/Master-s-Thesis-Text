\chapter{The French approach to type inference}

\label{chap1}

In this chapter, we discuss the possibility of using a modification of the algorithm W \cite{damas1982principal} for type inference, as was the approach of our previous work \cite{klepl2020type}. We then identify the challenges of this approach in the context of type-inferring a C-like language with a Hindley-Milner \cite{damas1982principal} type system extended by multi-parameter typeclasses for overloading. We then introduce an algorithm based on the `French approach' (this name was coined by users of the languages from the ML family developed at French institutions), which is simpler to use in the given context and allows for better extensibility.

As this thesis builds on our previous work \cite{klepl2020type}, we assume the same standard syntactic notation and semantics of the untyped lambda calculus, STLC, System F \cite{barendregt1992lambda}, Hindley-Milner type system (HM) \cite{damas1982principal}, and the HM type system with typeclasses described by Typing Haskell In Haskell (here shortened as THIH \cite{jones1999typing}).

Before we begin, let us lay down two terms we extensively use in arguments:

\begin{description}
\item[Type inference] is a process of semantic analysis, which assigns a type (according to the given type system) to each node of the abstract syntax tree (AST) representing the input code, where it makes sense (expressions, procedures, record types, etc.). Each such type can be either monotype or polytype, and the object is then monomorphic or polymorphic, respectively. Polytypes are characterized, in contrast to monotypes, by being quantified, and thus they can represent multiple monotypes.

\item[Monomorphization] statically interprets the generalized callgraph (also including references to type definition) in a type-inferred code, and, for every reference to a polymorphic object, it generates a monomorphic copy of this object, which has the polytype appropriately instantiated to the monotype fitting the current context.
\end{description}

\section{Algorithm W and its limitations}

Algorithm W is a type inference algorithm introduced by \citet{damas1982principal} that infers simple but practical polymorphic types for lambda calculus with let-construction. It takes an input in the language of lambda terms, outputting the inferred type assignments -- a mapping from the input terms to their principal types. The principal type is some least generic polytype (quantified type) that instantiates into all valid monotype (unquantified type) assignments of the term.

When type inferring a term $x$, algorithm W follows the inference rules listed in \cref{fig:wRules} with a certain \emph{context} $\Gamma$. Context $\Gamma$ is the set of assumed type assignments and, in the THIH extension, predicates (both possibly quantified and with some further assumed predicates). From this point forward, if not stated otherwise, by algorithm W, we understand the variation described by \citet{jones1999typing}.

\begin{figure}
    \caption{Type inference rules for the Hindley-Milner type system}
    \label{fig:wRules}
    $$\boxed{\renewcommand{\arraystretch}{2.2}\begin{array}{cc}
        \infer[\text{(Variable)}]{\Gamma \vdash x : \sigma}{x : \sigma \in \Gamma} \\
        \infer[\text{(Application)}]{\Gamma \vdash e e' : \tau}{\Gamma \vdash e : \tau' \rightarrow \tau & \Gamma \vdash e' : \tau'} \\
        \infer[\text{(Abstraction)}]{\Gamma \vdash \lambda x . e : \tau' \rightarrow \tau}{\Gamma_x \cup \{x : \tau'\} \vdash e : \tau} \\
        \infer[\text{(Instantiation)}]{\Gamma \vdash e : \sigma'}{\Gamma \vdash e : \sigma & \sigma > \sigma'}\\
        \infer[\text{(Generalization)}]{\Gamma \vdash e : \forall \alpha . \sigma}{\Gamma \vdash e : \sigma & \alpha \text{ not free in }\Gamma}\\
        \infer[\text{(Let-polymorphism)}]{\Gamma \vdash \text{let } x = e \text{ in } e' : \tau'}{\renewcommand{\arraystretch}{1.1}\begin{array}{c}\Gamma \vdash e : \forall \vect{\alpha} . \tau \\ \Gamma_x \cup \{x :\forall \vect{\alpha} . \tau \} \vdash e' : \tau'\end{array}}
    \end{array}}$$
\end{figure}

Throughout this section, we will show that algorithm W provides a reasonable basis for the type checking of the type system. However, with more extensions, namely multi-parameter typeclasses with functional dependencies \cite{jones2000type} (we abbreviate them MPTCs), it becomes unnecessarily complex and inefficient if we want to use it for type inference as well. We will describe how we can use the same principles to implement a simpler and more extensible two-phase algorithm based on constraint solving.

\subsection{Extensions of algorithm W}

The algorithm W, which we used in the previous work \cite{klepl2020type}, assumed no recursion in the code. Thus, we had to rewrite any real program with a trivial or nontrivial recursion into a form where recursive functions were represented by the \lstinline{fix} operator applied to some argument. Type of the \lstinline{fix} operator is $\forall \alpha . \pars {\alpha \to \alpha} \to \alpha$.

The algorithm for preprocessing trivial recursions is derived from fixed point theorem \cite{barendregt1992lambda,damas1982principal}): a function represented as $f \vect{x} = \calC[f, \vect{x}]$, where $C$ is a lambda term such that $\{f\} \cup \vect{x} \subseteq \free \calC$, is simply rewritten into $f = \mathit{fix}\ \lambda F \vect{X} .\calC[f := F, \vect{x} := \vect{X}]$.

For a nontrivial recursion (strongly connected components of the call graph), we use a generalization of the fixed-point theorem, double fixed-point theorem \cite{stepanek}, which describes how to construct this rewriting for a component of size two and can be easily generalized for an arbitrary component.

We demonstrated that it is indeed possible to represent the source code in the language of nonrecursive terms. However, doing so is complicated as we have to map every language feature to a term. It also does not scale well to possible extensions with more advanced language features (those might introduce further recursive patterns, requiring a redesign of the mapping to terms).

\subsubsection{Type inference for record field accessors}
\label{ex_structs}

In the previous work, for extending C to use an HM-style type system with typeclasses, we determined that the algorithm W, as described in THIH, is too weak. In the following subsections, we explore a possible extension of the algorithm W and discuss its practicality and how it gives motivation for the deferred solving algorithm introduced in \cref{defer_solve}.

In the context of algorithm W, for the needs of type inference, we represent \emph{field accessors} as functions from a record structure to the given field (or, equivalently, with a pointer constructor applied to them), each such function named after the given field name.

Algorithm W can either type-infer a program with no field overloading (all record field names are unique even between multiple record types) or type-check a program but with limited type inference if we would allow field name overloading.

In the rest of this subsection, we show how to type-check record fields using the algorithm W. Let us define two record types:

\begin{lstlisting}
    struct A {int x} a;
    struct B {float x} b;
\end{lstlisting}

Now, if we write \lstinline{a.x} or \lstinline{b.x}, it is straightforward to see that these expressions have types \lstinline{int} and \lstinline{float}, respectively. If we renamed the field of \lstinline{B} to have different name (for example, \lstinline{y}), we could represent the field accessors as functions $.x : A \to int$ and $.y : B \to float$.

But we do not want to bother the programmer by requiring unique names of all fields in the program. To allow the fields to have different types even though they share the same name, we have to overload the field accessor functions. Overloading is achieved via introducing a typeclass. We define the field accessors as two instances of a method \li{.x} of the ``Has\_x'' typeclass with two type arguments, one for the record type and the other for the field type. The first type argument uniquely represents the instance of the field accessor, and the second one allows us to specify the matching field type.

We define the multi-parameter typeclass (MPTC) \li{Has_x}: ($a \to b$ is a functional dependency stating that $a$ uniquely represents $b$)

\begin{center}
    \lstinline/class Has_x a b | a -> b {.x : a -> b}/
\end{center}

The two instances are then defined:

\begin{lstlisting}
instance Has_x A int {.x : A -> int}
instance Has_x B float {.x : B -> float}
\end{lstlisting}

This constraint cannot be expressed in the HM type system with typeclasses, as each class constraint can take only a single type variable. In the THIH variation \cite{jones1999typing}, this can be indeed expressed (it is not reflected in the original presentation, but the supplementary implementation allows an arbitrary number of class arguments), and the algorithm is strong enough to type-check such construction for concrete types. However, it lacks the ability to infer the obvious types of the previously mentioned expressions \lstinline{a.x} and \lstinline{b.x} even though we know the types of both \lstinline{a} and \lstinline{b}.

\subsection{Tackling the limitations of syntax-driven inference}
\label{sec:tackle}

In \cref{ex_structs}, the inability to type-infer the types of \lstinline{a.x} and \lstinline{b.x} could be possibly solved during the subsequent monomorphization phase. The monomorphization phase, given the complete knowledge of the record types \li{a} and \li{b}, would be able to follow the types of the computation just as the type inference algorithm of C (strictly top-down approach). However, in the general case, this would mean the monomorphization could fail for reasons of type checking (thus, the previous type checking would be only partial), and we could not infer the types against the direction of the computation, for which we aimed. Therefore, we should address this issue in the type inference itself, in the algorithm W.

\subsubsection{Using the algorithm W in the type system with MPTCs}

Consider that we would like to modify the algorithm W, so it solves principal types even in the presence of MPTCs. To function properly, we can easily observe that it has to, on each subprogram (a strongly connected component of the program's call-graph; we perform type inference on these subprograms in topological order, beginning with the leaf components), first perform the usual unifications on the syntactic structure of the subprogram, returning a type assignment in the form: $\mtt{name} :: \forall \vect \alpha . P  \To t$, where $P$ is a set of constraints, each having a form $C \vect \tau$, where $C$ is a predicate on the types $\vect \tau$, propagated from the subexpressions of $\mtt {name}$, under some $\vect \alpha = \free t$. And then, it has to solve the constraints in $P$ according to some extended constraint solving mechanism for MPTCs, for example, with maps, as explored by \cite{jones2000type}. We assume this approach. Here we see the first hints of the modified algorithm W turning into a more complicated version of deferred solving we introduce in \cref{defer_solve} as this constraint solving becomes a progressively more significant part of the algorithm.

\subsubsection{Example: solving class constraints from known proofs}

We show how to solve a class constraint by instantiating a know predicate proof on a small example: given $\Gamma = \{\forall a . \mtt {Ord} [a]\}$, we can prove $\Gamma \vdash \mtt {Ord} [Int]$, using substitution $[a := \mtt {Int}]$ \cite{jones1999typing}.

Notice that the substitution $\sigma$ that is used to prove a constraint $C \tau$ from a known predicate proof $C \tau'$ has to satisfy $\tau = \tau' \sigma$, which then also means, it satisfies $\dom \sigma \subseteq \free \tau' \land \ran \sigma \subseteq \free \tau$ (it consumes the free variables of $\tau'$ not present in $\tau$).

Using a more complicated example, we can demonstrate that class constraints often generate additional (more complex) class constraints: To prove $\Gamma \vdash \mtt {Ord} [Int]$ in the context $\Gamma = \{\mtt {Ord}\ \mtt {Int}; \forall a . \mtt {Ord}\ a \To \mtt {Ord} [a]\}$, we first deduce $\Gamma \vdash \mtt {Ord} [Int] \Leftarrow \Gamma \vdash \mtt {Ord}\ Int$ (proven using substitution $[a := \mtt {Int}]$ and the proof $\forall a . \mtt {Ord}\ a \To \mtt {Ord} [a]$) and then $\Gamma \vdash \mtt {Ord}\ Int$ (proven by being an assumption of the context; therefore, using an empty substitution).

Unfortunately, in general, the termination of the process depends on the type system choosing the correct proof --- naively, it could have explored a valid branch that proves $\mtt Ord [a]$ from a proof assuming $\mtt Ord [[a]]$, etc., which could continue indefinitely. This shows that the type system can be pushed to generate infinite chains of proofs for some contexts (for example, if we introduce language extensions such as MPTCs and non-reducing proofs without some appropriate requirements).

\subsection{Challenges of extending the algorithm W}


Using algorithm W requires nontrivial preprocessing in case of recursions or more complicated language sugar. After that, it works by assigning a principal type for each subterm of the solved (sub-)program in the syntactically given order as demonstrated by \citet{jones1999typing}. This is often impossible when the language contains any form of type-level computation, including MPTCs. In \cref{sec:tackle}, we showed the difficulty of attempting to modify the algorithm to make this possible.

Then, algorithm W has to match each constraint to a proof. If the proofs have any assumptions, this generates further constraints, forming chains or even trees of subsequent constraints.

The solving could be simplified by collecting all relevant constraints on the types (equalities and predicates) and then solving the types independently on their order while having some indication on predicates that would state whether they are already solved to avoid unnecessary computations. These are exactly the principles of deferred solving we describe in \cref{defer_solve}, a two-phase algorithm, which does not require the described difficult preprocessing nor syntactic mapping of subterms to types, and introduces \emph{witnesses} that simplify solving the multiple class constraints.

\section{Deferred solving}
\label{defer_solve}

The idea of using deferred solving (proposed by \citet{vytiniotis2011outsidein}) is to first \emph{elaborate} the source program with special variables (\emph{elaborations}), then to solve the constraints generated from a semantic analysis of the source program, over the elaboration variables. This allows the constraint solver to be independent of the source program's syntactic specifics as the program's type semantics are all captured in a simpler \emph{constraint language}. After the inference successfully finishes, we insert the inferred information back into the source program in place of the elaborations.

Elaborations consist of type variables, and then \emph{dictionaries} and \emph{coercions} (defined by \citet{vytiniotis2011outsidein}) that help the subsequent monomorphization phase decide what instances to use at each referring site. In the deferred solving algorithm, they serve as proofs for each constraint. \emph{Dictionaries} represent typeclass instances that prove class constraints. And \emph{coercions} prove type identities. We call dictionaries and coercions, collectively, \emph{witnesses}.

Deferred solving, or deferred inference, is very similar to the idea of modified algorithm W we presented earlier, but it does not require each of the syntactic structures appearing in the type-inferred subprogram to be directly mapped to a type. It just requires a set of constraints that capture the semantics of the program. In the basic version, these constraints consist of type equalities between types that have to be equal and class constraints put on certain types that require a proof of the given tuple of types belonging to the given class (these proofs represent instances defined in the type-inferred language).

The notion of type assignments is simplified in such a way that all types considered by the algorithm are monotypes. Type quantification with predicates, which, in the algorithm W is considered a part of the type assignment, is, in this algorithm, instead, applied to a set of constraints.

We use the following constructs:


\begin{description}
    \item[Qualification] A construct consisting of a set of predicates (or constraints) being applied to an unquantified term of the type language. We then call it a qualified term.

    The term, which is qualified, is usually a type or a set of constraints. For a term $x$ and a set of predicates $P$, the following shows a qualified term:

    $$P \To x$$

    \item[Type Quantification] A quantification that universally binds a set of type variables.

    We often omit the word `type' when referring to type quantification. For a set of variables $\vect \alpha$, a quantification is:

    $$\forall \vect \alpha$$

    The following then shows the quantification applied to a term $x$ of the type language:

    $$\forall \vect \alpha . x$$

    \item[Scheme] A term scheme is a quantification applied to a qualified term of the type language. We will call \emph{type schemes} just schemes.

    \item[Free type variables] In a given term of the type language, \emph{free type variables} are those type variables that are not bound by a quantification. Formally, for a quantification: $\free {\forall \vect \alpha . x} = \free {x} \setminus \vect \alpha$; for a type variable $v$: $\free v = \{v\}$; otherwise: unification of free type variables of all subterms.
\end{description}

\subsection{Constraint language}

The language of constraints is an extension of traditional type language with type variables annotated by ``context levels'', giving a language of flat and nested constraints:

\begin{defn}[Language of constraints]
    \label{def:defer_constr}

    Language $F$ of \emph{flat constraints} is defined as follows:

    \begin{center}\begin{grammar}
      \firstcasesubtil{$F$}{d: C \tau^\ast}{Class constraint}
      \otherform{g : \tau_1 \sim \tau_2}{Equality constraint}
    \end{grammar}\end{center}

    The language $W$ of \emph{nested constraints} is defined inductively as follows:

    \begin{center}\begin{grammar}
      \firstcasesubtil{$W$}{F}{Flat constraint}
      \otherform{\forall \alpha^\ast . F^\ast \To W^\ast}{Implication constraint}
    \end{grammar}\end{center}

    \label{constraint_language}
\end{defn}

We call $d$ and $g$ the \emph{witnesses} of the corresponding constraint, because they represent the instances that prove the given constraints.

Unless we allow overlapping proofs \footnote{\emph{overlapping proofs}: proofs such that can instantiate into the same type; equivalently: proofs that have a unification}, the witnesses are isomorphic to the rest of their respective constraints. For example, given two constraints $d_1: C \vect \tau_1$, $d_2: C \vect \tau_1$, then $d_1 = d_2 \GetsTo \tau_1 = \tau_2$.

Additionally to the already described equality constraints and class constraints, the constraint language also contains ``implication constraints'', which are constraint schemes. They represent typings, both explicit and implicit, of functions declared by the programmer. Existentials contained in function types (e.g., parameters with run-time type binding, as described in the following subsection) are represented by their own implication constraint.

\begin{defn}[Naming]
    \begin{itemize}
        \item A set of constraints that is qualified in a scheme of an implication constraint is called \emph{nested} in the given implication constraint. The constraints in the nested set are called \emph{nested} in the given implication constraint as well.

        \item We call the set of nested constraints of all the parent implication constraints a \emph{scope} or a \emph{context}.

        \item We use scopes to describe a context level of a constraint. For a constraint, a \emph{context level} is the maximum length of implications leading to it.

        \item \emph{Context level of an unbound type variable} is defined to be equal to the context level of its scope. Note that all appearances of a certain type variable have the same context level.

        \item Type variables bound by a parent implication constraint are called \emph{Skolem constants}.
    \end{itemize}
\end{defn}

For example, in the following set of constraints
\[ \left\{ a \sim b; \forall c. d \sim e \To \left\{ a \sim d \to e \right\} \right\} \]
the type variables $a, b$ have context level $0$; the type variables $d, e$ have context level $1$; and $c$, being a Skolem constant, has no context level.

\subsection{Existentials}

\label{existentials}

Existentials are types with existentially-quantified type variables, the semantics of such is that there exists some assignment to the existentially-qualified variables that, within the given constraints, describes the value of the existential. Usually, this assignment can be solved only during the dynamic interpretation of the code (i.e., `at runtime').

Semantically, the `universals' (universally quantified types) and existentials differ in that universals have unknown monotypes during definitions, and they have to be defined polymorphically, while existentials have unknown types in references and can be defined monomorphically. This leads to an interesting practical consequence: Code specialization required for universal types often leads to code multiplication. On the other hand, the use of existentials leads to the necessity of runtime witnesses.

\subsubsection{Existentials as constraints}

Before we start describing the construction of the type system based on constraint solving, we illustrate the aims using a translation of existentials to the constraints language on an example adapted from \citet{peytonjones2019type}.
The Haskell code in \cref{cex:existentials} shows a definition of a type \lstinline{T} with an existentially-typed field \lstinline{getT}, and then a list of variables of that type along with a function that consumes them, accessing the existential.

\begin{codex}
\lstinputlisting[style=haskellStyle]{examples/existentials.hs}
\caption{Example use of existentials in a Haskell program (Functions \texttt{traverse\_}, \texttt{putStr}, \texttt{map} and \texttt{show} are taken from standard Haskell 2010 library)}
\label{cex:existentials}
\end{codex}

We want to translate this example into the constraint language. We decorate the variables in the elaborations with $@$ for clarity, and ignore the elaboration levels for brevity (all global variables are in level 0, and all locally-mentioned variables are in level 1). The functions and their types get elaborated, as shown in \cref{cex:exist-elab}, by putting an extra type variable in each place where the output from the type inference is expected.


\begin{codex}
\caption{Elaborated program from \cref{cex:existentials}.}
\label{cex:exist-elab}
\begin{lstlisting}[style=haskellStyle]
MkT :: forall @x . (@y : Show @x) => @x -> T

show :: forall @s . (@r : Show @s) => @s -> String

xs = [ MkT @a (@e : Show @a) 5
     , MkT @b (@f : Show @b) "String"]

f :: @s -> @t
f (MkT @c (@g : Show @c) {getT=(x : @c)} : T) =
    show @d @h x
\end{lstlisting}
\end{codex}

The observations from the code are collected into the set of constraints, which is shown in \cref{cex:exist-constr}. The constraints in the sample may be solved by assigning $@a := \mtt{Int}$, $@b := \mtt{String}$, $@e := \mtt{Show}\ \mtt{Int}$, $@b : \mtt{Show}\ \mtt{String}$, $@s := t$, $@d := @c$, $@h := @g$, and $@t := \mtt{String}$. The resulting type scheme for \li{f} is: $T \to \mtt {String}$.

\begin{codex}
\caption{Constraints generated from \cref{cex:existentials}.}
\label{cex:exist-constr}
\begin{lstlisting}[style=haskellStyle]
-- from xs:
@a ~ Int,    @e : Show @a,
@b ~ String, @f : Show @a

-- from f:
@s ~ T -- because the input parameter has type T
forall @c . (@g : Show @c) => -- w. required by MkT
    @d ~ @c, -- from the type signature of `show'
    @h : Show @d, -- witness required by show
    @t ~ String -- returned by show
\end{lstlisting}
\end{codex}

\FloatBarrier
\subsection{Deferred inference algorithm}

The Deferred inference algorithm performs the type inference by rewriting the set of constraints that were generated during the code elaboration into an equivalent, simpler set of constraints in each step until it either solves all constraints or ends up with a set of constraints that can not be reduced further. \cite{peytonjones2019type}

Our simplified presentation of the algorithm (\Cref{deferred_loop}) is further subdivided to a special case that handles a class constraint (\Cref{case_class}) and an equality constraint (\Cref{case_equality}).

\begin{algorithm}
    \caption{Main loop of deferred inference algorithm (simplified, from \citet{peytonjones2019type})}
    \label{deferred_loop}
    \begin{algorithmic}
        \Require $\mtt {Ctx} \equiv \cdots \forall \vect \alpha_2 . A_2 \To \forall \vect \alpha_1 . A_1 \To \mcal W$ is the current context (possibly without the quantifications and corresponding qualifications; or with more)
        \State \Comment {$A_1, A_2, \cdots$ are assumptions of the current context $\mtt{Ctx}$}
        \State $\mtt{Level} \gets$ the number of quantifications in $\mtt{Ctx}$
        \State $P \gets \emptyset$ is a list of deferred constraints
        \State $\mtt{Continue} \gets \bot$ is a flag signifying whether to continue with deferred constraints
        \While{$\mcal W$ is not empty} \Comment {There are unsolved constraints}
            \If{$\mcal W = P$} \Comment {All constraints are deferred}
                \If {$\mtt {Continue} = \top$} \Comment {Continue with deferred constraints}
                    \State $P \gets \emptyset$
                    \State $\mtt {Continue} = \bot$
                \Else
                    \State \Return $\mcal W$ \Comment {Residual set of constraints}
                \EndIf
            \EndIf
            \State $c \gets $ choose from $\mcal W \setminus P$ (according to an arbitrary heuristic)
            \If{$c \equiv d : C \vect t$ is a class constraint}
                \State perform \cref{case_class}
            \ElsIf{$c \equiv g : a \sim b$ is an equality constraint}
                \State perform \cref{case_equality}
            \ElsIf{$c \equiv \forall \vect \beta . P \To \vect W$ is an implication constraint}
                \If{$\vect W$ is empty}
                    \State $\mcal W \gets \mcal W \setminus \{c\}$ \Comment {Remove the redundant constraint $c$}
                \Else
                    \State perform \cref{deferred_loop} with the context $\forall \vect \alpha . A_1 \To \forall \beta . P \To \vect W$, take the resulting $\vect W'$ and update context accordingly
                    \If {a failure $F$ is reported}
                        \State \textbf{propagate failure} $F$
                        \State \Return $\mcal W$
                    \EndIf
                    \State $\mtt{Continue} \gets \mtt{Continue} \lor (\vect W \neq \vect W')$
                \EndIf
            \EndIf
        \EndWhile
    \end{algorithmic}
\end{algorithm}
\begin{algorithm}
    \caption{Solving a class constraint case in deferred inference}
    \label{case_class}
    \begin{algorithmic}
        \Require {$c \equiv d : C \vect t$}
        \If{$d$ is a witness of a concrete assumption or proof}
            \State $\mcal W \gets \mcal W \setminus \{c\}$ \Comment {Remove the redundant constraint $c$}
        \ElsIf{$C \vect t$ is proven by some assumption $d' : C \vect t'$}
            \State unify $d$ and $d'$ and globally rewrite all constraints accordingly
            \State $\mcal W \gets \mcal W \setminus \{c\}; \mtt{Continue} \gets \top$ \Comment {Solved the constraint $c$}
        \ElsIf{$C \vect t$ matches some proof scheme $s \equiv \forall \vect \alpha . A \To d' : C \vect t'$}
            \State instantiate $s$ into $s' \equiv A' \To d' : C \vect t$
            \State $\mcal{W} \gets \mcal{W} \cup A'$ \Comment {add $A'$ to the current context}
            \State unify $d$ and $d'$ and globally rewrite all constraints accordingly
            \State $\mcal W \gets \mcal W \setminus \{c\}; \mtt{Continue} \gets \top$ \Comment {Solved the constraint $c$}
        \Else
            \State $P \gets P \cup \{c\}$ \Comment {The constraint $c$ is deferred}
        \EndIf
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{Solving an equality constraint in deferred inference}
    \label{case_equality}
    \begin{algorithmic}
        \Require {$\mtt {Ctx} \equiv \cdots \forall \vect \alpha_2 . A_2 \To \forall \vect \alpha_1 . A_1 \To \mcal W$} \Comment{the current context (possibly without the quantifications and corresponding qualifications; or with more)}
        \Require $\mtt {Level}$ \Comment{The current context level}
        \Require {$c \equiv g : a \sim b$}
        \If{$a$ and $b$ are the same}
        \State $\mcal W \gets \mcal W \setminus \{c\}$ \Comment {Remove the redundant constraint $c$}
        \ElsIf{$\mtt{Level} \geq 1 \land \vect \alpha_1 \cap (\free a \cup \free b) = \emptyset$}
            \State $\mtt{globallyPromoteUnificationVariables}(\mtt {Level}, c)$ \Comment {``promote'' the unification variables of the current context appearing in $c$, globally decreasing their context level}
            \State $\mtt {Ctx} \gets \mtt{floatConstraintHigher}(\mtt{Ctx}, c)$
        \ElsIf{Neither of $a$ and $b$ is a unification variable}
            \If{$a \equiv T \vect s, b \equiv T \vect t, |\vect s| = |\vect t|$}
                \State $\mcal W \gets \mcal W \setminus \{c\}$ \Comment {Remove the constraint $c$}
                \State $\mcal W \gets \mcal W \cup \{s_n \sim t_n | 1 \leq n \leq |s|\}$
            \ElsIf{$a \equiv S \vect s, b \equiv T \vect t, S \neq T \lor |\vect s| \neq |\vect t|$}
                \State \textbf{report unification failure} of $c$
                \State \Return $\mcal W$
            \Else \Comment{Two Skolem constants, type functions, etc.}
            \State $P \gets P \cup \{c\}$ \Comment {The constraint $c$ is deferred}
            \EndIf
        \Else
            \ \Comment{W. L. O. G. Let us assume that $a$ is a unification variable and, if $b$ is one as well, $a$ has an context level greater or equal to $b$'s.}
            \If {$a \in \free b$ }\Comment{Occurs check}
                \State \textbf{report occurs failure} of $c$
                \State \Return $\mcal W$
            \ElsIf {$\mtt{ctxLevel} (a) = \mtt {Level}$}
                \State $\mtt {globallyUnify}(a,b)$  \Comment {rewrite all constraints accordingly}
                \State $\mcal W \gets \mcal W \setminus \{c\}; \mtt{Continue} \gets \top$ \Comment {Solved the constraint $c$}
            \Else
                \State $P \gets P \cup \{c\}$ \Comment {The constraint $c$ is deferred}
            \EndIf
        \EndIf
    \end{algorithmic}
\end{algorithm}

\citet{vytiniotis2011outsidein} proved that the algorithm is terminating, given some requirements are met in the set of the allowed input constraints. At the same time, the algorithm is confluent --- it produces the same result regardless of the reordering of the input.

The precise requirements on the input constraints are rather complicated. We, therefore, highlight only the ones that are directly relevant to our results:

\begin{enumerate}
    \item All assumptions for witness proofs have to be of the form $d : C a$, where $C a$ is the assumed constraint, $d$ is its witness, and $a$ is a type variable. \label{triv_assump}
    \item All type variables appearing in assumptions of witness proofs have to be free in the proven constraint \label{free_free}
    \item Witness proofs have to be non-overlapping (equivalently: the proven constraints of two distinct proofs cannot be unifiable). \label{noover}
    \item Witness proof schemes have to be of the form $\forall \vect \alpha . P \vect \alpha \To d : C (T \vect \alpha)$, where $P \vect \alpha$ is a set of assumptions, $C (T \vect \alpha)$ is the proven constraint, and $T$ is a type constructor and $d$ is the witness \label{lookup_t}
    \item Ambiguous schemes are not allowed (universally) \label{ambi_rule}
\end{enumerate}

We consider an assignment $\mtt{name} :: \forall \vect \alpha . P  \To t$ \emph{ambiguous} if, after solving all solvable constraints, $\free {P} \setminus \free t \setminus \free \Gamma \neq \emptyset$ (in THIH, there is \emph{defaulting} described, which can resolve these ambiguities under some strict conditions, but we will not consider them here as they do not apply to general cases).

The rules \ref{triv_assump}, \ref{free_free} and \ref{lookup_t} together ensure that the steps solving a class constraint are always reducing. The rule \ref{noover} then ensures the class constraints satisfy the confluence property.

The rule \ref{lookup_t} also ensures the proof schemes can be looked up efficiently, as the lookup can be implemented via a lookup table indexed by the $T$ type constructor.

Finally, the rules \ref{triv_assump} and \ref{lookup_t} can also be intuitively generalized for use with multi-parameter typeclasses.

\subsection{Improvements over the algorithm W}

We have introduced a new algorithm, which we use in the prototype compiler. We have shown that it is more general than the algorithm W as it allows us to cover the MPTCs and existentials (if we used them) and is easier to generalize, only requiring careful implementation of additional cases to the constraint-solving loop.

\begin{itemize}
    \item  The solving of constraints can be performed in any arbitrary order as long as the extensions of the solver preserve its confluence property \cite{vytiniotis2011outsidein}.

    \item The solving is completely independent on the specifics of the typed language structure, this is achieved via the elaborating it with placeholders for types and witnesses, then, expressing the input for inference via those placeholders alone, and then finally filling in the placeholders back to the language.

    \item The notion of witnesses allows us to reduce the amount of explorable proof chains, because the constraints of related or equivalent proofs can be easily unified and removed.
\end{itemize}

\section{Deferred solving for systems programming}
\label{sys_defer}

In this thesis, we use an algorithm based on the deferred inference algorithm because it provides easy-to-extend formalisms that require only a small number of modifications to achieve the coverage of the desired set of typing capabilities -- namely, MPTCs.

We extended the type system with a highly specialized version of subtyping, necessary for type-checking and type-inferring secondary properties of variables defined in \cmm (we introduce those in \cref{chap2}, which describes the language specifics). Then, we also introduce functional dependencies. These are implemented via a rewriting scheme that takes precedence over the class constraint resolution. We specify the details in \cref{chap3}.

In our type system, we do not support existentials, as we want the maximal transparency of the resulting code for possible use in systems programming. Existentials generate irremovable witnesses, as we explained in \cref{defer_solve} because the actual type of the input existentials is known only in run-time (\citet{grossman2002existential} provides a closer exploration of this topic). This approach guarantees type safety as it ensures the witnesses are passed properly, but it does so at a cost we would like not to have.

Not using existentials in our type system allows us to completely omit the context levels from the deferred solving algorithm, which then allows us to solve all equality constraints eagerly, as the solvability of such constraints is limited only by the context levels. We also solve all constraints of the whole program globally. This turned out to be working but a very inefficient approach, as discussed in \cref{chap5}.
