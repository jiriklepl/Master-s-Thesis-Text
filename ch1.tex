\chapter{The French approach to type inference}

\label{chap1}

\section{The Algorithm W and how it connects to deferred solving}

As this thesis builds on the work of the author's bachelor thesis \cite{klepl2020type}, we assume the same syntactic notation and semantics of the untyped lambda calculus, STLC and System F (see \cite{barendregt1992lambda}), Hindley-Milner type system (see \cite{damas1982principal}), and the THIH-inspired type system (for THIH, see \cite{jones1999typing}).

We concluded that, if a term is typable in these systems, the Algorithm W will find it's principal type (with an exception of the System F, typing of which is undecidable, see \cite{wells1999typability}).

Throughout this section, we will show that the Algorithm W provides a good basis for typechecking of the type, but with more extensions, namely functional dependencies (see \cite{jones2000type}), it becomes unnecessarily complex and inefficient if we want to use it for type inference as well. We will describe, how the same principles can be used to implement much simpler and more extensible two-phase algorithm based on constraint solving.

\begin{defn}[Type inference and monomorphization]
    Before we begin, let us lay down two terms we will extensively use.

    \textbf{Type inference} is a process of semantic analysis, which assigns a type (according to the given type system) to each node of the AST, where it makes sense (expressions, procedures, record types, etc.). Each such type can be either monotype or polytype and the object then monomorphic or polymorphic, respectively. Polytypes are characterized by being quantified.

    \textbf{Monomorphization} statically interprets the generalized callgraph (also including references to type definition) and, for every reference to a polymorphic object, it generates a monomorphic copy of this object, which has the polytype instantiated to the monotype fitting the current context.
\end{defn}

\subsection{Limitations of Algorithm W}

The Algorithm W, which we used in the previous work(\cite{klepl2020type}), assumed no recursion in the code and thus any real program with any trivial or nontrivial recursion had to be rewritten into a form where recursive functions were represented by the \lstinline{fix} operator applied to some argument. Type of the \lstinline{fix} operator is $\forall \alpha . \pars {\alpha \to \alpha} \to \alpha$.

The algorithm for preprocessing trivial recursions is the following (derived from fixed point theorem, see \cite{barendregt1992lambda} and \cite{damas1982principal}): a function represented as $f \vect{x} = \calC[f, \vect{x}]$, where $C$ is a lambda term such that $\{f\} \cup \vect{x} \subseteq \free \calC$, is simply rewritten into $f = \mathit{fix}\ \lambda F \vect{X} .\calC[f := F, \vect{x} := \vect{X}]$.

For a nontrivial recursion (which we can identify using an algorithm for finding strongly connected components of the call graph), we use a generalization of fixed-point theorem, so-called double fixed-point theorem (see \cite{stepanek}), which describes how to construct this rewriting for a component of size two and can be easily generalized for an arbitrary component.

For our purposes of generalizing C to use a HM-style type system with type classes, we determined that the Algorithm W, as described in THIH, is too weak. In the following subsections we will explore possible extension of the Algorithm W and discuss its practicality and how it gives a motivation for the Deferred Solving algorithm introduced in \cref{defer_solve}.

\subsection{Limitations of the syntax-driven aspect of Algorithm W}
\label{ex_structs}

\begin{defn}[Field accessors]
    For the needs of type inference, we, in the context of Algorithm W, represent field accessors as functions from a record structure to the given field (or, equivalently, with a pointer constructor applied to them), each such function named after the given field name.
\end{defn}

The Algorithm W was able to either type-infer a program with no field overloading (all record field names are unique even between multiple record types) or type-check a program but with limited type inference if we would allow field name overloading.

What does this mean? Let us define two record types:

\begin{lstlisting}
    struct A {int x} a;
    struct B {float x} b;
\end{lstlisting}

Now, if we write \lstinline{a.x} or \lstinline{b.x}, it is obvious that these expressions have types \lstinline{int} and \lstinline{float}, respectively.

If we renamed the field of \lstinline{B} to have different name (for example, \lstinline{y}), we could represent the field accessors \lstinline{.a} and \lstinline{.b} as functions $.x : A \to int$ and $.y : B \to float$.

But we do not want to bother the programmer with such strong constraints of requiring the fields to be uniquely named, so to allow them have different types even though they share the same name, we define them as two instances of a method of ``HasX'' type class with two type arguments (one for the record type and the other for the field type).

This is also because the field accessors can have semantic meaning separate from the fact of being a named field of its respective structure. Take \li{Age} as an example: many different structures can have some age. And so we define a class \li{Has_x a b}, which states that the record type $a$ has a field $x$ typed $b$.

We then define the field accessors \li{.x} as instances of the following method in multi-parameter class \li{Has_x}: \lstinline|class HasX a b {.x : a -> b}| (with an $a \to b$ functional dependency which we will describe later).

The two instances are then defined:

\begin{lstlisting}
    instance HasX A int {.x : A -> int}
    instance HasX B float {.x : B -> float}
\end{lstlisting}

In the HM type system, this constraint cannot be expressed at all as the class constraints can be defined solely in the terms of taking a single type variable.

In the THIH variation (see \cite{jones1999typing}), this can be indeed expressed, and the algorithm is strong enough to type-check such construction, but it lacks the ability to infer the obvious types of previously mentioned expressions \lstinline{a.x} and \lstinline{b.x} even though we know the types of both \lstinline{a} and \lstinline{b} and the expressions are defined by them.

\subsection{Tackling the limitations of Algorithm W}

In the \cref{ex_structs}, the inability to capture functional dependencies could be possibly solved during subsequent monomorphization phase, which would know that the type of a record type determines the types of its fields, but in a general case, this would mean that the subsequent monomorphization phase would have to be unnecessarily complex and it would function as a second type inference. So we have to address this issue in the type inference itself.

\subsubsection*{Using the Algorithm W in the type system with functional dependencies}

If we want to simply modify the Algorithm W so it solves principal types even in the case of functional dependencies and other such constructs, we can easily observe that in order to function properly, it has to, on each subprogram (strongly connected component of the program's call-graph; we perform type inference on these subprograms in topological order, beginning with the leaf components), first perform the usual unifications on the syntactic structure of the subprogram, returning a type assignment in the form: $\mtt{name} :: \forall \vect \alpha . P  \To t$, where $P$ is a set of constraints, each having a form $C \vect \tau$, where $C$ is a predicate on the types $\vect \tau$, propagated from the subexpressions of $\mtt {name}$, under some $\vect \alpha = \free t$. And then it has to solve the constraints in $P$ according to some extended constraint solving mechanism, for example with maps, as explored by \cite{jones2000type}. Here we see the first hints of the algorithm turning into Deferred Solving we will introduce in \cref{defer_solve}.

\begin{defn}[Ambiguous assignments]
    We consider an assignment $\mtt{name} :: \forall \vect \alpha . P  \To t$ ambiguous if, after solving all solvable constraints, $\free {P} \setminus \free t \setminus \free \Gamma \neq \emptyset$ (in THIH, there is so-called defaulting described, which can resolve these ambiguities under some strict conditions, but we will not consider them here as they do not apply to general cases).
\end{defn}

THIH describes the constraint propagation through the syntactic structure of the subprogram to the scheme of the resulting type well (and it is easy to generalize it on other syntactic structures), but then, it does not describe how to solve the constraint $C$ if it does not fall into the case of being instantiable from a known predicate.

\subsubsection*{Intermission: solving class constraints from known proofs}

\begin{ex}[Solving a constraint by instantiating a known predicate]
    For example, given $\Gamma = \{\forall a . \mtt {Ord} [a]\}$ (using a simplified representation for contexts, combining class environments and assumptions into one set $\Gamma$), we can prove $\Gamma \vdash \mtt {Ord} [Int]$, using substitution $[a := \mtt {Int}]$.
\end{ex}

\begin{observe}[Proof is supported by an instantiating substiution]
    Notice that the substitution $\sigma$ that is used to prove a constraint $C \tau$ from a known predicate (or proof) $C \tau'$ has to satisfy $\tau = \tau' \sigma$, which then also means, it satisfies $\dom \sigma \subseteq \free \tau' \land \ran \sigma \subseteq \free \tau$ (it consumes the free variables of $\tau'$ not present in $\tau$).
\end{observe}

\begin{ex}[Class constraints often generate more class constraints]
    \label{chaining}
    More complicated example would be proving $\Gamma \vdash \mtt {Ord} [Int]$ in the context $\Gamma = \{\mtt {Ord}\ \mtt {Int}; \forall a . \mtt {Ord}\ a \To \mtt {Ord} [a]\}$. First, we deduce $\Gamma \vdash \mtt {Ord} [Int] \Leftarrow \Gamma \vdash \mtt {Ord}\ Int$ (proven using substitution $[a := \mtt {Int}]$ and the proof $\forall a . \mtt {Ord}\ a \To \mtt {Ord} [a]$) and then $\Gamma \vdash \mtt {Ord}\ Int$ (proven by being an assumption of the context; therefore, using an empty substitution). This shows that, if we introduce functional dependencies to the type system (or even non-reducing proofs), the typing can be infinite for some contexts and thus undecidable.
\end{ex}

\subsection{The challenges of the modified W, rise to a two-phase algorithm}

In the \cref{chaining}, we show that each constraint can generate more constraints, and the Algorithm W as described by THIH, then solves each of theses chains separately. And often, they can break into trees. This then requires a significant amount of work and progressively more complicated code, which resolves each of the cases.

\begin{observe}[Hints of a two-phase approach to type inference]
    The modified Algorithm W with recursion being present in the language alongside functional dependencies we deduced from the record types, is then very complicated ,and it should be obvious from the previous examples the modified algorithm, regardless of modifications, deconstructs into a two-phase process where the first generates some constrained type assignments and then the second resolves the constraints.
\end{observe}

Using the Algorithm W requires some nontrivial preprocessing and mapping the subprogram to the resulting type (as we can see in \cite{jones1999typing}), and then, the subsequent constraint solving performs very inefficient chains of nested proof lookups shown by \cref{chaining}.

We will describe a two-phase algorithm, which does not require the described difficult preprocessing, and introduces ``so-called'' witnesses, which simplify solving multiple class constraints.

\section{Deferred Solving}
\label{defer_solve}

The idea of using Deferred solving (proposed by \cite{vytiniotis2011outsidein}) is first ``elaborating'' the source program with special variables (elaborations), then running the inference on constraints generated from analysis of the source program, over the elaboration variables. This allows the constraint solver be independent on the syntactic specifics of the source program as the type semantics of the program are all captured in a simpler \emph{constraint language}. After the inference successfully finishes,  we insert the inferred information back in the source program in place of the elaborations.

elaborations consist of type variables, and then \emph{dictionaries} and \emph{coercions} (defined by \cite{vytiniotis2011outsidein}) that help the subsequent monomorphization phase decide the concrete instances that have to be used at each referring site. In the deferred solving algorithm they serve as proofs for each constraint. \emph{Dictionaries} represent type class instances that prove class constraints. And \emph{coercions} prove type identities. We call dictionaries and coercions, collectively, \emph{witnesses}.

The deferred solving, or deferred inference, is very similar to the idea of modified Algorithm W we presented earlier, but it does not require each of the objects appearing in the type-inferred subprogram be directly syntactically mapped to a certain type. It just requires a set of constraints that capture the semantics of the program. These constraints, in the basic version, consist of type equalities between types that have to be equal and class constraints put on certain types that require a proof of the given tuple of types belonging to the given class (these proofs represent instances defined in the type-inferred language).

It also simplifies the notion of type assignments in such a way that all types considered by the algorithm are monotypes. And the type quantification with predicates, which, in the Algorithm W is considered a part of the type assignment, is, in this algorithm, instead, applied to a set of constraints.

\begin{defn}[Type Quantification]
    A quantification that universally binds a set of type variables.

    We will often omit the word type, when referring to type quantification.
\end{defn}

\begin{defn}[Qualification]
    Qualification is a construct consisting of a set of predicates (or constraints) being applied to an unquantified term of the type language. We then call it a qualified term.

    The term being qualified is usually a type or a set of constraints.
\end{defn}

\begin{defn}[Scheme]
    A <term> scheme is a quantification applied to a qualified term of the type language.

    We will call type schemes just schemes.
\end{defn}

\subsection{Constraint language}

The language of constraints is an extension of traditional type language with type variables annotated by ``context levels'', giving a language of flat and nested constraints:
\begin{defn}[Language of constraints]
  Language $F$ of \emph{flat constraints} is defined as follows:

    \begin{center}\begin{grammar}
      \firstcasesubtil{$F$}{d: C \tau^\star}{Class constraint}
      \otherform{g : \tau_1 \sim \tau_2}{Equality constraint}
    \end{grammar}\end{center}

  The language $W$ of \emph{nested constraints} is defined inductively as follows:

    \begin{center}\begin{grammar}
      \firstcasesubtil{$W$}{F}{Flat constraint}
      \otherform{\forall \alpha^\star . F^\star \To W^\star}{Implication constraint}
    \end{grammar}\end{center}

    \label{constraint_language}
\end{defn}

We call $d$ and $g$ the \emph{witnesses} of the corresponding constraint, because they represent the instances that prove the given constraints.

\begin{remark}
    Unless we allow overlapping proofs, the witnesses are isomorphic to the rest of their respective constraint. For example, given two constraints $d_1: C \vect \tau_1$, $d_2: C \vect \tau_1$, then $d_1 = d_2 \GetsTo \tau_1 = \tau_2$.
\end{remark}

Additionally, to the already described equality constraints and class constraints, the constraint language also contains ``implication constraints'', which are constraint schemes. They represent typings, both explicit and implicit, of functions declared by the programmer. Existentials contained in function types (e.g., parameters with run-time type binding) are represented by their own implication constraint.

\begin{defn}[Naming]
    \begin{itemize}
        \item We call the set of constraints, which is then qualified, in the scheme of the implication constraint \emph{nested} in this implication constraint.

        \item We call the set of nested constraints of all the parent implication constraints a \emph{scope} or a \emph{context}.

        \item We use scopes to describe a context level of a constraint. For a constraint, a \emph{context level} is the maximum length of implications leading to it.

        \item \emph{Context level} of the unbound type variables introduced in some scope in the \cref{constraint_language} have the context level which is equal to the context level of the scope. \label{constness_level}

        All the appearances of a certain type variable have the same context level.

        \item \emph{Skolem constants} in the given scope are the type variables bound by some parent implication constraint.
    \end{itemize}
\end{defn}

\begin{ex}[Context levels] Consider the following set of constraints:

    \begin{lstlisting}
a ~ b,
forall c. d ~ e => a ~ d -> e
    \end{lstlisting}

    The type variables $a, b$ have context level $0$; the type variables $d, e$ have context level $1$; and $c$, a skolem constant, has no context level.
\end{ex}

\subsection{Existentials}

\label{existentials}

Existentials are types with existentially-quantified type variables, the semantics of such are that there exists some assignment to the existentially-qualified variables that, within the given constraints, describes the value of the existential. This assignment has to be solved during dynamic interpretation of the code (runtime).

\begin{remark}[Semantic distinction between universals and existentials]
    A distinction between universals (universally quantified types) and existentials is that universals have unknown monotypes during definitions and they have to be defined polymorphically, while existentials have unknown types in references and can be defined monomorphically.
\end{remark}

\begin{remark}[Practical difference between existentials and universals]
    In practice, universal types lead to code multiplication, while existentials lead to a necessity of runtime type (coercion) witnesses and constraint witnesses.
\end{remark}

Existentials are typically constrained by some class, which then means, they carry the corresponding class's witness for a method lookup. We then do not need the type witness as it can be derived from the class witness. This is a possible optimization, which will be not considered in the provided example for better clarity. If the types in the existential are not constrained, then we need the type witness.

\begin{ex}[Existentials, adapted from \cite{peytonjones2019type}]
    The following haskell code with the \lstinline{GADTs} extensions shows a definition of an type \lstinline{T} with existentially-typed field \lstinline{getT} and then a list of variables of that type along with a function that consumes them, accessing the existential.

    \lstinputlisting[language=Haskell]{examples/existentials.hs}

    Now we would like to translate this example into the constraint language. First, we have to elaborate the program: (we will decorate the elaborations with $@$ for clarity; note that each elaboration comes with a context level, but we will not write them in the example as we will describe them collectively)

    The Context level for all globally-mentioned variables (outside any quantification) is $0$ and for all locally-mentioned variables (within a quantification) is $1$.

    \lstinline{MkT} gets translated into: (the quantified type variables and the class constraint serve as new arguments created by the elaboration: these new arguments are witnesses for the type and for the \li{Show} instance)
    \begin{lstlisting}[language=Haskell]
        MkT :: forall @x . (@y : Show @x) => @x -> T
    \end{lstlisting}

    (we will use the following type signature for \li{Show})
    \begin{lstlisting}
        show :: forall @s . (@r : Show @s) => @s -> String
    \end{lstlisting}

    \lstinline{xs} gets translated into:

    \begin{lstlisting}[language=Haskell]
        xs = [ MkT @a (@e : Show @a) 5
             , MkT @b (@f : Show @b) "String"]
    \end{lstlisting}

    \lstinline{f} gets translated into:

    \begin{lstlisting}[language=Haskell]
        f :: @s -> @t
        f (MkT @c (@g : Show @c) {getT=(x : @c)} : T) =
            show @d @h x
    \end{lstlisting}

    From this elaborated program, we simply write the constraints:

    \begin{lstlisting}[language=Haskell]
        -- from xs:
        @a ~ Int,    @e : Show @a,
        @b ~ String, @f : Show @a

        -- from f:
        @s ~ T -- because the input parameter is T
        forall @c . (@g : Show @c) => -- w. required by MkT
            @d ~ @c, -- from the type signature of `show'
            @h : Show @d, -- witness required by show
            @t ~ String -- returned by show
    \end{lstlisting}

    This set of variables is then solved by: the following assignments $@a := \mtt{Int}, @b := \mtt{String}, @e := \mtt{Show}\ \mtt{Int}, @b : \mtt{Show}\ \mtt{String}, @s := t, @d := @c, @h := @g, @t := \mtt{String}$ with resulting scheme for \li{f} being: \li{T -> String}.

    We will shortly specify how the algorithm, which provides the desired assignment, works.

    All unnested constraint in the top part of the example and nested constraints in the implication constraint are solved by the given assignment and therefore the inference has no leftover constraints.
\end{ex}

\begin{defn}[Unification variable]
    Unification variable is any type variable which is not bound in an implication constraint quantification (in other words, is not a skolem constant of a given scope).
\end{defn}

\subsection{Deferred Inference algorithm}

The inference is performed by rewriting the set of constraints into an equivalent set of constraints in each step. The Algorithm works as laid out in the \cref{deferred_loop}.



\begin{algorithm}
    \caption{Main loop of Deferred Inference (simplified version, presented by \cite{peytonjones2019type})}
    \label{deferred_loop}
    \begin{algorithmic}
        \Require $\mtt {Ctx} \equiv \cdots \forall \vect \alpha_2 . A_2 \To \forall \vect \alpha_1 . A_1 \To \mcal W$ is the current context (possibly without the quantifications and corresponding qualifications; or with more)
        \State \Comment $A_1, A_2, \cdots$ are assumptions of the current context $\mtt{Ctx}$
        \State $\mtt{Level} \gets$ the number of quantifications in $\mtt{Ctx}$
        \State $P \gets \emptyset$ is a list of postponed constraints
        \State $\mtt{Continue} \gets \bot$ is a flag signifying whether to continue with postponed constraints
        \While{$\mcal W$ is not empty} \Comment there are unsolved constraints
            \If{$\mcal W = P$} \Comment all constraints are postponed
                \If {$\mtt {Continue} = \top$} \Comment continue with postponed constraints
                    \State $P \gets \emptyset$
                    \State $\mtt {Continue} = \bot$
                \Else
                    \State \Return $\mcal W$ \Comment residual set of constraints
                \EndIf
            \EndIf
            \State $c \gets $ choose from $\mcal W \setminus P$ (according to an arbitrary heuristic)
            \If{$c \equiv d : C \vect t$ is a class constraint}
                \State perform the \cref{case_class}
            \ElsIf{$c \equiv g : a \sim b$ is an equality constraint}
                \State perform the \cref{case_equality}
            \ElsIf{$c \equiv \forall \vect \beta . P \To \vect W$ is an implication constraint}
                \If{$\vect W$ is empty}
                    \State $\mcal W \gets \mcal W \setminus \{c\}$ \Comment remove the redundant constraint $c$
                \Else
                    \State perform the \cref{deferred_loop} with the context $\forall \vect \alpha . A_1 \To \forall \beta . P \To \vect W$, take the resulting $\vect W'$ and update context accordingly
                    \If {a failure $F$ is reported}
                        \State \textbf{propagate failure} $F$
                        \State \Return $\mcal W$
                    \EndIf
                    \State $\mtt{Continue} \gets \mtt{Continue} \lor (\vect W \neq \vect W')$
                \EndIf
            \EndIf
        \EndWhile
    \end{algorithmic}
\end{algorithm}
\begin{algorithm}
    \caption{Class constraint case}
    \label{case_class}
    \begin{algorithmic}
        \Require {$c \equiv d : C \vect t$}
        \If{$d$ is a witness of a concrete assumption or proof}
            \State $\mcal W \gets \mcal W \setminus \{c\}$ \Comment remove the redundant constraint $c$
        \ElsIf{$C \vect t$ is proven by some assumption $d' : C \vect t'$}
            \State unify $d$ and $d'$ and globally rewrite all constraints accordingly
            \State $\mcal W \gets \mcal W \setminus \{c\}; \mtt{Continue} \gets \top$ \Comment Solved the constraint $c$
        \ElsIf{$C \vect t$ matches some proof scheme $s \equiv \forall \vect \alpha . A \To d' : C \vect t'$}
            \State instantiate $s$ into $s' \equiv A' \To d' : C \vect t$
            \State $\mcal{W} \gets \mcal{W} \cup A'$ \Comment add $A'$ to the current context
            \State unify $d$ and $d'$ and globally rewrite all constraints accordingly
            \State $\mcal W \gets \mcal W \setminus \{c\}; \mtt{Continue} \gets \top$ \Comment Solved the constraint $c$
        \Else
            \State $P \gets P \cup \{c\}$ \Comment the constraint $c$ is postponed
        \EndIf
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{Equality constraint case}
    \label{case_equality}
    \begin{algorithmic}
        \Require {$\mtt {Ctx} \equiv \cdots \forall \vect \alpha_2 . A_2 \To \forall \vect \alpha_1 . A_1 \To \mcal W$ is the current context (possibly without the quantifications and corresponding qualifications; or with more)}
        \Require {$c \equiv g : a \sim b$}
        \If{$a$ and $b$ are the same}
        \State $\mcal W \gets \mcal W \setminus \{c\}$ \Comment remove the redundant constraint $c$
        \ElsIf{$\mtt{Level} \geq 1 \land \vect \alpha_1 \cap (\free a \cup \free b) = \emptyset$}
            \State ``promote'' the unification variables of the current context appearing in $c$, globally decreasing their context level, and ``float'' the constraint $c$, moving it into the enclosing (caller's) context; change all contexts (this and the contexts of enclosing loops) accordingly
        \ElsIf{Neither of $a$ and $b$ is a unification variable}
            \If{$a \equiv T \vect s, b \equiv T \vect t, |\vect s| = |\vect t|$} \Comment $a$ and $b$ contain the same type constructor $T$ at the top
                \State $\mcal W \gets \mcal W \setminus \{c\}$ \Comment remove the constraint $c$
                \State $\mcal W \gets \mcal W \cup \{s_n \sim t_n | 1 \leq n \leq |s|\}$ \Comment more equality constraints with corresponding immediate subterms of $a$ and $b$
            \ElsIf{$a \equiv S \vect s, b \equiv T \vect t, S \neq T \lor |\vect s| \neq |\vect t|$} \Comment {$a$ and $b$ contain different type constructors at the top}
                \State \textbf{report unification failure} of $c$
                \State \Return $\mcal W$
            \Else \Comment{Two skolems, type functions, etc.}
            \State $P \gets P \cup \{c\}$ \Comment the constraint $c$ is postponed
            \EndIf
        \Else
            \ {W. L. O. G. Let us assume that $a$ is a unification variable and, if $b$ is one as well, $a$ has an context level greater or equal to $b$'s.}
            \If {$a \in \free b$ }\Comment{Occurs check}
                \State \textbf{report occurs failure} of $c$
                \State \Return $\mcal W$
            \ElsIf {$\mtt{Level} (a) = \mtt {Level}$}
                \State unify $a$ and $b$ and globally rewrite all constraints accordingly
                \State $\mcal W \gets \mcal W \setminus \{c\}; \mtt{Continue} \gets \top$ \Comment Solved the constraint $c$
            \Else
                \State $P \gets P \cup \{c\}$ \Comment the constraint $c$ is postponed
            \EndIf
        \EndIf
    \end{algorithmic}
\end{algorithm}

\begin{remark}\label{defer_properties}
    The algorithm is confluent and terminating (see \cite{vytiniotis2011outsidein}) under some requirements on the allowed constraints. Confluence is the property of the inference system that it gives the same result regardless of reordering of the input.
\end{remark}

\subsubsection{Requirements on the input constraints}
\label{requirements}

We present a nonexhaustive list of the requirements on the input constraints that guarantee confluence and termination, listing just the ones that are significant in our presented lemmas and remarks:

\begin{enumerate}
    \item All assumptions for witness proofs have to be of the form $d : C a$, where $C a$ is the assumed constraint, $d$ is its witness, and $a$ is a type variable. \label{triv_assump}
    \item All type variables appearing in assumptions of witness proofs have to be free in the proven constraint \label{free_free}
    \item Witness proofs have to be non-overlapping, the proven constraints of two distinct proofs cannot be unifiable. \label{noover}
    \item Witness proof schemes have to be of the form $\forall \vect \alpha . P \vect \alpha \To d : C (T \vect \alpha)$, where $P \vect \alpha$ is a set of assumptions, $C (T \vect \alpha)$ is the proven constraint, and $T$ is a type constructor and $d$ is the witness \label{lookup_t}
    \item Ambiguous schemes are not allowed (universally) \label{ambi_rule}
\end{enumerate}

The rules \ref{triv_assump}, \ref{free_free} and \ref{lookup_t} together ensure that the steps solving a class constraint are always reducing. The rule \ref{noover} then ensures the class constraints satisfy the confluence property.

The rule \ref{lookup_t} also ensures the proof schemes can be looked up efficiently, as the lookup can be implemented via a lookup table indexed by the $T$ type constructor.

\begin{remark}[Generalization of the rules for multi-parameter type classes]
    Rules \ref{free_free}, \ref{noover} and \ref{ambi_rule} are the same even in the context of multi-parameter type classes. \ref{triv_assump} and \ref{lookup_t} are generalized intuitively.
\end{remark}

\subsection{Improvements over the Algorithm W}

\begin{itemize}
    \item  The solving can be performed in an arbitrary order as long as the modifications preserve its confluence property, see the \cref{defer_properties}.

    \item The solving is completely independent on the specifics of the typed language structure, this is achieved via the elaboration it with placeholders for types and witnesses, then expressing the input for inference via those placeholders alone, and then finally filling in the placeholders back to the language.

    \item The notion of witnesses allows us to compress the many proof chains we explored in the \cref{chaining}.
\end{itemize}



\section{Deferred Solving for systems programming}
\label{sys_defer}

In this thesis, we use an algorithm based on the deferred inference algorithm is because it provides a very easy to extend formalisms which require only small number of modifications to it to achieve the coverage of the desired set of typing capabilities.

In our type system, we do not support existentials, as we want the maximal transparency of the resulting code for possible use in systems programming. Existentials generate irremovable witnesses we introduced in the \cref{defer_solve} and further explained in the \cref{existentials}, because the actual type of the input existentials is known only in run-time (see \cite{grossman2002existential} for an exploration of this topic). This approach guarantees type-safety as it ensures the witnesses are passed properly, but it does so at a cost we would like to not have. We leave exploration of including them into the type system, after sufficient analysis of runtime representations of the witnesses, for future work.

The lack of existential types is why they are described only briefly. They are still important to mention as one of the main motivations for the design of the deferred solving algorithm and as one of its supported features. They then provide good insight into the meaning of witnesses, which are still used for class constraint resolution during the inference and subsequent monomorphization.

The type system we use is much more extended compared to the type system we assumed so far in a special limited version of subtyping added to the system, which will provide us the necessary means of type-checking and type-inferring whether a variable is semantically a compile time constant and also what is its data kind (we will introduce those in the \cref{chap2}, which describes the language specifics).

Then, we also introduce functional dependencies. These are implemented via a rewriting scheme that takes precedence over the class constraint resolution. It simply replaces the given class constraint with a different set of constraints depending on the defined functional dependencies. We will specify the details in the \cref{chap3}.

As we do not use existentials in our type system, we do not need the context levels, which then allows us to solve all equality constraints eagerly, as the solvability of such constraints is limited only by the context levels.

We also solve all constraints of the whole program globally. This turned out to be a working, but very inefficient approach later \xxx{add to conclusion if it is not there}.
