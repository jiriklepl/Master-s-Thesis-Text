\providecommand{\free}[1]{\mtt{free} (#1)}
\providecommand{\dom}[1]{\mtt{Dom} (#1)}
\providecommand{\ran}[1]{\mtt{Ran} (#1)}

\providecommand{\bmath}[1]{\boldsymbol{#1}}

\chapter{The french approach}

\todo{CHANGE THE NAME}

\section{History \xxx{, connect it to the so-called damas-millner}}

As this thesis builds on the knowledge of the Bachelor thesis \xxx{add reference}, we assume the same syntactic notation and semantics of the untyped lambda calculus, STLC \xxx{reference?}, System F, Hindley-Milner type system, and the THIH-inspired type system.

We concluded that if a term is typable in these systems, the algorithm W will find it's principal type (with an exception of the System F, typing of which is undecidable).

The algorithm W assumed no recursion in the code and thus any real program with any trivial or nontrivial recursion had to be rewritten into a form where recursive functions were represented by the \lstinline{fix} operator applied to some argument. Type of the \lstinline{fix} operator is $\forall \alpha . \pars {\alpha \to \alpha} \to \alpha$.

The algorithm for trivial recursions is simple. A function represented as $f \bmath{x} = \calC[f, \bmath{x}]$, where $C$ is a lambda term such that $\{f\} \cup \bmath{x} \subseteq \free \calC$, is simply rewritten into $f = \mathit{fix}\ \lambda F \bmath{X} .\calC[f := F, \bmath{x} := \bmath{X}]$. For a nontrivial recursion (which we can identify using an algorithm for finding strongly connected components of the call graph), we use a generalization of the double fixed-point \xxx{reference it} theorem, describes how to construct this rewriting for a component of size two.

For our purposes of generalizing C to use a HM-style type system with type classes, we determined that the algorithm W as described in THIH is too weak. It was able to either type-infer a program with no field overloading (all record field names are unique even between multiple record types) or type-check a program with limited type inference if we would allow field name overloading. What does this mean? It is best to give an example:

Let us define two record types \lstinline|struct A {int x} a|, \lstinline|struct B {float x} b|. Now if we write \lstinline{a.x} or \lstinline{b.x}, it is obvious that they will have types \lstinline{int} and \lstinline{float}, respectively. If we renamed the field of \lstinline{B} to have different name (\lstinline{y}), we could represent the field accessors \lstinline{.a} and \lstinline{.b} as functions $.x : A \to int$ and $.y : B \to float$.

But we do not want to bother the programmer with such strong constraints, so to allow them have different types even though they share the same name, we define them as two instances of a method of ``HasX'' type class with two type arguments (one for the record type and the other for the field type). We define the method as \lstinline|class HasX a b {.x : a -> b}| (with an $a \to b$ functional dependency which we will describe later) and its two instances are \lstinline|instance HasX A int {.x : A -> int}| and \lstinline|instance HasX B float {.x : B -> float}| (all type signature are explicit for clarity, but the method instance types are result of simple type substitution).

The algorithm W as described in THIH is strong enough to type-check such construction, but it lacks the ability to infer the obvious types of previously mentioned expressions \lstinline{a.x} or \lstinline{b.x} even though we know the types of both \lstinline{a} and \lstinline{b}. In this particular example, this could be solved during subsequent monomorphization phase which would know that the type of a record type determines the types of its fields (for example, by using a simple map that would map structure types to field types). But in a general case, this would mean that the subsequent monomorphization phase would have to be unnecessarily complex and that it would have to defer monomorphizations of polytype targets (without functional dependencies present, the monomorphization algorithm would simply report that the program is not monomorphizable as it is polytyped). This leads to turning monomorphization into second type inference. And we will later introduce an inference algorithm that efficiently performs this deferred solving and then allows for simple linear monomorphization (linear in the terms of the size of the resulting monomorphized code).

If we want to simply modify the algorithm W so it solves principal types even in the case of functional dependencies and other such constructs, we can easily observe that in order to function properly, it has to first perform the usual unifications on the syntactic structure of each subprogram (strongly connected component of the program's call-graph; we perform type inference on these subprograms in topological order, beginning with the leaf components) returning type assignments in the form $\mtt{name} :: \forall \bmath \alpha . C \bmath \tau \To t$, where $C$ is the constraint on the types $\bmath \tau$ and $\bmath \alpha = $ (note that it can be broken into more granular constraints) propagated from the subexpressions of $\mtt {name}$. We consider such assignments as ambiguous if $\free \tau \setminus \free t \setminus \free \Gamma \neq \emptyset$ (in THIH, there is so-called defaulting described, which can resolve these ambiguities under some conditions, but we will not consider them here as they do not apply general cases). THIH describes the constraint propagation well, but then the algorithm it does not describe how to solve the constraint $C$ if it does not fall into the trivial case of being instantiable from a known predicate, for example, given $\mtt {Ord} [a] \in \Gamma$ (using a simplified representation for contexts combining class environments with assumptions into one set $\Gamma$), we can deduce $\Gamma \vdash \mtt {Ord} [Int]$, proven by substitution $[a := \mtt {Int}]$. Notice that the substitution $\sigma$ that proves a constraint $C \tau$ from a known predicate $C \tau'$ has to satisfy $\dom \sigma \subseteq \free \tau' \land \ran \sigma \subseteq \free \tau$, while also being an mgu \xxx{what is mgu} of $\tau$ and $\tau'$. \xxx{Connect it to the later algorithm.} \xxx{better styling etc?}

Another, more complicated, example would be deducing $\Gamma \vdash \mtt {Ord} [Int]$ from the context $\mtt {Ord}\ \mtt {Int}, \mtt {Ord}\ a \To \mtt {Ord} [a] \in \Gamma$. First, we deduce $\Gamma \vdash \mtt {Ord} [Int] \Leftarrow \Gamma \vdash \mtt {Ord}\ Int$ (proven by substitution $[a := \mtt {Int}]$) and then $\Gamma \vdash \mtt {Ord}\ Int$ (proven by empty substitution). This shows that if we introduce functional dependencies to the type system, the typing can be easily undecidable under certain conditions. \xxx{get back to it later}.

If we were concerned with substitutions being applied to bound variables, we can replace the quantified type assignments with unquantified constrained types, closures of which, they are. \xxx{this observation will be relevant for the french algorithm}.

The logic for modified algorithm W for recursion being present in the language alongside functional dependencies we deduced from the existence of record types is very complicated and it should be obvious from the previous examples the modified algorithm deconstructs into a two-phase process where the first generates some constrained type assignments and then it resolves their constraints. This required some nontrivial preprocessing and then the subsequent constraint solving has the limitation of performing the type inference strictly in topological order dictated by the call graph (otherwise the \xxx{above observation} would not hold).

\section{Deferring solving, aka the french approach}

The idea of this algorithm is ``elaborating'' the source program with special variables, dictionaries and coercions that then help the subsequent monomorphization decide the concrete instances that have to be used at each referring site.

In the deferring solving algorithm they serve as proofs for each constraint. Dictionaries represent type class instances that prove class constraints. And coercions prove type identities.

\subsection{Constraint language}

\begin{defn}[Language of constraints]
    The language of constraints is an extension of traditional type language with type variables annotated with ``context levels'' extended by the following two constructs, flat constraints (F) and nested constraints (W).

    \begin{grammar}{F \Rightarrow }{Flat constraint}
        d: C \bmath \tau & Class constraint \\
        \mid g : \tau_1 \sim \tau_2 & Equality constraint \\
    \end{grammar}

    $d$ and $g$ are witnesses of the constraint, they represent the instances that prove the given constraints

    \begin{grammar}{W \Rightarrow }{Nested constraint}
        F & Flat constraint \\
        W_1, W_2 & Conjunction \\
        \forall \bmath \alpha . \{F\} \To W & Implication constraint \\
    \end{grammar}

    \label{constraint_language}
\end{defn}

The equality constraint is generated when two types are required to represent the same type, class constraints come from containing methods of various type classes, and implication constraints represent declarations (in let-in polymorphism, this would be equivalent to let binds).

\begin{defn}[Context levels]
    Context level of the type variables introduced in nested constraints in the previous definition (\ref{constraint_language}) have the context level which is equal to the nesting level of the enclosing implication constraint. 

    The quantified type variables do not have context levels.
\end{defn}

\begin{ex}
    If we count the levels from $0$, then the variables introduced inside the first implication constraint would have context level equal to $1$.
\end{ex}

\subsection{Existentials}

Existential are types with existentially-quantified type variables. The semantics of such types are that there exists some type that, within the given constraints, describes the value of the existential. Just as universals can provide a value for every type variable assignments, existentials can provide a value for certain type variable assignments.

They are typically constrained by some class, which then provides the witness that proves the existence.

\begin{ex}[Existentials]
    The following code shows a definition of an type \lstinline{T} with existentially-typed field \lstinline{getT} and then a list of variables of that type along with a function that consumes them, accessing the existential. \xxx{ExistentialQuantification, GADTSyntax}

    \begin{lstlisting}[language=Haskell]
        data T where
            MkT :: Show a => { getT :: a } -> T
        -- defines a type `T' with
        --   an existentially-type field `getT'

        let xs = [MkT 5, MkT "String"]

        -- f :: T -> String
        f MkT{getT=a} = show a

        traverse_ putStr (fmap f xs)
        -- prints: 5"hello"

        -- `fmap f xs' applies `f' to each `x' in `xs'
        --   and collects the results
        -- `traverse_ action values' performs `action'
        --   on each `value' in `values'
    \end{lstlisting}

    Now we would like to translate this example into the constraint language. First, we have to elaborate the program: (we will decorate the elaborations with $@$ for clarity; note that each elaboration comes with a context level, but we will not include them in the example)

    The Context levels for all globally-mentioned variables (outside quantifications) is $0$ and for all locally-mentioned variables (within a quantification) is $1$.

    \lstinline{MkT} gets translated into: (the quantified type variables and the class constraint serve as new arguments created by the elaboration)
    \begin{lstlisting}[language=Haskell]
        MkT :: forall @x . (@y : Show @x) => @x -> T
    \end{lstlisting}

    (we will use the following type signature for `Show')
    \begin{lstlisting}
        show :: forall @s . (@r : Show @s) => @s -> String
    \end{lstlisting}

    \lstinline{xs} gets translated into: 

    \begin{lstlisting}[language=Haskell]
        xs = [ MkT @a (@e : Show @a) 5
             , MkT @b (@f : Show @b) "String"]
    \end{lstlisting}

    \lstinline{f} gets translated into:

    \begin{lstlisting}[language=Haskell]
        f :: @s -> @t
        f (MkT @c (@g : Show @c) {getT=(x : @c)} : T) = 
            show @d @h x
    \end{lstlisting}

    From this elaborated program, we simply write the constraints:
    
    \begin{lstlisting}[language=Haskell]
        -- from xs:
        @a ~ Int,    @e : Show @a,
        @b ~ String, @f : Show @a

        -- from f:
        @s ~ T -- because the input parameter is T
        forall @a . (@g : Show @c) => -- from MkT
            @d ~ @c, -- from the type signature of `show'
            @h : Show @d, -- 
            @t ~ String
    \end{lstlisting}
\end{ex}

\begin{defn}[Deffering inference]
    The inference is performed by rewriting the set of constraints into an equivalent set of constraints in each step.
    
    The algorithm is as follows:

    \begin{enumerate}
        \item We choose a constraint and continue with step \ref{true_constraint} if successful, otherwise with the step \ref{false_constraint} \label{choose_constraint}
        \item \label{true_constraint}
        \begin{itemize}
            \item If it is a class constraint, we decide whether it is proven by the assumptions of the current context, unifying the witnesses, otherwise we set the constraint aside and choose a different constraint
            \item If it is a equality constraint
                \begin{itemize}
                    \item If the quantified variables in the closest enclosing implication constraint don't appear free in the equality, float the equality into the enclosing context, promoting the unification variables to have the context level of the enclosing scope.
                        
                    \item If neither of the operands is a unification variable, we break the equality in more granular eqaulities, or, if it contains unresolved type functions, we set aside the constraint until we know more.
                    
                    \item WLOG, let us assume that it contains a unification variable on the left side (if both sides contain a unification variable, let us assume it is the one with higher context level, if they differ).
                        
                    
                    If the unification variable has context level of the current scope, we perform the unification and rewrite other constraints accordingly, removing the equality constraint.

                \end{itemize}

            \item If it is an implication constraint, we increment the context level of the solver (if it doesn't infer the context level from the enclosing contexts) and choose a constraint from the current scope.
            
            If the implication constraint is empty (has no nested constraints; on the right side of the implication), we erase it.
            
        \end{itemize}
        \item We continue to the step \ref{choose_constraint}

        \item \label{false_constraint} If no constraint can be chosen and then successfully resolved, the algorithm stops.
    \end{enumerate}
\end{defn}

The algorithm is confluent and terminating \xxx{reference to the essence of ML TI}



\section{What we need from the algorithm, what we add and what we remove}

In our type system, we don't have existentials as we want the maximal transparency of the resulting code. Existentials generate irremovable dictionaries we introduced \xxx{where?} because the code in the run-time can encounter different types. This approach can ensure type-safety, but it does so at a cost we would like to not have. \xxx{more explanation?}

The lack of existential types is why they are introduced only briefly. They are still important to mention as one of the main motivations for the design of the algorithm and as its feature.

The type system we use is much more extended than the type system defined by type equalities and class constraints in a different way.

We introduce a special limited version of subtyping.
